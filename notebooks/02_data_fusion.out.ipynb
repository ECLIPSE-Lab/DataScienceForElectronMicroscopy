{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regression example - sensor fusion\n",
        "\n",
        "## 02.2 Regression example: Sensor Fusion EDX + HAADF\n",
        "\n",
        "<br>"
      ],
      "id": "c3c9fc11-8a3c-4e1e-b070-8dab5c341c64"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h2>"
      ],
      "id": "3285a25a-7817-4098-8b7c-dbbb46c8fa38"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Science in Electron Microscopy"
      ],
      "id": "74cb01cd-dac2-4f16-89ee-7660c54177cd"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h2>"
      ],
      "id": "3f803024-9672-4b53-a5f8-8e77a33814b8"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<hr>"
      ],
      "id": "478b592d-6582-479d-b27b-1684289894e8"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h3>"
      ],
      "id": "b5656f49-e5f5-4f37-8158-0d6e5eb5b9a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Philipp Pelz"
      ],
      "id": "6446e0fb-d7ae-43ff-a322-6efcc380ec86"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h3>"
      ],
      "id": "7aa5c46f-cbd8-4335-838a-b250f639fc50"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h3>"
      ],
      "id": "a9a766df-10ad-4dc2-b5e6-404e7a4ab730"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2024"
      ],
      "id": "98ed006a-ca1e-4dbd-9d4f-a34a37793229"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h3>"
      ],
      "id": "5a841658-dd8e-4ba1-ab90-6cafde1d6815"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ],
      "id": "2bcaa1a7-facc-4ebe-86d1-e5ecbf042dff"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h3>"
      ],
      "id": "0e079947-00b3-47d4-8973-f912f5db96bf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  <https://github.com/ECLIPSE-Lab/WS24_DataScienceForEM>"
      ],
      "id": "a9f6951b-8cc9-42f2-937a-eb0ed9ccfd7a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h3>"
      ],
      "id": "f36154ff-0e1e-4456-a0e3-e3c74e6c8448"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Python modules and Chemical Images"
      ],
      "id": "28845e2a-9618-4f55-8c71-ad7fd6eba111"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Data handling\n",
        "import h5py\n",
        "\n",
        "# Deep learning\n",
        "import torch as th\n",
        "import kornia\n",
        "\n",
        "# Image processing\n",
        "import cv2\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Local utilities\n",
        "import fusion_utils as utils\n",
        "\n",
        "## Load Chemical Maps \n",
        "\n",
        "fname = 'CoSX_maps.h5'; mapNum = 'map7/'\n",
        "\n",
        "# Parse Chemical Maps\n",
        "elementList = ['Co', 'O', 'S']\n",
        "\n",
        "# Load Raw Data and Reshape\n",
        "file = h5py.File(fname, 'r')\n",
        "\n",
        "print('Available EDX Maps: ', list(file))\n",
        "\n",
        "x = []\n",
        "for ee in elementList:\n",
        "    # Read Chemical Map for Element \"ee\"\n",
        "    edsMap = file[mapNum+ee][:,:]\n",
        "    # Set Noise Floor to Zero and Normalize Chemical Maps\n",
        "    edsMap -= np.min(edsMap); edsMap /= np.max(edsMap)\n",
        "    # Concatenate Chemical Map to Variable of Interest\n",
        "    x.append(edsMap)\n",
        "x0 = np.stack(x)\n",
        "print(f'x0 shape {x0.shape}')"
      ],
      "id": "cell-1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Total Variation Denoiser Module"
      ],
      "id": "bc0c8bee-8377-4ba4-ab2c-dbad3719338e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TVDenoise(th.nn.Module):\n",
        "    \"\"\"Total variation denoising implemented as a neural network module.\n",
        "\n",
        "    This class implements an optimization-based denoising approach that minimizes\n",
        "    a combination of:\n",
        "    1) Mean squared error between the clean and noisy images\n",
        "    2) Total variation of the clean image weighted by lambdaTV\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    noisy_image : torch.Tensor\n",
        "        The input noisy image to be denoised\n",
        "    lambdaTV : float\n",
        "        Weight coefficient for the total variation regularization term\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    forward()\n",
        "        Computes the combined loss: MSE(clean, noisy) + lambdaTV * TV(clean)\n",
        "    get_clean_image()\n",
        "        Returns the optimized clean image tensor\n",
        "    \"\"\"\n",
        "    def __init__(self, noisy_image, lambdaTV):\n",
        "        super(TVDenoise, self).__init__()\n",
        "        self.lambdaTV = lambdaTV\n",
        "        self.l2_term = th.nn.MSELoss(reduction='mean')\n",
        "        self.regularization_term = kornia.losses.TotalVariation()\n",
        "        # create the variable which will be optimized to produce the noise free image\n",
        "        self.clean_image = th.nn.Parameter(data=noisy_image.clone(), requires_grad=True)\n",
        "        self.noisy_image = noisy_image\n",
        "\n",
        "    def forward(self):\n",
        "        return self.l2_term(self.clean_image, self.noisy_image) + self.lambdaTV * self.regularization_term(self.clean_image)\n",
        "\n",
        "    def get_clean_image(self):\n",
        "        return self.clean_image"
      ],
      "id": "cell-3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Total Variation Denoising Function"
      ],
      "id": "0744148e-7530-4e00-a764-5d70ac1e8252"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def denoise_total_variation(x, lambda_TV, ng):\n",
        "  \"\"\"Denoise an image using total variation regularization.\n",
        "\n",
        "  This function takes a noisy image and performs total variation denoising by optimizing\n",
        "  a combination of mean squared error and total variation regularization terms.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : torch.Tensor\n",
        "      Input noisy image tensor to be denoised\n",
        "  lambda_TV : float \n",
        "      Weight coefficient for the total variation regularization term\n",
        "  ng : int\n",
        "      Number of gradient descent iterations to run the optimization\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "      A tuple containing:\n",
        "      - numpy.ndarray: The denoised image\n",
        "      - float: The final loss value after optimization\n",
        "  \"\"\"\n",
        "  # read the image with OpenCV\n",
        "  img = x\n",
        "  # convert to torch tensor\n",
        "  noisy_image = img\n",
        "  # define the total variation denoising network\n",
        "  tv_denoiser = TVDenoise(noisy_image, lambda_TV)\n",
        "  # define the optimizer to optimize the 1 parameter of tv_denoiser\n",
        "  optimizer = th.optim.SGD(tv_denoiser.parameters(), lr=0.1, momentum=0.9)\n",
        "  # run the optimization loop\n",
        "  for i in range(ng):\n",
        "      optimizer.zero_grad()\n",
        "      loss = tv_denoiser()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "  # convert back to numpy\n",
        "  img_clean: np.ndarray = kornia.tensor_to_image(tv_denoiser.get_clean_image())\n",
        "  return img_clean, loss.item()"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Denoise the Chemical Maps with Total Variation Regularization Only"
      ],
      "id": "d606a08f-64a0-4795-8276-6abf4a62db16"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lambda_TV = 5e-2\n",
        "ng = 40\n",
        "\n",
        "device = 'cuda'\n",
        "x0 = th.as_tensor(x0).to(device)\n",
        "x0_den, loss0 = denoise_total_variation(x0[0], lambda_TV, ng)\n",
        "x1_den, loss1 = denoise_total_variation(x0[1], lambda_TV, ng)\n",
        "x2_den, loss2 = denoise_total_variation(x0[2], lambda_TV, ng)\n",
        "fig, ax = plt.subplots(3,2, figsize=(10,15))\n",
        "axs = ax.ravel()\n",
        "title_list = ['x_0','x_0 denoised','x_1','x_1 denoised','x_2','x_2 denoised']\n",
        "for axi, xi, li in zip(axs, [x[0],x0_den,x[1],x1_den,x[2],x2_den], title_list):\n",
        "  axi.imshow(xi, cmap='bone')\n",
        "  axi.set_xticks([])\n",
        "  axi.set_yticks([])\n",
        "  axi.set_title(li) "
      ],
      "id": "cell-7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the HAADF Signal"
      ],
      "id": "3d44976c-c64e-4eb7-9186-6dc5b7350b56"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HAADF Signal (Measurements)\n",
        "b = file[mapNum+'HAADF'] \n",
        "# Data Subtraction and Normalization \n",
        "b -= np.min(b); b /= np.max(b)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(b)\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_title('HAADF image') \n",
        "plt.show()"
      ],
      "id": "cell-9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform Multi-Modal Data Fusion"
      ],
      "id": "f1854427-73c2-4a12-9d97-d3fa5752dd83"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convergence Parameters\n",
        "gamma = 1.6\n",
        "\n",
        "nIter = 10\n",
        "bkg = 1e-3\n",
        "\n",
        "# TV Min Parameters\n",
        "regularize = True\n",
        "ng = 30\n",
        "\n",
        "# Image Dimensions\n",
        "(nx, ny) = edsMap.shape\n",
        "nPix = nx * ny\n",
        "nz = len(elementList)\n",
        "lambdaHAADF = 3e-2\n",
        "lambdaEDS = 5e-6\n",
        "lambdaTV = 5e-3\n",
        "\n",
        "fusion_loss = th.nn.MSELoss() \n",
        "# noise_loss = th.nn.PoissonNLLLoss(log_input=False, eps=bkg)\n",
        "noise_loss = th.nn.MSELoss() \n",
        "\n",
        "#create arrays to store the loss function\n",
        "costHAADF = np.zeros(nIter,dtype=np.float32)\n",
        "costEDS = np.zeros(nIter, dtype=np.float32)\n",
        "costTV = np.zeros(nIter, dtype=np.float32);\n",
        "\n",
        "\n",
        "x0 = th.as_tensor(x0, dtype=th.float32).to(device)\n",
        "b = th.as_tensor(b, dtype=th.float32).to(device)\n",
        "x = th.nn.Parameter(th.clone(x0)).to(device)\n",
        "\n",
        "optimizer = th.optim.SGD([x], lr=1)\n",
        "\n",
        "# Main Loop\n",
        "for kk in tqdm(range(nIter)):\n",
        "    optimizer.zero_grad()\n",
        "    # Add small epsilon to prevent numerical instability\n",
        "    eps = 1e-8\n",
        "    # Clip x values to prevent extreme values\n",
        "    with th.no_grad():\n",
        "        x.clamp_(min=eps)\n",
        "    \n",
        "    # More stable power operation\n",
        "    powered_sum = th.pow(x[0] + eps, gamma) + th.pow(x[1] + eps, gamma) + th.pow(x[2] + eps, gamma)\n",
        "    loss1 = lambdaHAADF * fusion_loss(powered_sum, b)\n",
        "    loss2 = lambdaEDS * noise_loss(x, x0)\n",
        "    loss = loss1 + loss2 \n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Clamp values after optimization step\n",
        "    with th.no_grad():\n",
        "        x.clamp_(min=eps)\n",
        "\n",
        "    # Measure Cost Function\n",
        "    costHAADF[kk] = loss1.item()\n",
        "    costEDS[kk] = loss2.item()\n",
        "\n",
        "    with th.no_grad():\n",
        "        x[x<0] = 1e-2\n",
        "\n",
        "    # Regularization \n",
        "    if regularize:\n",
        "      # print(f\"2 Number of NaN elements in x: {th.isnan(x).sum().item()}\")\n",
        "      \n",
        "      x0_den, loss0 = denoise_total_variation(x[0].detach(), lambda_TV, ng)\n",
        "      x1_den, loss1 = denoise_total_variation(x[1].detach(), lambda_TV, ng)\n",
        "      x2_den, loss2 = denoise_total_variation(x[2].detach(), lambda_TV, ng)\n",
        "      # print(f\"3 Number of NaN elements in x: {th.isnan(x).sum().item()}\")\n",
        "\n",
        "\n",
        "      with th.no_grad():\n",
        "        x[0, ...] = th.as_tensor(x0_den)\n",
        "        x[1, ...] = th.as_tensor(x1_den)\n",
        "        x[2, ...] = th.as_tensor(x2_den)   \n",
        "        # x[x==0] = 1e-2 \n",
        "\n",
        "      \n",
        "\n",
        "      costTV[kk] = loss0 + loss1 + loss2\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12,8))\n",
        "plt.suptitle(f'λHAADF={lambdaHAADF}, λEDS={lambdaEDS}, λTV={lambdaTV}')\n",
        "# Plot original data (x0)\n",
        "for i in range(3):\n",
        "    axes[0,i].imshow(x0[i].cpu().detach().numpy().reshape(nx,ny), cmap='gray')\n",
        "    axes[0,i].set_title(f'Original - Element {i+1}')\n",
        "    axes[0,i].axis('off')\n",
        "\n",
        "# Plot reconstructed data (x) \n",
        "for i in range(3):\n",
        "    axes[1,i].imshow(x[i].data.cpu().detach().numpy(), cmap='gray')\n",
        "    axes[1,i].set_title(f'Reconstructed - Element {i+1}')\n",
        "    axes[1,i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "utils.plot_convergence(costHAADF, lambdaHAADF, costEDS, lambdaEDS, costTV, lambdaTV)"
      ],
      "id": "cell-11"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x.max(), x.min()"
      ],
      "id": "cell-12"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Cost Functions and Descent Parameters"
      ],
      "id": "c4414b32-9394-4c83-b691-f32cb13c811c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.plot_convergence(costHAADF, lambdaHAADF, costEDS, lambdaEDS, costTV, lambdaTV)"
      ],
      "id": "cell-14"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show Reconstructed Signal"
      ],
      "id": "58709be9-ecb7-405b-92bd-c1067dc3dc1e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(12,8))\n",
        "plt.suptitle(f'λHAADF={lambdaHAADF}, λEDS={lambdaEDS}, λTV={lambdaTV}')\n",
        "# Plot original data (x0)\n",
        "for i in range(3):\n",
        "    axes[0,i].imshow(x0[i].cpu().detach().numpy().reshape(nx,ny), cmap='gray')\n",
        "    axes[0,i].set_title(f'Original - Element {i+1}')\n",
        "    axes[0,i].axis('off')\n",
        "\n",
        "# Plot reconstructed data (x) \n",
        "for i in range(3):\n",
        "    axes[1,i].imshow(x[i].cpu().detach().numpy().reshape(nx,ny), cmap='gray')\n",
        "    axes[1,i].set_title(f'Reconstructed - Element {i+1}')\n",
        "    axes[1,i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "cell-16"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  }
}