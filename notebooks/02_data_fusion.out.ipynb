{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regression example - sensor fusion\n",
        "\n",
        "## 02.2 Regression example: Sensor Fusion EDX + HAADF\n",
        "\n",
        "<br>"
      ],
      "id": "b5f11eee-9d4f-4537-ad52-f293ee64a712"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h2>"
      ],
      "id": "97631ca6-e155-4b5a-8abe-9cfc2c7eca13"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Science in Electron Microscopy"
      ],
      "id": "f8113972-8b15-4cc5-a943-8146207e9409"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h2>"
      ],
      "id": "aa6b2bf5-60ed-4daf-be99-bc79a2d5898b"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<hr>"
      ],
      "id": "0ecc79c0-859f-4ccf-8aa1-6d516973f5ca"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h3>"
      ],
      "id": "0a86a0a7-89a6-4348-81d8-d09fb488df60"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Philipp Pelz"
      ],
      "id": "915b246c-7b43-4f1e-a342-230e0acba696"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h3>"
      ],
      "id": "da798e85-c9bd-48bf-9a48-801d235cebef"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h3>"
      ],
      "id": "63ae4d96-da8d-44e2-8b38-ec0a5c73b1cc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2024"
      ],
      "id": "5a12893d-2240-4767-91e7-7b6f560075b5"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h3>"
      ],
      "id": "7ba0ebda-6058-4e38-8265-80ead12d5f4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ],
      "id": "f95fe68e-8098-46a2-8dfc-b15edfda97b7"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h3>"
      ],
      "id": "21bc6369-9e49-4171-b876-f61a5e85a0ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  <https://github.com/ECLIPSE-Lab/WS24_DataScienceForEM>"
      ],
      "id": "552c44fa-a958-47f8-8512-3ce2704dcc34"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h3>"
      ],
      "id": "34fe4cc3-549c-4c97-a3d6-c3f8d4df1b75"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Python modules and Chemical Images"
      ],
      "id": "136da154-73b3-451a-aa5d-b1dc26f33eb2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Data handling\n",
        "import h5py\n",
        "\n",
        "# Deep learning\n",
        "import torch as th\n",
        "import kornia\n",
        "\n",
        "# Image processing\n",
        "import cv2\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Local utilities\n",
        "import fusion_utils as utils\n",
        "\n",
        "## Load Chemical Maps \n",
        "\n",
        "fname = 'CoSX_maps.h5'; mapNum = 'map7/'\n",
        "\n",
        "# Parse Chemical Maps\n",
        "elementList = ['Co', 'O', 'S']\n",
        "\n",
        "# Load Raw Data and Reshape\n",
        "file = h5py.File(fname, 'r')\n",
        "\n",
        "print('Available EDX Maps: ', list(file))\n",
        "\n",
        "x = []\n",
        "for ee in elementList:\n",
        "    # Read Chemical Map for Element \"ee\"\n",
        "    edsMap = file[mapNum+ee][:,:]\n",
        "    # Set Noise Floor to Zero and Normalize Chemical Maps\n",
        "    edsMap -= np.min(edsMap); edsMap /= np.max(edsMap)\n",
        "    # Concatenate Chemical Map to Variable of Interest\n",
        "    x.append(edsMap)\n",
        "x0 = np.stack(x)\n",
        "print(f'x0 shape {x0.shape}')"
      ],
      "id": "cell-1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Total Variation Denoiser Module"
      ],
      "id": "030dab4f-51b4-4745-a172-b95ef0fa09bc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TVDenoise(th.nn.Module):\n",
        "    \"\"\"Total variation denoising implemented as a neural network module.\n",
        "\n",
        "    This class implements an optimization-based denoising approach that minimizes\n",
        "    a combination of:\n",
        "    1) Mean squared error between the clean and noisy images\n",
        "    2) Total variation of the clean image weighted by lambdaTV\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    noisy_image : torch.Tensor\n",
        "        The input noisy image to be denoised\n",
        "    lambdaTV : float\n",
        "        Weight coefficient for the total variation regularization term\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    forward()\n",
        "        Computes the combined loss: MSE(clean, noisy) + lambdaTV * TV(clean)\n",
        "    get_clean_image()\n",
        "        Returns the optimized clean image tensor\n",
        "    \"\"\"\n",
        "    def __init__(self, noisy_image, lambdaTV):\n",
        "        super(TVDenoise, self).__init__()\n",
        "        self.lambdaTV = lambdaTV\n",
        "        self.l2_term = th.nn.MSELoss(reduction='mean')\n",
        "        self.regularization_term = kornia.losses.TotalVariation()\n",
        "        # create the variable which will be optimized to produce the noise free image\n",
        "        self.clean_image = th.nn.Parameter(data=noisy_image.clone(), requires_grad=True)\n",
        "        self.noisy_image = noisy_image\n",
        "\n",
        "    def forward(self):\n",
        "        return self.l2_term(self.clean_image, self.noisy_image) + self.lambdaTV * self.regularization_term(self.clean_image)\n",
        "\n",
        "    def get_clean_image(self):\n",
        "        return self.clean_image"
      ],
      "id": "cell-3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Total Variation Denoising Function"
      ],
      "id": "9410481d-8718-49d0-9590-928873e4d214"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def denoise_total_variation(x, lambda_TV, ng):\n",
        "  \"\"\"Denoise an image using total variation regularization.\n",
        "\n",
        "  This function takes a noisy image and performs total variation denoising by optimizing\n",
        "  a combination of mean squared error and total variation regularization terms.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : torch.Tensor\n",
        "      Input noisy image tensor to be denoised\n",
        "  lambda_TV : float \n",
        "      Weight coefficient for the total variation regularization term\n",
        "  ng : int\n",
        "      Number of gradient descent iterations to run the optimization\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "      A tuple containing:\n",
        "      - numpy.ndarray: The denoised image\n",
        "      - float: The final loss value after optimization\n",
        "  \"\"\"\n",
        "  # read the image with OpenCV\n",
        "  img = x\n",
        "  # convert to torch tensor\n",
        "  noisy_image = img\n",
        "  # define the total variation denoising network\n",
        "  tv_denoiser = TVDenoise(noisy_image, lambda_TV)\n",
        "  # define the optimizer to optimize the 1 parameter of tv_denoiser\n",
        "  optimizer = th.optim.SGD(tv_denoiser.parameters(), lr=0.1, momentum=0.9)\n",
        "  # run the optimization loop\n",
        "  for i in range(ng):\n",
        "      optimizer.zero_grad()\n",
        "      loss = tv_denoiser()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "  # convert back to numpy\n",
        "  img_clean: np.ndarray = kornia.tensor_to_image(tv_denoiser.get_clean_image())\n",
        "  return img_clean, loss.item()"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Denoise the Chemical Maps with Total Variation Regularization Only"
      ],
      "id": "907d4f49-2a4d-45a7-b9a5-d6fa2f6a1814"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lambda_TV = 5e-2\n",
        "ng = 40\n",
        "\n",
        "device = 'cuda'\n",
        "x0 = th.as_tensor(x0).to(device)\n",
        "x0_den, loss0 = denoise_total_variation(x0[0], lambda_TV, ng)\n",
        "x1_den, loss1 = denoise_total_variation(x0[1], lambda_TV, ng)\n",
        "x2_den, loss2 = denoise_total_variation(x0[2], lambda_TV, ng)\n",
        "fig, ax = plt.subplots(3,2, figsize=(10,15))\n",
        "axs = ax.ravel()\n",
        "title_list = ['x_0','x_0 denoised','x_1','x_1 denoised','x_2','x_2 denoised']\n",
        "for axi, xi, li in zip(axs, [x[0],x0_den,x[1],x1_den,x[2],x2_den], title_list):\n",
        "  axi.imshow(xi, cmap='bone')\n",
        "  axi.set_xticks([])\n",
        "  axi.set_yticks([])\n",
        "  axi.set_title(li) "
      ],
      "id": "cell-7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the HAADF Signal"
      ],
      "id": "3016cbc3-59af-4daa-b2fc-02e63f29f860"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HAADF Signal (Measurements)\n",
        "b = file[mapNum+'HAADF'] \n",
        "# Data Subtraction and Normalization \n",
        "b -= np.min(b); b /= np.max(b)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(b)\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_title('HAADF image') \n",
        "plt.show()"
      ],
      "id": "cell-9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform Multi-Modal Data Fusion"
      ],
      "id": "349508f7-8135-496f-9fff-91889db729f8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convergence Parameters\n",
        "gamma = 1.6\n",
        "\n",
        "nIter = 10\n",
        "bkg = 1e-3\n",
        "\n",
        "# TV Min Parameters\n",
        "regularize = True\n",
        "ng = 30\n",
        "\n",
        "# Image Dimensions\n",
        "(nx, ny) = edsMap.shape\n",
        "nPix = nx * ny\n",
        "nz = len(elementList)\n",
        "lambdaHAADF = 3e-2\n",
        "lambdaEDS = 5e-6\n",
        "lambdaTV = 5e-3\n",
        "\n",
        "fusion_loss = th.nn.MSELoss() \n",
        "# noise_loss = th.nn.PoissonNLLLoss(log_input=False, eps=bkg)\n",
        "noise_loss = th.nn.MSELoss() \n",
        "\n",
        "#create arrays to store the loss function\n",
        "costHAADF = np.zeros(nIter,dtype=np.float32)\n",
        "costEDS = np.zeros(nIter, dtype=np.float32)\n",
        "costTV = np.zeros(nIter, dtype=np.float32);\n",
        "\n",
        "\n",
        "x0 = th.as_tensor(x0, dtype=th.float32).to(device)\n",
        "b = th.as_tensor(b, dtype=th.float32).to(device)\n",
        "x = th.nn.Parameter(th.clone(x0)).to(device)\n",
        "\n",
        "optimizer = th.optim.SGD([x], lr=1)\n",
        "\n",
        "# Main Loop\n",
        "for kk in tqdm(range(nIter)):\n",
        "    optimizer.zero_grad()\n",
        "    # Add small epsilon to prevent numerical instability\n",
        "    eps = 1e-8\n",
        "    # Clip x values to prevent extreme values\n",
        "    with th.no_grad():\n",
        "        x.clamp_(min=eps)\n",
        "    \n",
        "    # More stable power operation\n",
        "    powered_sum = th.pow(x[0] + eps, gamma) + th.pow(x[1] + eps, gamma) + th.pow(x[2] + eps, gamma)\n",
        "    loss1 = lambdaHAADF * fusion_loss(powered_sum, b)\n",
        "    loss2 = lambdaEDS * noise_loss(x, x0)\n",
        "    loss = loss1 + loss2 \n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Clamp values after optimization step\n",
        "    with th.no_grad():\n",
        "        x.clamp_(min=eps)\n",
        "\n",
        "    # Measure Cost Function\n",
        "    costHAADF[kk] = loss1.item()\n",
        "    costEDS[kk] = loss2.item()\n",
        "\n",
        "    with th.no_grad():\n",
        "        x[x<0] = 1e-2\n",
        "\n",
        "    # Regularization \n",
        "    if regularize:\n",
        "      # print(f\"2 Number of NaN elements in x: {th.isnan(x).sum().item()}\")\n",
        "      \n",
        "      x0_den, loss0 = denoise_total_variation(x[0].detach(), lambda_TV, ng)\n",
        "      x1_den, loss1 = denoise_total_variation(x[1].detach(), lambda_TV, ng)\n",
        "      x2_den, loss2 = denoise_total_variation(x[2].detach(), lambda_TV, ng)\n",
        "      # print(f\"3 Number of NaN elements in x: {th.isnan(x).sum().item()}\")\n",
        "\n",
        "\n",
        "      with th.no_grad():\n",
        "        x[0, ...] = th.as_tensor(x0_den)\n",
        "        x[1, ...] = th.as_tensor(x1_den)\n",
        "        x[2, ...] = th.as_tensor(x2_den)   \n",
        "        # x[x==0] = 1e-2 \n",
        "\n",
        "      \n",
        "\n",
        "      costTV[kk] = loss0 + loss1 + loss2\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12,8))\n",
        "plt.suptitle(f'λHAADF={lambdaHAADF}, λEDS={lambdaEDS}, λTV={lambdaTV}')\n",
        "# Plot original data (x0)\n",
        "for i in range(3):\n",
        "    axes[0,i].imshow(x0[i].cpu().detach().numpy().reshape(nx,ny), cmap='gray')\n",
        "    axes[0,i].set_title(f'Original - Element {i+1}')\n",
        "    axes[0,i].axis('off')\n",
        "\n",
        "# Plot reconstructed data (x) \n",
        "for i in range(3):\n",
        "    axes[1,i].imshow(x[i].data.cpu().detach().numpy(), cmap='gray')\n",
        "    axes[1,i].set_title(f'Reconstructed - Element {i+1}')\n",
        "    axes[1,i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "utils.plot_convergence(costHAADF, lambdaHAADF, costEDS, lambdaEDS, costTV, lambdaTV)"
      ],
      "id": "cell-11"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x.max(), x.min()"
      ],
      "id": "cell-12"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Cost Functions and Descent Parameters"
      ],
      "id": "bbaa7ea7-6d42-417e-8495-0f05953de5fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.plot_convergence(costHAADF, lambdaHAADF, costEDS, lambdaEDS, costTV, lambdaTV)"
      ],
      "id": "cell-14"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show Reconstructed Signal"
      ],
      "id": "dc5472de-6939-4e6f-8094-99b207e4cbca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(12,8))\n",
        "plt.suptitle(f'λHAADF={lambdaHAADF}, λEDS={lambdaEDS}, λTV={lambdaTV}')\n",
        "# Plot original data (x0)\n",
        "for i in range(3):\n",
        "    axes[0,i].imshow(x0[i].cpu().detach().numpy().reshape(nx,ny), cmap='gray')\n",
        "    axes[0,i].set_title(f'Original - Element {i+1}')\n",
        "    axes[0,i].axis('off')\n",
        "\n",
        "# Plot reconstructed data (x) \n",
        "for i in range(3):\n",
        "    axes[1,i].imshow(x[i].cpu().detach().numpy().reshape(nx,ny), cmap='gray')\n",
        "    axes[1,i].set_title(f'Reconstructed - Element {i+1}')\n",
        "    axes[1,i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "cell-16"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  }
}