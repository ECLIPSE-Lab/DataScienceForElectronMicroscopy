{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regression example - sensor fusion\n",
        "\n",
        "## Regression example: Sensor Fusion EDX + HAADF\n",
        "\n",
        "<br>"
      ],
      "id": "79114f73-eefa-482f-af29-7c7227191f16"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h2>"
      ],
      "id": "afafe602-a6a8-41fa-92d2-1e4ecd71c25d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Science in Electron Microscopy"
      ],
      "id": "8680511e-4279-4b72-9c0f-314f9e7081b4"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h2>"
      ],
      "id": "1cc9b091-e845-419d-bb99-16f8ae1e2d7a"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<hr>"
      ],
      "id": "0ffa1bb8-21b0-4460-864c-8d93df6267a9"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h3>"
      ],
      "id": "d3b8ad7a-5df6-4afc-b2f2-738069958eb8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Philipp Pelz"
      ],
      "id": "096a9055-b32f-4b94-b4d3-2a9c3387592c"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h3>"
      ],
      "id": "57c7a75a-58fe-4be3-9f6f-ca3bf6439301"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h3>"
      ],
      "id": "12a1f3c0-8d70-4c78-908f-d43679b8a225"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2024"
      ],
      "id": "16656cba-0191-44a7-8265-4d2d7f779290"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h3>"
      ],
      "id": "e755fa5e-958d-444f-9e2d-be6655d1418a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ],
      "id": "1d5b7509-8d69-4e8b-8db4-de90190d96ca"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<h3>"
      ],
      "id": "29428d49-bc76-4b32-b93c-85dac390951d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Â  <https://github.com/ECLIPSE-Lab/WS24_DataScienceForEM>"
      ],
      "id": "451122b5-5911-44b9-aa13-c168526c86fd"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</h3>"
      ],
      "id": "9ecb0802-9a7b-4944-bfb1-8267a9f42158"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Python modules and Chemical Images"
      ],
      "id": "176fa09e-7278-4124-bd90-1ce3b35bda87"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard libraries\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Data handling\n",
        "import h5py\n",
        "\n",
        "# Deep learning\n",
        "import torch as th\n",
        "import kornia\n",
        "\n",
        "# Image processing\n",
        "import cv2\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Local utilities\n",
        "import fusion_utils as utils\n",
        "\n",
        "## Load Chemical Maps \n",
        "\n",
        "fname = 'CoSX_maps.h5'; mapNum = 'map7/'\n",
        "\n",
        "# Parse Chemical Maps\n",
        "elementList = ['Co', 'O', 'S']\n",
        "\n",
        "# Load Raw Data and Reshape\n",
        "file = h5py.File(fname, 'r')\n",
        "\n",
        "print('Available EDX Maps: ', list(file))\n",
        "\n",
        "x = []\n",
        "for ee in elementList:\n",
        "    # Read Chemical Map for Element \"ee\"\n",
        "    edsMap = file[mapNum+ee][:,:]\n",
        "    # Set Noise Floor to Zero and Normalize Chemical Maps\n",
        "    edsMap -= np.min(edsMap); edsMap /= np.max(edsMap)\n",
        "    # Concatenate Chemical Map to Variable of Interest\n",
        "    x.append(edsMap)\n",
        "x0 = np.stack(x)\n",
        "print(f'x0 shape {x0.shape}')"
      ],
      "id": "cell-1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Total Variation Denoiser Module"
      ],
      "id": "7f15938e-490f-4d55-be4c-db883a39de3c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TVDenoise(th.nn.Module):\n",
        "    \"\"\"Total variation denoising implemented as a neural network module.\n",
        "\n",
        "    This class implements an optimization-based denoising approach that minimizes\n",
        "    a combination of:\n",
        "    1) Mean squared error between the clean and noisy images\n",
        "    2) Total variation of the clean image weighted by lambdaTV\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    noisy_image : torch.Tensor\n",
        "        The input noisy image to be denoised\n",
        "    lambdaTV : float\n",
        "        Weight coefficient for the total variation regularization term\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    forward()\n",
        "        Computes the combined loss: MSE(clean, noisy) + lambdaTV * TV(clean)\n",
        "    get_clean_image()\n",
        "        Returns the optimized clean image tensor\n",
        "    \"\"\"\n",
        "    def __init__(self, noisy_image, lambdaTV):\n",
        "        super(TVDenoise, self).__init__()\n",
        "        self.lambdaTV = lambdaTV\n",
        "        self.l2_term = th.nn.MSELoss(reduction='mean')\n",
        "        self.regularization_term = kornia.losses.TotalVariation()\n",
        "        # create the variable which will be optimized to produce the noise free image\n",
        "        self.clean_image = th.nn.Parameter(data=noisy_image.clone(), requires_grad=True)\n",
        "        self.noisy_image = noisy_image\n",
        "\n",
        "    def forward(self):\n",
        "        return self.l2_term(self.clean_image, self.noisy_image) + self.lambdaTV * self.regularization_term(self.clean_image)\n",
        "\n",
        "    def get_clean_image(self):\n",
        "        return self.clean_image"
      ],
      "id": "cell-3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the Total Variation Denoising Function"
      ],
      "id": "1c4a8ec5-3e89-4fe9-b84d-e27afab4c0a3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def denoise_total_variation(x, lambda_TV, ng):\n",
        "  \"\"\"Denoise an image using total variation regularization.\n",
        "\n",
        "  This function takes a noisy image and performs total variation denoising by optimizing\n",
        "  a combination of mean squared error and total variation regularization terms.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : torch.Tensor\n",
        "      Input noisy image tensor to be denoised\n",
        "  lambda_TV : float \n",
        "      Weight coefficient for the total variation regularization term\n",
        "  ng : int\n",
        "      Number of gradient descent iterations to run the optimization\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "      A tuple containing:\n",
        "      - numpy.ndarray: The denoised image\n",
        "      - float: The final loss value after optimization\n",
        "  \"\"\"\n",
        "  # read the image with OpenCV\n",
        "  img = x\n",
        "  # convert to torch tensor\n",
        "  noisy_image = img\n",
        "  # define the total variation denoising network\n",
        "  tv_denoiser = TVDenoise(noisy_image, lambda_TV)\n",
        "  # define the optimizer to optimize the 1 parameter of tv_denoiser\n",
        "  optimizer = th.optim.SGD(tv_denoiser.parameters(), lr=0.1, momentum=0.9)\n",
        "  # run the optimization loop\n",
        "  for i in range(ng):\n",
        "      optimizer.zero_grad()\n",
        "      loss = tv_denoiser()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "  # convert back to numpy\n",
        "  img_clean: np.ndarray = kornia.tensor_to_image(tv_denoiser.get_clean_image())\n",
        "  return img_clean, loss.item()"
      ],
      "id": "cell-5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Denoise the Chemical Maps with Total Variation Regularization Only"
      ],
      "id": "5d1f0477-8f8f-4854-93de-4d46f620170e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lambda_TV = 5e-2\n",
        "ng = 40\n",
        "\n",
        "device = 'cuda'\n",
        "x0 = th.as_tensor(x0).to(device)\n",
        "x0_den, loss0 = denoise_total_variation(x0[0], lambda_TV, ng)\n",
        "x1_den, loss1 = denoise_total_variation(x0[1], lambda_TV, ng)\n",
        "x2_den, loss2 = denoise_total_variation(x0[2], lambda_TV, ng)\n",
        "fig, ax = plt.subplots(3,2, figsize=(10,15))\n",
        "axs = ax.ravel()\n",
        "title_list = ['x_0','x_0 denoised','x_1','x_1 denoised','x_2','x_2 denoised']\n",
        "for axi, xi, li in zip(axs, [x[0],x0_den,x[1],x1_den,x[2],x2_den], title_list):\n",
        "  axi.imshow(xi, cmap='bone')\n",
        "  axi.set_xticks([])\n",
        "  axi.set_yticks([])\n",
        "  axi.set_title(li) "
      ],
      "id": "cell-7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the HAADF Signal"
      ],
      "id": "297cb40c-f01b-4c5f-8da7-8bb4ef23ef3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HAADF Signal (Measurements)\n",
        "b = file[mapNum+'HAADF'] \n",
        "# Data Subtraction and Normalization \n",
        "b -= np.min(b); b /= np.max(b)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.imshow(b)\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "ax.set_title('HAADF image') \n",
        "plt.show()"
      ],
      "id": "cell-9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform Multi-Modal Data Fusion"
      ],
      "id": "aac21403-9bf9-4f3d-bb1c-63f7e07375fd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convergence Parameters\n",
        "gamma = 1.6\n",
        "\n",
        "nIter = 10\n",
        "bkg = 1e-3\n",
        "\n",
        "# TV Min Parameters\n",
        "regularize = True\n",
        "ng = 30\n",
        "\n",
        "# Image Dimensions\n",
        "(nx, ny) = edsMap.shape\n",
        "nPix = nx * ny\n",
        "nz = len(elementList)\n",
        "lambdaHAADF = 3e-2\n",
        "lambdaEDS = 5e-6\n",
        "lambdaTV = 5e-3\n",
        "\n",
        "fusion_loss = th.nn.MSELoss() \n",
        "# noise_loss = th.nn.PoissonNLLLoss(log_input=False, eps=bkg)\n",
        "noise_loss = th.nn.MSELoss() \n",
        "\n",
        "#create arrays to store the loss function\n",
        "costHAADF = np.zeros(nIter,dtype=np.float32)\n",
        "costEDS = np.zeros(nIter, dtype=np.float32)\n",
        "costTV = np.zeros(nIter, dtype=np.float32);\n",
        "\n",
        "\n",
        "x0 = th.as_tensor(x0, dtype=th.float32).to(device)\n",
        "b = th.as_tensor(b, dtype=th.float32).to(device)\n",
        "x = th.nn.Parameter(th.clone(x0)).to(device)\n",
        "\n",
        "optimizer = th.optim.SGD([x], lr=1)\n",
        "\n",
        "# Main Loop\n",
        "for kk in tqdm(range(nIter)):\n",
        "    optimizer.zero_grad()\n",
        "    # Add small epsilon to prevent numerical instability\n",
        "    eps = 1e-8\n",
        "    # Clip x values to prevent extreme values\n",
        "    with th.no_grad():\n",
        "        x.clamp_(min=eps)\n",
        "    \n",
        "    # More stable power operation\n",
        "    powered_sum = th.pow(x[0] + eps, gamma) + th.pow(x[1] + eps, gamma) + th.pow(x[2] + eps, gamma)\n",
        "    loss1 = lambdaHAADF * fusion_loss(powered_sum, b)\n",
        "    loss2 = lambdaEDS * noise_loss(x, x0)\n",
        "    loss = loss1 + loss2 \n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Clamp values after optimization step\n",
        "    with th.no_grad():\n",
        "        x.clamp_(min=eps)\n",
        "\n",
        "    # Measure Cost Function\n",
        "    costHAADF[kk] = loss1.item()\n",
        "    costEDS[kk] = loss2.item()\n",
        "\n",
        "    with th.no_grad():\n",
        "        x[x<0] = 1e-2\n",
        "\n",
        "    # Regularization \n",
        "    if regularize:\n",
        "      # print(f\"2 Number of NaN elements in x: {th.isnan(x).sum().item()}\")\n",
        "      \n",
        "      x0_den, loss0 = denoise_total_variation(x[0].detach(), lambda_TV, ng)\n",
        "      x1_den, loss1 = denoise_total_variation(x[1].detach(), lambda_TV, ng)\n",
        "      x2_den, loss2 = denoise_total_variation(x[2].detach(), lambda_TV, ng)\n",
        "      # print(f\"3 Number of NaN elements in x: {th.isnan(x).sum().item()}\")\n",
        "\n",
        "\n",
        "      with th.no_grad():\n",
        "        x[0, ...] = th.as_tensor(x0_den)\n",
        "        x[1, ...] = th.as_tensor(x1_den)\n",
        "        x[2, ...] = th.as_tensor(x2_den)   \n",
        "        # x[x==0] = 1e-2 \n",
        "\n",
        "      \n",
        "\n",
        "      costTV[kk] = loss0 + loss1 + loss2\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(12,8))\n",
        "plt.suptitle(f'Î»HAADF={lambdaHAADF}, Î»EDS={lambdaEDS}, Î»TV={lambdaTV}')\n",
        "# Plot original data (x0)\n",
        "for i in range(3):\n",
        "    axes[0,i].imshow(x0[i].cpu().detach().numpy().reshape(nx,ny), cmap='gray')\n",
        "    axes[0,i].set_title(f'Original - Element {i+1}')\n",
        "    axes[0,i].axis('off')\n",
        "\n",
        "# Plot reconstructed data (x) \n",
        "for i in range(3):\n",
        "    axes[1,i].imshow(x[i].data.cpu().detach().numpy(), cmap='gray')\n",
        "    axes[1,i].set_title(f'Reconstructed - Element {i+1}')\n",
        "    axes[1,i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "utils.plot_convergence(costHAADF, lambdaHAADF, costEDS, lambdaEDS, costTV, lambdaTV)"
      ],
      "id": "cell-11"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x.max(), x.min()"
      ],
      "id": "cell-12"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Cost Functions and Descent Parameters"
      ],
      "id": "a720daff-fd79-4658-a292-0372d58f9885"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "utils.plot_convergence(costHAADF, lambdaHAADF, costEDS, lambdaEDS, costTV, lambdaTV)"
      ],
      "id": "cell-14"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show Reconstructed Signal"
      ],
      "id": "9d4d5b06-b87a-46bc-8b72-39c5193569f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(12,8))\n",
        "plt.suptitle(f'Î»HAADF={lambdaHAADF}, Î»EDS={lambdaEDS}, Î»TV={lambdaTV}')\n",
        "# Plot original data (x0)\n",
        "for i in range(3):\n",
        "    axes[0,i].imshow(x0[i].cpu().detach().numpy().reshape(nx,ny), cmap='gray')\n",
        "    axes[0,i].set_title(f'Original - Element {i+1}')\n",
        "    axes[0,i].axis('off')\n",
        "\n",
        "# Plot reconstructed data (x) \n",
        "for i in range(3):\n",
        "    axes[1,i].imshow(x[i].cpu().detach().numpy().reshape(nx,ny), cmap='gray')\n",
        "    axes[1,i].set_title(f'Reconstructed - Element {i+1}')\n",
        "    axes[1,i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "cell-16"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    }
  }
}