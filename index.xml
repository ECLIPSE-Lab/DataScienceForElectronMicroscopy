<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving
and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
  <front>
    <journal-meta>
      <journal-id/>
      <journal-title-group>
        <journal-title>Friedrich-Alexander Universitaet
Erlangen-Nuernberg</journal-title>
      </journal-title-group>
      <issn/>
      <publisher>
        <publisher-name/>
      </publisher>
    </journal-meta>
    <article-meta>
      <title-group>
        <article-title>Lecture Data Science for Electron Microscopy Winter
2024</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <contrib-id contrib-id-type="orcid">0000-0002-8009-4515</contrib-id>
          <name>
            <surname>Pelz</surname>
            <given-names>Philipp</given-names>
          </name>
          <string-name>Philipp Pelz</string-name>
          <email>philipp.pelz@fau.de</email>
          <role vocab="https://credit.niso.org" vocab-term="investigation" vocab-term-identifier="https://credit.niso.org/contributor-roles/investigation/">Investigation</role>
          <role vocab="https://credit.niso.org" vocab-term="project administration" vocab-term-identifier="https://credit.niso.org/contributor-roles/project-administration/">Project
administration</role>
          <role vocab="https://credit.niso.org" vocab-term="software" vocab-term-identifier="https://credit.niso.org/contributor-roles/software/">Software</role>
          <role vocab="https://credit.niso.org" vocab-term="visualization" vocab-term-identifier="https://credit.niso.org/contributor-roles/visualization/">Visualization</role>
          <xref ref-type="aff" rid="aff-1">a</xref>
          <xref ref-type="corresp" rid="cor-1">*</xref>
        </contrib>
      </contrib-group>
      <aff id="aff-1">
        <institution-wrap>
          <institution>FAU Erlangen-Nuernberg</institution>
        </institution-wrap>
      </aff>
      <author-notes>
        <corresp id="cor-1">philipp.pelz@fau.de</corresp>
      </author-notes>
      <pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-11-15">
        <year>2024</year>
        <month>11</month>
        <day>15</day>
      </pub-date>
      <history/>
      <abstract>
        <p>This is the website for the Data Science for Electron Microscopy
Lecture</p>
      </abstract>
      <kwd-group kwd-group-type="author">
        <kwd>Data Science</kwd>
        <kwd>Electron Microscopy</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <list list-type="bullet">
      <list-item>
        <p>
          <ext-link ext-link-type="uri" xlink:href="https://pelzlab.science">Pelz
    Lab website</ext-link>
        </p>
      </list-item>
      <list-item>
        <p>
          <ext-link ext-link-type="uri" xlink:href="https://www.studon.fau.de/campo/course/421992">Studon
    Link</ext-link>
        </p>
      </list-item>
    </list>
    <sec id="lecture-1-intro-25.10.2024">
      <title>1 Lecture 1: Intro (25.10.2024)</title>
      <list list-type="bullet">
        <list-item>
          <p>Introduction</p>
        </list-item>
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://d2l.ai/chapter_preliminaries/index.html">d2l
      Chapter 2: Preliminaries</ext-link>
          </p>
        </list-item>
      </list>
    </sec>
    <sec id="sec-lecture2">
      <title>2 Lecture 2: Regression and Sensor Fusion (8.11.2024)</title>
      <list list-type="bullet">
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://d2l.ai/chapter_linear-regression/index.html">d2l
      Chapter 3: Regression</ext-link>
          </p>
        </list-item>
        <list-item>
          <p>Sensor Fusion Slides</p>
        </list-item>
      </list>
    </sec>
    <sec id="sec-lecture3">
      <title>3 Lecture 3: CNNs (15.11.2024)</title>
      <list list-type="bullet">
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://d2l.ai/chapter_convolutional-neural-networks/index.html">d2l
      Chapter 7: CNNs</ext-link>
          </p>
        </list-item>
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://d2l.ai/chapter_convolutional-modern/index.html">d2l
      Chapter 8: CNNs</ext-link>
          </p>
        </list-item>
      </list>
    </sec>
    <sec id="sec-lecture4">
      <title>4 Lecture 4: Classification, Segmentation, AutoEncoders
  (22.11.2024)</title>
      <list list-type="bullet">
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://d2l.ai/chapter_linear-classification/index.html">d2l
      Chapter 4: Classification</ext-link>
          </p>
        </list-item>
        <list-item>
          <p>
            <ext-link ext-link-type="uri" xlink:href="https://d2l.ai/chapter_computer-vision/semantic-segmentation-and-dataset.html">d2l
      Chapter 14.9: Segmentation</ext-link>
          </p>
        </list-item>
        <list-item>
          <p>Segmentation</p>
        </list-item>
        <list-item>
          <p>Dimensionality Reduction</p>
          <list list-type="bullet">
            <list-item>
              <p>PCA</p>
            </list-item>
            <list-item>
              <p>Autoencoder</p>
            </list-item>
            <list-item>
              <p>Variational Autoencoder</p>
            </list-item>
          </list>
        </list-item>
      </list>
    </sec>
    <sec id="sec-lecture5">
      <title>5 Miniproject (29.11. - 13.12.2024)</title>
      <list list-type="order">
        <list-item>
          <p>Segmentation</p>
        </list-item>
        <list-item>
          <p>VAE &amp; Dimensionality Reduction</p>
        </list-item>
        <list-item>
          <p>Denoising</p>
        </list-item>
        <list-item>
          <p>Image-to-Image Translation</p>
        </list-item>
      </list>
    </sec>
    <sec id="sec-lecture6">
      <title>6 Lecture 5: Mixed Bag (10.1.2025)</title>
      <list list-type="bullet">
        <list-item>
          <p>Project presentation</p>
        </list-item>
        <list-item>
          <p>Generative Adversarial Networks</p>
        </list-item>
        <list-item>
          <p>Gaussian Processes 1</p>
        </list-item>
      </list>
    </sec>
    <sec id="sec-lecture7">
      <title>7 Lecture 6: GPs (17.1.2025)</title>
    </sec>
    <sec id="sec-lecture8">
      <title>8 Lecture 7: Bayesian Optimization, Active Learning, Deep
  Kernel Learning (24.1.2025)</title>
    </sec>
    <sec id="sec-lecture9">
      <title>9 Lecture 8: Inverse Imaging Problems 1: Tomography,
  Deconvolution (31.1.2025)</title>
    </sec>
    <sec id="sec-lecture10">
      <title>10 Lecture 9: Inverse Imaging Problems 2: Phase Contrast
  Imaging, Superresolution Imaging (7.2.2025)</title>
    </sec>
  </body>
  <back>
</back>
  <sub-article article-type="notebook" id="nb-2-nb-1">
    <front-stub>
      <title-group>
        <article-title>01.3 Automatic Differentiation</article-title>
      </title-group>
    </front-stub>
    <body>
      <sec id="cell-4144c129-nb-1" specific-use="notebook-content">
        <p>Recall from Section Calculus that calculating derivatives is the
crucial step in all the optimization algorithms that we will use to
train deep networks. While the calculations are straightforward, working
them out by hand can be tedious and error-prone, and these issues only
grow as our models become more complex.</p>
        <p>Fortunately all modern deep learning frameworks take this work off
our plates by offering <italic>automatic differentiation</italic> (often
shortened to <italic>autograd</italic>). As we pass data through each
successive function, the framework builds a <italic>computational
graph</italic> that tracks how each value depends on others. To
calculate derivatives, automatic differentiation works backwards through
this graph applying the chain rule. The computational algorithm for
applying the chain rule in this fashion is called
<italic>backpropagation</italic>.</p>
        <p>While autograd libraries have become a hot concern over the past
decade, they have a long history. In fact the earliest references to
autograd date back over half of a century
:cite:<monospace>Wengert.1964</monospace>. The core ideas behind modern
backpropagation date to a PhD thesis from 1980
:cite:<monospace>Speelpenning.1980</monospace> and were further
developed in the late 1980s :cite:<monospace>Griewank.1989</monospace>.
While backpropagation has become the default method for computing
gradients, it is not the only option. For instance, the Julia
programming language employs forward propagation
:cite:<monospace>Revels.Lubin.Papamarkou.2016</monospace>. Before
exploring methods, let‚Äôs first master the autograd package.</p>
      </sec>
      <sec id="cell-130439cd-nb-1" specific-use="notebook-content">
        <code language="python">import torch</code>
      </sec>
      <sec id="e2ab3850-nb-1" specific-use="notebook-content">
        <sec id="a-simple-function-nb-1">
          <title>A Simple Function</title>
          <p>Let‚Äôs assume that we are interested in (<bold>differentiating the
  function <inline-formula><alternatives><tex-math><![CDATA[y = 2\mathbf{x}^{\top}\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>ùê±</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùê±</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  with respect to the column vector <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>.</bold>)
  To start, we assign <monospace>x</monospace> an initial value.</p>
        </sec>
        <sec id="cell-4253cfab-nb-1" specific-use="notebook-content">
          <code language="python">x = torch.arange(4.0)
x</code>
          <boxed-text>
            <preformat>tensor([0., 1., 2., 3.])</preformat>
          </boxed-text>
        </sec>
        <sec id="e75614b0-nb-1" specific-use="notebook-content">
          <p>[<bold>Before we calculate the gradient of
  <inline-formula><alternatives><tex-math><![CDATA[y]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>
  with respect to <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>,
  we need a place to store it.</bold>] In general, we avoid allocating
  new memory every time we take a derivative because deep learning
  requires successively computing derivatives with respect to the same
  parameters a great many times, and we might risk running out of
  memory. Note that the gradient of a scalar-valued function with
  respect to a vector <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>
  is vector-valued with the same shape as <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>.</p>
        </sec>
        <sec id="cell-2a001d1e-nb-1" specific-use="notebook-content">
          <code language="python"># Can also create x = torch.arange(4.0, requires_grad=True)
x.requires_grad_(True)
x.grad  # The gradient is None by default</code>
        </sec>
        <sec id="cell-2e74bc02-nb-1" specific-use="notebook-content">
          <p>(<bold>We now calculate our function of <monospace>x</monospace>
  and assign the result to <monospace>y</monospace>.</bold>)</p>
        </sec>
        <sec id="cell-6e3bd777-nb-1" specific-use="notebook-content">
          <code language="python">y = 2 * torch.dot(x, x)
y</code>
          <boxed-text>
            <preformat>tensor(28., grad_fn=&lt;MulBackward0&gt;)</preformat>
          </boxed-text>
        </sec>
        <sec id="c3067490-nb-1" specific-use="notebook-content">
          <p>[<bold>We can now take the gradient of <monospace>y</monospace>
  with respect to <monospace>x</monospace></bold>] by calling its
  <monospace>backward</monospace> method. Next, we can access the
  gradient via <monospace>x</monospace>‚Äôs <monospace>grad</monospace>
  attribute.</p>
        </sec>
        <sec id="cell-21b134ae-nb-1" specific-use="notebook-content">
          <code language="python">y.backward()
x.grad</code>
          <boxed-text>
            <preformat>tensor([ 0.,  4.,  8., 12.])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-17d1390b-nb-1" specific-use="notebook-content">
          <p>(<bold>We already know that the gradient of the function
  <inline-formula><alternatives><tex-math><![CDATA[y = 2\mathbf{x}^{\top}\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>ùê±</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùê±</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  with respect to <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>
  should be <inline-formula><alternatives><tex-math><![CDATA[4\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>4</mml:mn><mml:mi>ùê±</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</bold>)
  We can now verify that the automatic gradient computation and the
  expected result are identical.</p>
        </sec>
        <sec id="cell-5030e37d-nb-1" specific-use="notebook-content">
          <code language="python">x.grad == 4 * x</code>
          <boxed-text>
            <preformat>tensor([True, True, True, True])</preformat>
          </boxed-text>
        </sec>
        <sec id="da440e48-nb-1" specific-use="notebook-content">
          <p>[<bold>Now let‚Äôs calculate another function of
  <monospace>x</monospace> and take its gradient.</bold>] Note that
  PyTorch does not automatically reset the gradient buffer when we
  record a new gradient. Instead, the new gradient is added to the
  already-stored gradient. This behavior comes in handy when we want to
  optimize the sum of multiple objective functions. To reset the
  gradient buffer, we can call <monospace>x.grad.zero_()</monospace> as
  follows:</p>
        </sec>
        <sec id="add5cf4b-nb-1" specific-use="notebook-content">
          <code language="python">x.grad.zero_()  # Reset the gradient
y = x.sum()
y.backward()
x.grad</code>
          <boxed-text>
            <preformat>tensor([1., 1., 1., 1.])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-8bdd4c0c-nb-1" specific-use="notebook-content">
</sec>
        <sec id="backward-for-non-scalar-variables-nb-1">
          <title>Backward for Non-Scalar Variables</title>
          <p>When <monospace>y</monospace> is a vector, the most natural
  representation of the derivative of <monospace>y</monospace> with
  respect to a vector <monospace>x</monospace> is a matrix called the
  <italic>Jacobian</italic> that contains the partial derivatives of
  each component of <monospace>y</monospace> with respect to each
  component of <monospace>x</monospace>. Likewise, for higher-order
  <monospace>y</monospace> and <monospace>x</monospace>, the result of
  differentiation could be an even higher-order tensor.</p>
          <p>While Jacobians do show up in some advanced machine learning
  techniques, more commonly we want to sum up the gradients of each
  component of <monospace>y</monospace> with respect to the full vector
  <monospace>x</monospace>, yielding a vector of the same shape as
  <monospace>x</monospace>. For example, we often have a vector
  representing the value of our loss function calculated separately for
  each example among a <italic>batch</italic> of training examples.
  Here, we just want to (<bold>sum up the gradients computed
  individually for each example</bold>).</p>
        </sec>
        <sec id="cell-9dda7124-nb-1" specific-use="notebook-content">
          <p>Because deep learning frameworks vary in how they interpret
  gradients of non-scalar tensors, PyTorch takes some steps to avoid
  confusion. Invoking <monospace>backward</monospace> on a non-scalar
  elicits an error unless we tell PyTorch how to reduce the object to a
  scalar. More formally, we need to provide some vector
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{v}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêØ</mml:mi></mml:math></alternatives></inline-formula>
  such that <monospace>backward</monospace> will compute
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{v}^\top \partial_{\mathbf{x}} \mathbf{y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùêØ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:msub><mml:mi>‚àÇ</mml:mi><mml:mi>ùê±</mml:mi></mml:msub><mml:mi>ùê≤</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  rather than <inline-formula><alternatives><tex-math><![CDATA[\partial_{\mathbf{x}} \mathbf{y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>‚àÇ</mml:mi><mml:mi>ùê±</mml:mi></mml:msub><mml:mi>ùê≤</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
  This next part may be confusing, but for reasons that will become
  clear later, this argument (representing
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{v}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêØ</mml:mi></mml:math></alternatives></inline-formula>)
  is named <monospace>gradient</monospace>. For a more detailed
  description, see Yang Zhang‚Äôs
  <ext-link ext-link-type="uri" xlink:href="https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29">Medium
  post</ext-link>.</p>
        </sec>
        <sec id="cell-1baa40bd-nb-1" specific-use="notebook-content">
          <code language="python">x.grad.zero_()
y = x * x
y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()
x.grad</code>
          <boxed-text>
            <preformat>tensor([0., 2., 4., 6.])</preformat>
          </boxed-text>
        </sec>
        <sec id="ffbd2c9d-nb-1" specific-use="notebook-content">
</sec>
        <sec id="detaching-computation-nb-1">
          <title>Detaching Computation</title>
          <p>Sometimes, we wish to [<bold>move some calculations outside of the
  recorded computational graph.</bold>] For example, say that we use the
  input to create some auxiliary intermediate terms for which we do not
  want to compute a gradient. In this case, we need to
  <italic>detach</italic> the respective computational graph from the
  final result. The following toy example makes this clearer: suppose we
  have <monospace>z = x * y</monospace> and
  <monospace>y = x * x</monospace> but we want to focus on the
  <italic>direct</italic> influence of <monospace>x</monospace> on
  <monospace>z</monospace> rather than the influence conveyed via
  <monospace>y</monospace>. In this case, we can create a new variable
  <monospace>u</monospace> that takes the same value as
  <monospace>y</monospace> but whose <italic>provenance</italic> (how it
  was created) has been wiped out. Thus <monospace>u</monospace> has no
  ancestors in the graph and gradients do not flow through
  <monospace>u</monospace> to <monospace>x</monospace>. For example,
  taking the gradient of <monospace>z = x * u</monospace> will yield the
  result <monospace>u</monospace>, (not <monospace>3 * x * x</monospace>
  as you might have expected since
  <monospace>z = x * x * x</monospace>).</p>
        </sec>
        <sec id="cell-107ac041-nb-1" specific-use="notebook-content">
          <code language="python">x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u</code>
          <boxed-text>
            <preformat>tensor([True, True, True, True])</preformat>
          </boxed-text>
        </sec>
        <sec id="e0378e1f-nb-1" specific-use="notebook-content">
          <p>Note that while this procedure detaches <monospace>y</monospace>‚Äôs
  ancestors from the graph leading to <monospace>z</monospace>, the
  computational graph leading to <monospace>y</monospace> persists and
  thus we can calculate the gradient of <monospace>y</monospace> with
  respect to <monospace>x</monospace>.</p>
        </sec>
        <sec id="cb8c674b-nb-1" specific-use="notebook-content">
          <code language="python">x.grad.zero_()
y.sum().backward()
x.grad == 2 * x</code>
          <boxed-text>
            <preformat>tensor([True, True, True, True])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-76f056ce-nb-1" specific-use="notebook-content">
</sec>
        <sec id="gradients-and-python-control-flow-nb-1">
          <title>Gradients and Python Control Flow</title>
          <p>So far we reviewed cases where the path from input to output was
  well defined via a function such as
  <monospace>z = x * x * x</monospace>. Programming offers us a lot more
  freedom in how we compute results. For instance, we can make them
  depend on auxiliary variables or condition choices on intermediate
  results. One benefit of using automatic differentiation is that
  [<bold>even if</bold>] building the computational graph of (<bold>a
  function required passing through a maze of Python control
  flow</bold>) (e.g., conditionals, loops, and arbitrary function
  calls), (<bold>we can still calculate the gradient of the resulting
  variable.</bold>) To illustrate this, consider the following code
  snippet where the number of iterations of the
  <monospace>while</monospace> loop and the evaluation of the
  <monospace>if</monospace> statement both depend on the value of the
  input <monospace>a</monospace>.</p>
        </sec>
        <sec id="a83327c2-nb-1" specific-use="notebook-content">
          <code language="python">def f(a):
    b = a * 2
    while b.norm() &lt; 1000:
        b = b * 2
    if b.sum() &gt; 0:
        c = b
    else:
        c = 100 * b
    return c</code>
        </sec>
        <sec id="cell-189f6785-nb-1" specific-use="notebook-content">
          <p>Below, we call this function, passing in a random value, as input.
  Since the input is a random variable, we do not know what form the
  computational graph will take. However, whenever we execute
  <monospace>f(a)</monospace> on a specific input, we realize a specific
  computational graph and can subsequently run
  <monospace>backward</monospace>.</p>
        </sec>
        <sec id="c5ef0264-nb-1" specific-use="notebook-content">
          <code language="python">a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()</code>
        </sec>
        <sec id="cell-51065133-nb-1" specific-use="notebook-content">
          <p>Even though our function <monospace>f</monospace> is, for
  demonstration purposes, a bit contrived, its dependence on the input
  is quite simple: it is a <italic>linear</italic> function of
  <monospace>a</monospace> with piecewise defined scale. As such,
  <monospace>f(a) / a</monospace> is a vector of constant entries and,
  moreover, <monospace>f(a) / a</monospace> needs to match the gradient
  of <monospace>f(a)</monospace> with respect to
  <monospace>a</monospace>.</p>
        </sec>
        <sec id="ab14ef91-nb-1" specific-use="notebook-content">
          <code language="python">a.grad == d / a</code>
          <boxed-text>
            <preformat>tensor(True)</preformat>
          </boxed-text>
        </sec>
        <sec id="a992f28c-nb-1" specific-use="notebook-content">
          <p>Dynamic control flow is very common in deep learning. For instance,
  when processing text, the computational graph depends on the length of
  the input. In these cases, automatic differentiation becomes vital for
  statistical modeling since it is impossible to compute the gradient
  <italic>a priori</italic>.</p>
        </sec>
        <sec id="discussion-nb-1">
          <title>Discussion</title>
          <p>You have now gotten a taste of the power of automatic
  differentiation. The development of libraries for calculating
  derivatives both automatically and efficiently has been a massive
  productivity booster for deep learning practitioners, liberating them
  so they can focus on less menial. Moreover, autograd lets us design
  massive models for which pen and paper gradient computations would be
  prohibitively time consuming. Interestingly, while we use autograd to
  <italic>optimize</italic> models (in a statistical sense) the
  <italic>optimization</italic> of autograd libraries themselves (in a
  computational sense) is a rich subject of vital interest to framework
  designers. Here, tools from compilers and graph manipulation are
  leveraged to compute results in the most expedient and
  memory-efficient manner.</p>
          <p>For now, try to remember these basics: (i) attach gradients to
  those variables with respect to which we desire derivatives; (ii)
  record the computation of the target value; (iii) execute the
  backpropagation function; and (iv) access the resulting gradient.</p>
        </sec>
        <sec id="exercises-nb-1">
          <title>Exercises</title>
          <list list-type="order">
            <list-item>
              <p>Why is the second derivative much more expensive to compute
      than the first derivative?</p>
            </list-item>
            <list-item>
              <p>After running the function for backpropagation, immediately run
      it again and see what happens. Investigate.</p>
            </list-item>
            <list-item>
              <p>In the control flow example where we calculate the derivative
      of <monospace>d</monospace> with respect to
      <monospace>a</monospace>, what would happen if we changed the
      variable <monospace>a</monospace> to a random vector or a matrix?
      At this point, the result of the calculation
      <monospace>f(a)</monospace> is no longer a scalar. What happens to
      the result? How do we analyze this?</p>
            </list-item>
            <list-item>
              <p>Let <inline-formula><alternatives><tex-math><![CDATA[f(x) = \sin(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>sin</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
      Plot the graph of <inline-formula><alternatives><tex-math><![CDATA[f]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
      and of its derivative <inline-formula><alternatives><tex-math><![CDATA[f']]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mi>‚Ä≤</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
      Do not exploit the fact that <inline-formula><alternatives><tex-math><![CDATA[f'(x) = \cos(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mi>‚Ä≤</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>cos</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      but rather use automatic differentiation to get the result.</p>
            </list-item>
            <list-item>
              <p>Let <inline-formula><alternatives><tex-math><![CDATA[f(x) = ((\log x^2) \cdot \sin x) + x^{-1}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mo>log</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>‚ãÖ</mml:mo><mml:mo>sin</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.
      Write out a dependency graph tracing results from
      <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      to <inline-formula><alternatives><tex-math><![CDATA[f(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
            </list-item>
            <list-item>
              <p>Use the chain rule to compute the derivative
      <inline-formula><alternatives><tex-math><![CDATA[\frac{df}{dx}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>
      of the aforementioned function, placing each term on the
      dependency graph that you constructed previously.</p>
            </list-item>
            <list-item>
              <p>Given the graph and the intermediate derivative results, you
      have a number of options when computing the gradient. Evaluate the
      result once starting from <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      to <inline-formula><alternatives><tex-math><![CDATA[f]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
      and once from <inline-formula><alternatives><tex-math><![CDATA[f]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
      tracing back to <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>.
      The path from <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      to <inline-formula><alternatives><tex-math><![CDATA[f]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
      is commonly known as <italic>forward differentiation</italic>,
      whereas the path from <inline-formula><alternatives><tex-math><![CDATA[f]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
      to <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      is known as backward differentiation.</p>
            </list-item>
            <list-item>
              <p>When might you want to use forward, and when backward,
      differentiation? Hint: consider the amount of intermediate data
      needed, the ability to parallelize steps, and the size of matrices
      and vectors involved.</p>
            </list-item>
          </list>
        </sec>
      </sec>
    </body>
    <back>
</back>
  </sub-article>
  <sub-article article-type="notebook" id="nb-4-nb-2">
    <front-stub>
      <title-group>
        <article-title>02.1 Optimization &amp; Linear Neural
Networks</article-title>
      </title-group>
    </front-stub>
    <body>
      <sec id="cell-1-nb-2" specific-use="notebook-content">
        <p/>
        <p>Data Science in Electron Microscopy</p>
        <p>Philipp Pelz</p>
        <p>2024</p>
        <p/>
        <p>¬†
<ext-link ext-link-type="uri" xlink:href="https://github.com/ECLIPSE-Lab/WS24_DataScienceForEM">https://github.com/ECLIPSE-Lab/WS24_DataScienceForEM</ext-link></p>
        <sec id="optimization-and-deep-learning-nb-2">
          <title>Optimization and Deep Learning</title>
          <list list-type="bullet">
            <list-item>
              <p>for optimization problem, we will usually define a <italic>loss
      function</italic> first. Once we have the loss function, we can
      use an optimization algorithm in attempt to minimize the loss.</p>
            </list-item>
            <list-item>
              <p>a loss function is often referred to as the <italic>objective
      function</italic> of the optimization problem. - By tradition and
      convention most optimization algorithms are concerned with
      <italic>minimization</italic>. - - If we ever need to maximize an
      objective there is a simple solution: just flip the sign on the
      objective.</p>
            </list-item>
          </list>
        </sec>
        <sec id="linear-regression-nb-2">
          <title>Linear Regression</title>
          <list list-type="bullet">
            <list-item>
              <p><italic>Regression</italic> problems pop up whenever we want to
      predict a numerical value.</p>
            </list-item>
            <list-item>
              <p>Common examples: predicting prices (of homes, stocks, etc.),
      predicting the length of stay (for patients in the hospital),
      forecasting demand (for retail sales)</p>
            </list-item>
            <list-item>
              <p>Not every prediction problem is one of classical regression.
      Later on, we will introduce classification problems, where the
      goal is to predict membership among a set of categories.</p>
            </list-item>
            <list-item>
              <p>example: suppose that we wish to estimate the prices of houses
      (in dollars) based on their area (in square feet) and age (in
      years).</p>
            </list-item>
            <list-item>
              <p>to develop model for predicting house prices, need to get our
      hands on data, including the sales price, area, and age for each
      home.</p>
            </list-item>
          </list>
          <p>:::</p>
          <sec id="cell-2-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>In the terminology of machine learning, the dataset is called a
      <italic>training dataset</italic> or <italic>training
      set</italic>, and each row (containing the data corresponding to
      one sale) is called an <italic>example</italic> (or <italic>data
      point</italic>, <italic>instance</italic>,
      <italic>sample</italic>).</p>
              </list-item>
              <list-item>
                <p>The thing we are trying to predict (price) is called a
      <italic>label</italic> (or <italic>target</italic>). The variables
      (age and area) upon which the predictions are based are called
      <italic>features</italic> (or <italic>covariates</italic>).</p>
              </list-item>
            </list>
          </sec>
          <sec id="cell-3-nb-2" specific-use="notebook-content">
            <code language="python">import math
import torch
import numpy as np
import time</code>
          </sec>
        </sec>
        <sec id="basics-nb-2">
          <title>Basics</title>
          <list list-type="bullet">
            <list-item>
              <p><italic>Linear regression</italic> is both the simplest and
      most popular among the standard tools for tackling regression
      problems.</p>
            </list-item>
            <list-item>
              <p>First, assume that relationship between features
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>
      and target <inline-formula><alternatives><tex-math><![CDATA[y]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>
      is approximately linear, i.e., that the conditional mean
      <inline-formula><alternatives><tex-math><![CDATA[E[Y \mid X=\mathbf{x}]]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>Y</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>ùê±</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      can be expressed as a weighted sum of the features
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>.</p>
            </list-item>
            <list-item>
              <p>allows that the target value may still deviate from its
      expected value on account of observation noise.</p>
            </list-item>
            <list-item>
              <p>Next, impose assumption that any such noise is well behaved,
      following a Gaussian distribution.</p>
            </list-item>
            <list-item>
              <p>we will use <inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
      to denote the number of examples in our dataset. We use
      superscripts to enumerate samples and targets, and subscripts to
      index coordinates.</p>
            </list-item>
            <list-item>
              <p>More concretely, <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}^{(i)}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>ùê±</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
      denotes the <inline-formula><alternatives><tex-math><![CDATA[i^{\textrm{th}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>i</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
      sample and <inline-formula><alternatives><tex-math><![CDATA[x_j^{(i)}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msubsup></mml:math></alternatives></inline-formula>
      denotes its <inline-formula><alternatives><tex-math><![CDATA[j^{\textrm{th}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>j</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
      coordinate.</p>
            </list-item>
          </list>
        </sec>
        <sec id="model-nb-2">
          <title>Model</title>
          <list list-type="bullet">
            <list-item>
              <p>At the heart of every solution is a model that describes how
      features can be transformed into an estimate of the target.</p>
            </list-item>
            <list-item>
              <p>The assumption of linearity means that the expected value of
      the target (price) can be expressed as a weighted sum of the
      features (area and age):</p>
            </list-item>
          </list>
          <p><disp-formula><alternatives><tex-math><![CDATA[\textrm{price} = w_{\textrm{area}} \cdot \textrm{area} + w_{\textrm{age}} \cdot \textrm{age} + b.]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mtext mathvariant="normal">price</mml:mtext><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext mathvariant="normal">area</mml:mtext></mml:msub><mml:mo>‚ãÖ</mml:mo><mml:mtext mathvariant="normal">area</mml:mtext><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext mathvariant="normal">age</mml:mtext></mml:msub><mml:mo>‚ãÖ</mml:mo><mml:mtext mathvariant="normal">age</mml:mtext><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
  :eqlabel:<monospace>eq_price-area</monospace></p>
          <list list-type="bullet">
            <list-item>
              <p>Here <inline-formula><alternatives><tex-math><![CDATA[w_{\textrm{area}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>w</mml:mi><mml:mtext mathvariant="normal">area</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives><tex-math><![CDATA[w_{\textrm{age}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>w</mml:mi><mml:mtext mathvariant="normal">age</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      are called <italic>weights</italic>, and
      <inline-formula><alternatives><tex-math><![CDATA[b]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>
      is called a <italic>bias</italic> (or <italic>offset</italic> or
      <italic>intercept</italic>).</p>
            </list-item>
            <list-item>
              <p>The weights determine the influence of each feature on our
      prediction. The bias determines the value of the estimate when all
      features are zero.</p>
            </list-item>
            <list-item>
              <p>Even though we will never see any newly-built homes with
      precisely zero area, we still need the bias because it allows us
      to express all linear functions of our features (rather than
      restricting us to lines that pass through the origin).</p>
            </list-item>
          </list>
          <sec id="cell-5-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>Strictly speaking, :eqref:<monospace>eq_price-area</monospace>
      is an <italic>affine transformation</italic> of input features,
      which is characterized by a <italic>linear transformation</italic>
      of features via a weighted sum, combined with a
      <italic>translation</italic> via the added bias.</p>
              </list-item>
              <list-item>
                <p>Given a dataset, our goal is to choose the weights
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê∞</mml:mi></mml:math></alternatives></inline-formula>
      and the bias <inline-formula><alternatives><tex-math><![CDATA[b]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>
      that, on average, make our model‚Äôs predictions fit the true prices
      observed in the data as closely as possible.</p>
              </list-item>
            </list>
          </sec>
          <list list-type="bullet">
            <list-item>
              <p>In disciplines where it is common to focus on datasets with
      just a few features, explicitly expressing models long-form, as in
      :eqref:<monospace>eq_price-area</monospace>, is common.</p>
            </list-item>
            <list-item>
              <p>In machine learning, we usually work with high-dimensional
      datasets,where it is more convenient to employ compact linear
      algebra notation.</p>
            </list-item>
            <list-item>
              <p>When inputs consist of <inline-formula><alternatives><tex-math><![CDATA[d]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>d</mml:mi></mml:math></alternatives></inline-formula>
      features, we can assign each an index (between
      <inline-formula><alternatives><tex-math><![CDATA[1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives><tex-math><![CDATA[d]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>d</mml:mi></mml:math></alternatives></inline-formula>)
      and express our prediction <inline-formula><alternatives><tex-math><![CDATA[\hat{y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:math></alternatives></inline-formula>
      (in general the ‚Äúhat‚Äù symbol denotes an estimate) as</p>
            </list-item>
          </list>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[\hat{y} = w_1  x_1 + \cdots + w_d  x_d + b.]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mover>
                      <mml:mi>y</mml:mi>
                      <mml:mo accent="true">ÃÇ</mml:mo>
                    </mml:mover>
                    <mml:mo>=</mml:mo>
                    <mml:msub>
                      <mml:mi>w</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                    <mml:mi>‚ãØ</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mi>w</mml:mi>
                      <mml:mi>d</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>d</mml:mi>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                    <mml:mi>b</mml:mi>
                    <mml:mi>.</mml:mi>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <sec id="cell-7-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>Collecting all features into a vector
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x} \in \mathbb{R}^d]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùê±</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      and all weights into a vector <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w} \in \mathbb{R}^d]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùê∞</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
      we can express our model compactly via the dot product between
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê∞</mml:mi></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>:</p>
              </list-item>
            </list>
            <p><disp-formula><alternatives><tex-math><![CDATA[\hat{y} = \mathbf{w}^\top \mathbf{x} + b.]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mi>ùê∞</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùê±</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
  :eqlabel:<monospace>eq_linreg-y</monospace></p>
            <list list-type="bullet">
              <list-item>
                <p>In :eqref:<monospace>eq_linreg-y</monospace>, the vector
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>
      corresponds to the features of a single example.</p>
              </list-item>
              <list-item>
                <p>We will often find it convenient to refer to features of our
      entire dataset of <inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
      examples via the <italic>design matrix</italic>
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X} \in \mathbb{R}^{n \times d}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêó</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>√ó</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
              </list-item>
              <list-item>
                <p>Here, <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
      contains one row for every example and one column for every
      feature.</p>
              </list-item>
              <list-item>
                <p>For a collection of features <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>,
      the predictions <inline-formula><alternatives><tex-math><![CDATA[\hat{\mathbf{y}} \in \mathbb{R}^n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mover><mml:mi>ùê≤</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      can be expressed via the matrix‚Äìvector product:</p>
              </list-item>
            </list>
            <p><disp-formula><alternatives><tex-math><![CDATA[{\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b,]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mover><mml:mi>ùê≤</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>ùêó</mml:mi><mml:mi>ùê∞</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
  :eqlabel:<monospace>eq_linreg-y-vec</monospace> - where broadcasting
  is applied during the summation.</p>
          </sec>
          <list list-type="bullet">
            <list-item>
              <p>Given features of a training dataset
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
      and corresponding (known) labels <inline-formula><alternatives><tex-math><![CDATA[\mathbf{y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê≤</mml:mi></mml:math></alternatives></inline-formula></p>
            </list-item>
            <list-item>
              <p>the goal of linear regression is to find the weight vector
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê∞</mml:mi></mml:math></alternatives></inline-formula>
      and the bias term <inline-formula><alternatives><tex-math><![CDATA[b]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>
      such that</p>
            </list-item>
            <list-item>
              <p>given features of a new data example sampled from the same
      distribution as <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>,
      the new example‚Äôs label will (in expectation) be predicted with
      the smallest error.</p>
            </list-item>
            <list-item>
              <p>Even if we believe that the best model for predicting
      <inline-formula><alternatives><tex-math><![CDATA[y]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>
      given <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>
      is linear, we would not expect to find a real-world dataset of
      <inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
      examples where <inline-formula><alternatives><tex-math><![CDATA[y^{(i)}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
      exactly equals <sup/>{(i)}+b$ for all
      <inline-formula><alternatives><tex-math><![CDATA[1 \leq i \leq n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo>‚â§</mml:mo><mml:mi>i</mml:mi><mml:mo>‚â§</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</p>
            </list-item>
            <list-item>
              <p>For example, whatever instruments we use to observe the
      features <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
      and labels <inline-formula><alternatives><tex-math><![CDATA[\mathbf{y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê≤</mml:mi></mml:math></alternatives></inline-formula>,
      there might be a small amount of measurement error.</p>
            </list-item>
            <list-item>
              <p>‚Äì&gt; even when we are confident that the underlying
      relationship is linear, we will incorporate a noise term to
      account for such errors.</p>
            </list-item>
          </list>
          <sec id="cell-9-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>Before we can go about searching for the best
      <italic>parameters</italic> (or <italic>model parameters</italic>)
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê∞</mml:mi></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives><tex-math><![CDATA[b]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>,
      we will need two more things:</p>
              </list-item>
              <list-item>
                <list list-type="roman-lower">
                  <list-item>
                    <label>(i)</label>
                    <p>a measure of the quality of some given model;</p>
                  </list-item>
                </list>
              </list-item>
              <list-item>
                <p>and (ii) a procedure for updating the model to improve its
      quality.</p>
              </list-item>
            </list>
          </sec>
          <sec id="loss-function-nb-2">
            <title>Loss Function</title>
            <list list-type="bullet">
              <list-item>
                <p>fitting our model to the data requires that we agree on some
      measure of <italic>fitness</italic> (or, equivalently, of
      <italic>unfitness</italic>).</p>
              </list-item>
              <list-item>
                <p><italic>Loss functions</italic> quantify the distance between
      the <italic>real</italic> and <italic>predicted</italic> values of
      the target.</p>
              </list-item>
              <list-item>
                <p>The loss will usually be a nonnegative number where smaller
      values are better and perfect predictions incur a loss of 0.</p>
              </list-item>
              <list-item>
                <p>For regression problems, the most common loss function is the
      squared error. When our prediction for an example
      <inline-formula><alternatives><tex-math><![CDATA[i]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>
      is <inline-formula><alternatives><tex-math><![CDATA[\hat{y}^{(i)}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
      and the corresponding true label is <inline-formula><alternatives><tex-math><![CDATA[y^{(i)}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>,
      the <italic>squared error</italic> is given by:</p>
              </list-item>
            </list>
            <p><disp-formula><alternatives><tex-math><![CDATA[l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùê∞</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mo>‚àí</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
  :eqlabel:<monospace>eq_mse</monospace></p>
            <list list-type="bullet">
              <list-item>
                <p>constant <inline-formula><alternatives><tex-math><![CDATA[\frac{1}{2}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:math></alternatives></inline-formula>
      makes no real difference but proves to be notationally convenient,
      since it cancels out when we take the derivative of the loss.</p>
              </list-item>
            </list>
          </sec>
          <list list-type="bullet">
            <list-item>
              <p>training dataset is given to us, and thus is out of our
      control, the empirical error is only a function of the model
      parameters. In :numref:<monospace>fig_fit_linreg</monospace>, we
      visualize the fit of a linear regression model in a problem with
      one-dimensional inputs.</p>
            </list-item>
          </list>
          <p><inline-graphic mimetype="image" mime-subtype="svg+xml" xlink:href="../img/fit-linreg.svg"><alt-text>Fitting a linear regression model to one-dimensional
    data.</alt-text></inline-graphic> :label:<monospace>fig_fit_linreg</monospace></p>
          <list list-type="bullet">
            <list-item>
              <p>Note that large differences between estimates
      <inline-formula><alternatives><tex-math><![CDATA[\hat{y}^{(i)}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
      and targets <inline-formula><alternatives><tex-math><![CDATA[y^{(i)}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
      lead to even larger contributions to the loss, due to its
      quadratic form (this quadraticity can be a double-edge sword;
      while it encourages the model to avoid large errors it can also
      lead to excessive sensitivity to anomalous data).</p>
            </list-item>
          </list>
          <sec id="cell-11-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>To measure the quality of a model on the entire dataset of
      <inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
      examples, we simply average (or equivalently, sum) the losses on
      the training set:</p>
              </list-item>
            </list>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>L</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùê∞</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>b</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mn>1</mml:mn>
                        <mml:mi>n</mml:mi>
                      </mml:mfrac>
                      <mml:munderover>
                        <mml:mo>‚àë</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:munderover>
                      <mml:msup>
                        <mml:mi>l</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>i</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùê∞</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>b</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mn>1</mml:mn>
                        <mml:mi>n</mml:mi>
                      </mml:mfrac>
                      <mml:munderover>
                        <mml:mo>‚àë</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:munderover>
                      <mml:mfrac>
                        <mml:mn>1</mml:mn>
                        <mml:mn>2</mml:mn>
                      </mml:mfrac>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:msup>
                            <mml:mi>ùê∞</mml:mi>
                            <mml:mi>‚ä§</mml:mi>
                          </mml:msup>
                          <mml:msup>
                            <mml:mi>ùê±</mml:mi>
                            <mml:mrow>
                              <mml:mo stretchy="true" form="prefix">(</mml:mo>
                              <mml:mi>i</mml:mi>
                              <mml:mo stretchy="true" form="postfix">)</mml:mo>
                            </mml:mrow>
                          </mml:msup>
                          <mml:mo>+</mml:mo>
                          <mml:mi>b</mml:mi>
                          <mml:mo>‚àí</mml:mo>
                          <mml:msup>
                            <mml:mi>y</mml:mi>
                            <mml:mrow>
                              <mml:mo stretchy="true" form="prefix">(</mml:mo>
                              <mml:mi>i</mml:mi>
                              <mml:mo stretchy="true" form="postfix">)</mml:mo>
                            </mml:mrow>
                          </mml:msup>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                      <mml:mi>.</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
            <list list-type="bullet">
              <list-item>
                <p>When training the model, we seek parameters
      (<inline-formula><alternatives><tex-math><![CDATA[\mathbf{w}^*, b^*]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùê∞</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>)
      that minimize the total loss across all training examples:</p>
              </list-item>
            </list>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b).]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:msup>
                        <mml:mi>ùê∞</mml:mi>
                        <mml:mo>*</mml:mo>
                      </mml:msup>
                      <mml:mo>,</mml:mo>
                      <mml:msup>
                        <mml:mi>b</mml:mi>
                        <mml:mo>*</mml:mo>
                      </mml:msup>
                      <mml:mo>=</mml:mo>
                      <mml:munder>
                        <mml:mo>argmin</mml:mo>
                        <mml:mrow>
                          <mml:mi>ùê∞</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mi>b</mml:mi>
                        </mml:mrow>
                      </mml:munder>
                      <mml:mspace width="0.222em"/>
                      <mml:mi>L</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùê∞</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>b</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mi>.</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
          </sec>
          <sec id="analytic-solution-nb-2">
            <title>Analytic Solution</title>
            <list list-type="bullet">
              <list-item>
                <p>Unlike most of the models that we will cover, linear regression
      presents us with a surprisingly easy optimization problem.</p>
              </list-item>
              <list-item>
                <p>we can find the optimal parameters (as assessed on the training
      data) analytically by applying a simple formula as follows.</p>
              </list-item>
              <list-item>
                <p>subsume the bias <inline-formula><alternatives><tex-math><![CDATA[b]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>
      into the parameter <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê∞</mml:mi></mml:math></alternatives></inline-formula>
      by appending a column to the design matrix consisting of all
      1s.</p>
              </list-item>
              <list-item>
                <p>Then our prediction problem is to minimize
      <inline-formula><alternatives><tex-math><![CDATA[\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>ùê≤</mml:mi><mml:mo>‚àí</mml:mo><mml:mi>ùêó</mml:mi><mml:mi>ùê∞</mml:mi><mml:msup><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.
      As long as the design matrix <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
      has full rank (no feature is linearly dependent on the others),
      then there will be just one critical point on the loss surface and
      it corresponds to the minimum of the loss over the entire
      domain.</p>
              </list-item>
              <list-item>
                <p>Taking the derivative of the loss with respect to
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê∞</mml:mi></mml:math></alternatives></inline-formula>
      and setting it equal to zero yields:</p>
              </list-item>
            </list>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[\begin{aligned}
      \partial_{\mathbf{w}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 =
      2 \mathbf{X}^\top (\mathbf{X} \mathbf{w} - \mathbf{y}) = 0
      \textrm{ and hence }
      \mathbf{X}^\top \mathbf{y} = \mathbf{X}^\top \mathbf{X} \mathbf{w}.
  \end{aligned}]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mtable>
                      <mml:mtr>
                        <mml:mtd columnalign="right" style="text-align: right">
                          <mml:msub>
                            <mml:mi>‚àÇ</mml:mi>
                            <mml:mi>ùê∞</mml:mi>
                          </mml:msub>
                          <mml:mo stretchy="false" form="postfix">‚à•</mml:mo>
                          <mml:mi>ùê≤</mml:mi>
                          <mml:mo>‚àí</mml:mo>
                          <mml:mi>ùêó</mml:mi>
                          <mml:mi>ùê∞</mml:mi>
                          <mml:msup>
                            <mml:mo stretchy="false" form="postfix">‚à•</mml:mo>
                            <mml:mn>2</mml:mn>
                          </mml:msup>
                          <mml:mo>=</mml:mo>
                          <mml:mn>2</mml:mn>
                          <mml:msup>
                            <mml:mi>ùêó</mml:mi>
                            <mml:mi>‚ä§</mml:mi>
                          </mml:msup>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:mi>ùêó</mml:mi>
                            <mml:mi>ùê∞</mml:mi>
                            <mml:mo>‚àí</mml:mo>
                            <mml:mi>ùê≤</mml:mi>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                          <mml:mo>=</mml:mo>
                          <mml:mn>0</mml:mn>
                          <mml:mrow>
                            <mml:mspace width="0.333em"/>
                            <mml:mtext mathvariant="normal"> and hence </mml:mtext>
                            <mml:mspace width="0.333em"/>
                          </mml:mrow>
                          <mml:msup>
                            <mml:mi>ùêó</mml:mi>
                            <mml:mi>‚ä§</mml:mi>
                          </mml:msup>
                          <mml:mi>ùê≤</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:msup>
                            <mml:mi>ùêó</mml:mi>
                            <mml:mi>‚ä§</mml:mi>
                          </mml:msup>
                          <mml:mi>ùêó</mml:mi>
                          <mml:mi>ùê∞</mml:mi>
                          <mml:mi>.</mml:mi>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
          </sec>
          <list list-type="bullet">
            <list-item>
              <p>Solving for <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê∞</mml:mi></mml:math></alternatives></inline-formula>
      provides us with the optimal solution for the optimization
      problem. Note that this solution</p>
            </list-item>
          </list>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:msup>
                      <mml:mi>ùê∞</mml:mi>
                      <mml:mo>*</mml:mo>
                    </mml:msup>
                    <mml:mo>=</mml:mo>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:msup>
                          <mml:mi>ùêó</mml:mi>
                          <mml:mi>‚ä§</mml:mi>
                        </mml:msup>
                        <mml:mi>ùêó</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>‚àí</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                    <mml:msup>
                      <mml:mi>ùêó</mml:mi>
                      <mml:mi>‚ä§</mml:mi>
                    </mml:msup>
                    <mml:mi>ùê≤</mml:mi>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <p>will only be unique when the matrix <inline-formula><alternatives><tex-math><![CDATA[\mathbf X^\top \mathbf X]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùêó</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùêó</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  is invertible, i.e., when the columns of the design matrix are
  linearly independent.</p>
          <sec id="cell-13-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>simple problems like linear regression may admit analytic
      solutions</p>
              </list-item>
              <list-item>
                <p>Although analytic solutions allow for nice mathematical
      analysis, the requirement of an analytic solution is so
      restrictive that it would exclude almost all exciting aspects of
      deep learning.</p>
              </list-item>
            </list>
          </sec>
          <sec id="minibatch-stochastic-gradient-descent-nb-2">
            <title>Minibatch Stochastic Gradient Descent</title>
            <list list-type="bullet">
              <list-item>
                <p>even in cases where we cannot solve the models analytically, we
      can still often train models effectively in practice.</p>
              </list-item>
              <list-item>
                <p>for many tasks, those hard-to-optimize models turn out to be so
      much better that figuring out how to train them ends up being well
      worth the trouble.</p>
              </list-item>
              <list-item>
                <p>key technique for optimizing nearly every deep learning model:
      algorithm is called <italic>gradient descent</italic>.</p>
              </list-item>
            </list>
          </sec>
          <list list-type="bullet">
            <list-item>
              <p>most naive application of gradient descent consists of taking
      the derivative of the loss function, which is an average of the
      losses computed on every single example in the dataset.</p>
            </list-item>
            <list-item>
              <p>In practice, this can be extremely slow: we must pass over the
      entire dataset before making a single update, even if the update
      steps might be very powerful
      :cite:<monospace>Liu.Nocedal.1989</monospace>.</p>
            </list-item>
            <list-item>
              <p>Even worse, if there is a lot of redundancy in the training
      data, the benefit of a full update is limited.</p>
            </list-item>
            <list-item>
              <p>The other extreme is to consider only a single example at a
      time and to take update steps based on one observation at a
      time.</p>
            </list-item>
            <list-item>
              <p>The resulting algorithm, <italic>stochastic gradient
      descent</italic> (SGD) can be an effective strategy
      :cite:<monospace>Bottou.2010</monospace>, even for large
      datasets.</p>
            </list-item>
          </list>
          <sec id="cell-15-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>Unfortunately, SGD has drawbacks, both computational and
      statistical. One problem arises from the fact that processors are
      a lot faster multiplying and adding numbers than they are at
      moving data from main memory to processor cache.</p>
              </list-item>
              <list-item>
                <p>It is up to an order of magnitude more efficient to perform a
      matrix‚Äìvector multiplication than a corresponding number of
      vector‚Äìvector operations.</p>
              </list-item>
              <list-item>
                <p>This means that it can take a lot longer to process one sample
      at a time compared to a full batch.</p>
              </list-item>
              <list-item>
                <p>A second problem is that some of the layers, such as batch
      normalization (to be described in
      :numref:<monospace>sec_batch_norm</monospace>), only work well
      when we have access to more than one observation at a time.</p>
              </list-item>
            </list>
          </sec>
          <list list-type="bullet">
            <list-item>
              <p>The solution to both problems is to pick an intermediate
      strategy: rather than taking a full batch or only a single sample
      at a time, we take a <italic>minibatch</italic> of observations
      :cite:<monospace>Li.Zhang.Chen.ea.2014</monospace>.</p>
            </list-item>
            <list-item>
              <p>The specific choice of the size of the said minibatch depends
      on many factors, such as the amount of memory, the number of
      accelerators, the choice of layers, and the total dataset size.-
      Despite all that, a number between 32 and 256, preferably a
      multiple of a large power of <inline-formula><alternatives><tex-math><![CDATA[2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>2</mml:mn></mml:math></alternatives></inline-formula>,
      is a good start.</p>
            </list-item>
            <list-item>
              <p>This leads us to <italic>minibatch stochastic gradient
      descent</italic>.</p>
            </list-item>
            <list-item>
              <p>In its most basic form, in each iteration
      <inline-formula><alternatives><tex-math><![CDATA[t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>,
      we first randomly sample a minibatch
      <inline-formula><alternatives><tex-math><![CDATA[\mathcal{B}_t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>‚Ñ¨</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      consisting of a fixed number <inline-formula><alternatives><tex-math><![CDATA[|\mathcal{B}|]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>‚Ñ¨</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      of training examples.</p>
            </list-item>
            <list-item>
              <p>We then compute the derivative (gradient) of the average loss
      on the minibatch with respect to the model parameters. Finally, we
      multiply the gradient by a predetermined small positive value
      <inline-formula><alternatives><tex-math><![CDATA[\eta]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>Œ∑</mml:mi></mml:math></alternatives></inline-formula>,
      called the <italic>learning rate</italic>, and subtract the
      resulting term from the current parameter values.</p>
            </list-item>
          </list>
          <sec id="cell-17-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>We can express the update as follows:</p>
              </list-item>
            </list>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùê∞</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>b</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>‚Üê</mml:mo>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùê∞</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>b</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>‚àí</mml:mo>
                      <mml:mfrac>
                        <mml:mi>Œ∑</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">|</mml:mo>
                          <mml:mi>‚Ñ¨</mml:mi>
                          <mml:mo stretchy="true" form="postfix">|</mml:mo>
                        </mml:mrow>
                      </mml:mfrac>
                      <mml:munder>
                        <mml:mo>‚àë</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>‚àà</mml:mo>
                          <mml:msub>
                            <mml:mi>‚Ñ¨</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                      </mml:munder>
                      <mml:msub>
                        <mml:mi>‚àÇ</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>ùê∞</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mi>b</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msup>
                        <mml:mi>l</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>i</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùê∞</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>b</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mi>.</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
            <p>In summary, minibatch SGD proceeds as follows: (i) initialize the
  values of the model parameters, typically at random; (ii) iteratively
  sample random minibatches from the data, updating the parameters in
  the direction of the negative gradient. For quadratic losses and
  affine transformations, this has a closed-form expansion:</p>
            <p><disp-formula><alternatives><tex-math><![CDATA[\begin{aligned} \mathbf{w} & \leftarrow \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) && = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \partial_b l^{(i)}(\mathbf{w}, b) &&  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:mi>ùê∞</mml:mi></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>‚Üê</mml:mo><mml:mi>ùê∞</mml:mi><mml:mo>‚àí</mml:mo><mml:mfrac><mml:mi>Œ∑</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>‚Ñ¨</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>‚àà</mml:mo><mml:msub><mml:mi>‚Ñ¨</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>‚àÇ</mml:mi><mml:mi>ùê∞</mml:mi></mml:msub><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùê∞</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="right" style="text-align: right"/><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:mi>ùê∞</mml:mi><mml:mo>‚àí</mml:mo><mml:mfrac><mml:mi>Œ∑</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>‚Ñ¨</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>‚àà</mml:mo><mml:msub><mml:mi>‚Ñ¨</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msup><mml:mi>ùê±</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>ùê∞</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:msup><mml:mi>ùê±</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>‚àí</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:mi>b</mml:mi></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>‚Üê</mml:mo><mml:mi>b</mml:mi><mml:mo>‚àí</mml:mo><mml:mfrac><mml:mi>Œ∑</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>‚Ñ¨</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>‚àà</mml:mo><mml:msub><mml:mi>‚Ñ¨</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi>‚àÇ</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:msup><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùê∞</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="right" style="text-align: right"/><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>‚àí</mml:mo><mml:mfrac><mml:mi>Œ∑</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>‚Ñ¨</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow></mml:mfrac><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>‚àà</mml:mo><mml:msub><mml:mi>‚Ñ¨</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>ùê∞</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:msup><mml:mi>ùê±</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo>‚àí</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
  :eqlabel:<monospace>eq_linreg_batch_update</monospace></p>
          </sec>
          <list list-type="bullet">
            <list-item>
              <p>Since we pick a minibatch <inline-formula><alternatives><tex-math><![CDATA[\mathcal{B}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚Ñ¨</mml:mi></mml:math></alternatives></inline-formula>
      we need to normalize by its size <inline-formula><alternatives><tex-math><![CDATA[|\mathcal{B}|]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>‚Ñ¨</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.
      Frequently minibatch size and learning rate are user-defined. Such
      tunable parameters that are not updated in the training loop are
      called <italic>hyperparameters</italic>.</p>
            </list-item>
            <list-item>
              <p>They can be tuned automatically by a number of techniques, such
      as Bayesian optimization
      :cite:<monospace>Frazier.2018</monospace>. In the end, the quality
      of the solution is typically assessed on a separate
      <italic>validation dataset</italic> (or <italic>validation
      set</italic>).</p>
            </list-item>
            <list-item>
              <p>After training for some predetermined number of iterations (or
      until some other stopping criterion is met), we record the
      estimated model parameters, denoted <inline-formula><alternatives><tex-math><![CDATA[\hat{\mathbf{w}}, \hat{b}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mover><mml:mi>ùê∞</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover><mml:mi>b</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>.</p>
            </list-item>
            <list-item>
              <p>Note that even if our function is truly linear and noiseless,
      these parameters will not be the exact minimizers of the loss, nor
      even deterministic.</p>
            </list-item>
            <list-item>
              <p>Although the algorithm converges slowly towards the minimizers
      it typically will not find them exactly in a finite number of
      steps.</p>
            </list-item>
            <list-item>
              <p>Moreover, the minibatches <inline-formula><alternatives><tex-math><![CDATA[\mathcal{B}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚Ñ¨</mml:mi></mml:math></alternatives></inline-formula>
      used for updating the parameters are chosen at random. This breaks
      determinism.</p>
            </list-item>
          </list>
          <sec id="cell-19-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>Linear regression happens to be a learning problem with a
      global minimum (whenever <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
      is full rank, or equivalently, whenever
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}^\top \mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùêó</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùêó</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      is invertible).</p>
              </list-item>
              <list-item>
                <p>However, the loss surfaces for deep networks contain many
      saddle points and minima. Fortunately, we typically do not care
      about finding an exact set of parameters but merely any set of
      parameters that leads to accurate predictions (and thus low
      loss).</p>
              </list-item>
              <list-item>
                <p>In practice, deep learning practitioners seldom struggle to
      find parameters that minimize the loss <italic>on training
      sets</italic>
      :cite:<monospace>Izmailov.Podoprikhin.Garipov.ea.2018,Frankle.Carbin.2018</monospace>.</p>
              </list-item>
              <list-item>
                <p>The more formidable task is to find parameters that lead to
      accurate predictions on previously unseen data, a challenge called
      <italic>generalization</italic>.</p>
              </list-item>
            </list>
          </sec>
          <sec id="predictions-nb-2">
            <title>Predictions</title>
            <list list-type="bullet">
              <list-item>
                <p>Given the model <inline-formula><alternatives><tex-math><![CDATA[\hat{\mathbf{w}}^\top \mathbf{x} + \hat{b}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mover><mml:mi>ùê∞</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùê±</mml:mi><mml:mo>+</mml:mo><mml:mover><mml:mi>b</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>,
      we can now make <italic>predictions</italic> for a new example,
      e.g., predicting the sales price of a previously unseen house
      given its area <inline-formula><alternatives><tex-math><![CDATA[x_1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
      and age <inline-formula><alternatives><tex-math><![CDATA[x_2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>.</p>
              </list-item>
              <list-item>
                <p>Deep learning practitioners have taken to calling the
      prediction phase <italic>inference</italic> but this is a bit of a
      misnomer‚Äî<italic>inference</italic> refers broadly to any
      conclusion reached on the basis of evidence, including both the
      values of the parameters and the likely label for an unseen
      instance.</p>
              </list-item>
              <list-item>
                <p>If anything, in the statistics literature
      <italic>inference</italic> more often denotes parameter inference
      and this overloading of terminology creates unnecessary confusion
      when deep learning practitioners talk to statisticians.</p>
              </list-item>
              <list-item>
                <p>In the following we will stick to <italic>prediction</italic>
      whenever possible.</p>
              </list-item>
            </list>
          </sec>
          <sec id="vectorization-for-speed-nb-2">
            <title>Vectorization for Speed</title>
            <list list-type="bullet">
              <list-item>
                <p>When training our models, we typically want to process whole
      minibatches of examples simultaneously.</p>
              </list-item>
              <list-item>
                <p>Doing this efficiently requires that (<bold>we</bold>)
      (<strike>should</strike>) (<bold>vectorize the calculations and
      leverage fast linear algebra libraries rather than writing costly
      for-loops in Python.</bold>)</p>
              </list-item>
              <list-item>
                <p>To see why this matters so much, let‚Äôs (<bold>consider two
      methods for adding vectors.</bold>)</p>
              </list-item>
              <list-item>
                <p>To start, we instantiate two 10,000-dimensional vectors
      containing all 1s.</p>
              </list-item>
              <list-item>
                <p>In the first method, we loop over the vectors with a Python
      for-loop.</p>
              </list-item>
              <list-item>
                <p>In the second, we rely on a single call to
      <monospace>+</monospace>.</p>
              </list-item>
            </list>
          </sec>
          <sec id="cell-20-nb-2" specific-use="notebook-content">
            <code language="python">n = 10000
a = torch.ones(n)
b = torch.ones(n)</code>
          </sec>
          <sec id="cell-21-nb-2" specific-use="notebook-content">
            <p>Now we can benchmark the workloads. First, [<bold>we add them, one
  coordinate at a time, using a for-loop.</bold>]</p>
          </sec>
          <sec id="cell-22-nb-2" specific-use="notebook-content">
            <code language="python">c = torch.zeros(n)
t = time.time()
for i in range(n):
    c[i] = a[i] + b[i]
f'{time.time() - t:.5f} sec'</code>
          </sec>
          <sec id="cell-23-nb-2" specific-use="notebook-content">
            <p>(<bold>Alternatively, we rely on the reloaded
  <monospace>+</monospace> operator to compute the elementwise
  sum.</bold>)</p>
          </sec>
          <sec id="cell-24-nb-2" specific-use="notebook-content">
            <code language="python">t = time.time()
d = a + b
f'{time.time() - t:.5f} sec'</code>
          </sec>
          <list list-type="bullet">
            <list-item>
              <p>second method is dramatically faster than the first.
      Vectorizing code often yields order-of-magnitude speedups.</p>
            </list-item>
            <list-item>
              <p>Moreover, we push more of the mathematics to the library so we
      do not have to write as many calculations ourselves, reducing the
      potential for errors and increasing portability of the code.</p>
            </list-item>
          </list>
        </sec>
        <sec id="the-normal-distribution-and-squared-loss-nb-2">
          <title>The Normal Distribution and Squared Loss</title>
          <list list-type="bullet">
            <list-item>
              <p>So far we have given a fairly functional motivation of the
      squared loss objective:</p>
            </list-item>
            <list-item>
              <p>the optimal parameters return the conditional expectation
      <inline-formula><alternatives><tex-math><![CDATA[E[Y\mid X]]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>Y</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      whenever the underlying pattern is truly linear, and the loss
      assigns large penalties for outliers.</p>
            </list-item>
            <list-item>
              <p>We can also provide a more formal motivation for the squared
      loss objective by making probabilistic assumptions about the
      distribution of noise.</p>
            </list-item>
            <list-item>
              <p>Linear regression was invented at the turn of the 19th
      century.</p>
            </list-item>
            <list-item>
              <p>While it has long been debated whether Gauss or Legendre first
      thought up the idea, it was Gauss who also discovered the normal
      distribution (also called the <italic>Gaussian</italic>). It turns
      out that the normal distribution and linear regression with
      squared loss share a deeper connection than common parentage.</p>
            </list-item>
          </list>
          <sec id="cell-26-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>To begin, recall that a normal distribution with mean
      <inline-formula><alternatives><tex-math><![CDATA[\mu]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>Œº</mml:mi></mml:math></alternatives></inline-formula>
      and variance <inline-formula><alternatives><tex-math><![CDATA[\sigma^2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>œÉ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
      (standard deviation <inline-formula><alternatives><tex-math><![CDATA[\sigma]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>œÉ</mml:mi></mml:math></alternatives></inline-formula>)
      is given as</p>
              </list-item>
            </list>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>p</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>x</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mn>1</mml:mn>
                        <mml:msqrt>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                            <mml:mi>œÄ</mml:mi>
                            <mml:msup>
                              <mml:mi>œÉ</mml:mi>
                              <mml:mn>2</mml:mn>
                            </mml:msup>
                          </mml:mrow>
                        </mml:msqrt>
                      </mml:mfrac>
                      <mml:mo>exp</mml:mo>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>‚àí</mml:mi>
                        <mml:mfrac>
                          <mml:mn>1</mml:mn>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                            <mml:msup>
                              <mml:mi>œÉ</mml:mi>
                              <mml:mn>2</mml:mn>
                            </mml:msup>
                          </mml:mrow>
                        </mml:mfrac>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:mi>x</mml:mi>
                            <mml:mo>‚àí</mml:mo>
                            <mml:mi>Œº</mml:mi>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:msup>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mi>.</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
            <p>Below [<bold>we define a function to compute the normal
  distribution</bold>].</p>
          </sec>
          <sec id="cell-27-nb-2" specific-use="notebook-content">
            <code language="python">def normal(x, mu, sigma):
    p = 1 / math.sqrt(2 * math.pi * sigma**2)

    return p * np.exp(-0.5 * (x - mu)**2 / sigma**2)</code>
          </sec>
          <sec id="cell-28-nb-2" specific-use="notebook-content">
            <p>We can now (<bold>visualize the normal distributions</bold>).</p>
          </sec>
          <sec id="cell-29-nb-2" specific-use="notebook-content">
            <code language="python">from d2l import torch as d2l
import matplotlib.pyplot as plt
# Use NumPy again for visualization
x = np.arange(-7, 7, 0.01)

# Mean and standard deviation pairs
params = [(0, 1), (0, 2), (3, 1)]
d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',
         ylabel='p(x)', figsize=(4.5, 2.5),
         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])
plt.show()</code>
          </sec>
          <p>Note that changing the mean corresponds to a shift along the
  <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>-axis,
  and increasing the variance spreads the distribution out, lowering its
  peak.</p>
          <sec id="cell-31-nb-2" specific-use="notebook-content">
            <p>One way to motivate linear regression with squared loss is to
  assume that observations arise from noisy measurements, where the
  noise <inline-formula><alternatives><tex-math><![CDATA[\epsilon]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>œµ</mml:mi></mml:math></alternatives></inline-formula>
  follows the normal distribution <inline-formula><alternatives><tex-math><![CDATA[\mathcal{N}(0, \sigma^2)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùí©</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mi>œÉ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>:</p>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[y = \mathbf{w}^\top \mathbf{x} + b + \epsilon \textrm{ where } \epsilon \sim \mathcal{N}(0, \sigma^2).]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>y</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:msup>
                        <mml:mi>ùê∞</mml:mi>
                        <mml:mi>‚ä§</mml:mi>
                      </mml:msup>
                      <mml:mi>ùê±</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi>b</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mi>œµ</mml:mi>
                      <mml:mrow>
                        <mml:mspace width="0.333em"/>
                        <mml:mtext mathvariant="normal"> where </mml:mtext>
                        <mml:mspace width="0.333em"/>
                      </mml:mrow>
                      <mml:mi>œµ</mml:mi>
                      <mml:mo>‚àº</mml:mo>
                      <mml:mi>ùí©</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>0</mml:mn>
                        <mml:mo>,</mml:mo>
                        <mml:msup>
                          <mml:mi>œÉ</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msup>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mi>.</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
            <p>Thus, we can now write out the <italic>likelihood</italic> of
  seeing a particular <inline-formula><alternatives><tex-math><![CDATA[y]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>
  for a given <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>
  via</p>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right).]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>P</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>y</mml:mi>
                        <mml:mo>‚à£</mml:mo>
                        <mml:mi>ùê±</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mn>1</mml:mn>
                        <mml:msqrt>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                            <mml:mi>œÄ</mml:mi>
                            <mml:msup>
                              <mml:mi>œÉ</mml:mi>
                              <mml:mn>2</mml:mn>
                            </mml:msup>
                          </mml:mrow>
                        </mml:msqrt>
                      </mml:mfrac>
                      <mml:mo>exp</mml:mo>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>‚àí</mml:mi>
                        <mml:mfrac>
                          <mml:mn>1</mml:mn>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                            <mml:msup>
                              <mml:mi>œÉ</mml:mi>
                              <mml:mn>2</mml:mn>
                            </mml:msup>
                          </mml:mrow>
                        </mml:mfrac>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:mi>y</mml:mi>
                            <mml:mo>‚àí</mml:mo>
                            <mml:msup>
                              <mml:mi>ùê∞</mml:mi>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msup>
                            <mml:mi>ùê±</mml:mi>
                            <mml:mo>‚àí</mml:mo>
                            <mml:mi>b</mml:mi>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:msup>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mi>.</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
            <p>As such, the likelihood factorizes. According to <italic>the
  principle of maximum likelihood</italic>, the best values of
  parameters <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê∞</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives><tex-math><![CDATA[b]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>
  are those that maximize the <italic>likelihood</italic> of the entire
  dataset:</p>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)} \mid \mathbf{x}^{(i)}).]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>P</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùê≤</mml:mi>
                        <mml:mo>‚à£</mml:mo>
                        <mml:mi>ùêó</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>=</mml:mo>
                      <mml:munderover>
                        <mml:mo>‚àè</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:munderover>
                      <mml:mi>p</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:msup>
                          <mml:mi>y</mml:mi>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:mi>i</mml:mi>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mo>‚à£</mml:mo>
                        <mml:msup>
                          <mml:mi>ùê±</mml:mi>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:mi>i</mml:mi>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mi>.</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
          </sec>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)} \mid \mathbf{x}^{(i)}).]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mi>ùê≤</mml:mi>
                      <mml:mo>‚à£</mml:mo>
                      <mml:mi>ùêó</mml:mi>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:munderover>
                      <mml:mo>‚àè</mml:mo>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>n</mml:mi>
                    </mml:munderover>
                    <mml:mi>p</mml:mi>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>i</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mo>‚à£</mml:mo>
                      <mml:msup>
                        <mml:mi>ùê±</mml:mi>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>i</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                    <mml:mi>.</mml:mi>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <list list-type="bullet">
            <list-item>
              <p>equality follows since all pairs <inline-formula><alternatives><tex-math><![CDATA[(\mathbf{x}^{(i)}, y^{(i)})]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>ùê±</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      were drawn independently of each other.</p>
            </list-item>
            <list-item>
              <p>Estimators chosen according to the principle of maximum
      likelihood are called <italic>maximum likelihood
      estimators</italic>.</p>
            </list-item>
            <list-item>
              <p>While, maximizing the product of many exponential functions,
      might look difficult, we can simplify things significantly,
      without changing the objective, by maximizing the logarithm of the
      likelihood instead.</p>
            </list-item>
            <list-item>
              <p>For historical reasons, optimizations are more often expressed
      as minimization rather than maximization.</p>
            </list-item>
          </list>
          <sec id="cell-33-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>without changing anything, we can <italic>minimize</italic> the
      <italic>negative log-likelihood</italic>, which we can express as
      follows:</p>
              </list-item>
            </list>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>‚àí</mml:mi>
                      <mml:mo>log</mml:mo>
                      <mml:mi>P</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>ùê≤</mml:mi>
                        <mml:mo>‚à£</mml:mo>
                        <mml:mi>ùêó</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>=</mml:mo>
                      <mml:munderover>
                        <mml:mo>‚àë</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:munderover>
                      <mml:mfrac>
                        <mml:mn>1</mml:mn>
                        <mml:mn>2</mml:mn>
                      </mml:mfrac>
                      <mml:mo>log</mml:mo>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mn>2</mml:mn>
                        <mml:mi>œÄ</mml:mi>
                        <mml:msup>
                          <mml:mi>œÉ</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msup>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>+</mml:mo>
                      <mml:mfrac>
                        <mml:mn>1</mml:mn>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:msup>
                            <mml:mi>œÉ</mml:mi>
                            <mml:mn>2</mml:mn>
                          </mml:msup>
                        </mml:mrow>
                      </mml:mfrac>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:msup>
                            <mml:mi>y</mml:mi>
                            <mml:mrow>
                              <mml:mo stretchy="true" form="prefix">(</mml:mo>
                              <mml:mi>i</mml:mi>
                              <mml:mo stretchy="true" form="postfix">)</mml:mo>
                            </mml:mrow>
                          </mml:msup>
                          <mml:mo>‚àí</mml:mo>
                          <mml:msup>
                            <mml:mi>ùê∞</mml:mi>
                            <mml:mi>‚ä§</mml:mi>
                          </mml:msup>
                          <mml:msup>
                            <mml:mi>ùê±</mml:mi>
                            <mml:mrow>
                              <mml:mo stretchy="true" form="prefix">(</mml:mo>
                              <mml:mi>i</mml:mi>
                              <mml:mo stretchy="true" form="postfix">)</mml:mo>
                            </mml:mrow>
                          </mml:msup>
                          <mml:mo>‚àí</mml:mo>
                          <mml:mi>b</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                      <mml:mi>.</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
            <list list-type="bullet">
              <list-item>
                <p>If we assume that <inline-formula><alternatives><tex-math><![CDATA[\sigma]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>œÉ</mml:mi></mml:math></alternatives></inline-formula>
      is fixed, we can ignore the first term, because it does not depend
      on <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê∞</mml:mi></mml:math></alternatives></inline-formula>
      or <inline-formula><alternatives><tex-math><![CDATA[b]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>.</p>
              </list-item>
              <list-item>
                <p>The second term is identical to the squared error loss
      introduced earlier, except for the ultiplicative constant
      <inline-formula><alternatives><tex-math><![CDATA[\frac{1}{\sigma^2}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>œÉ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:math></alternatives></inline-formula>.</p>
              </list-item>
              <list-item>
                <p>Fortunately, the solution does not depend on
      <inline-formula><alternatives><tex-math><![CDATA[\sigma]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>œÉ</mml:mi></mml:math></alternatives></inline-formula>
      either. It follows that minimizing the mean squared error is
      equivalent to the maximum likelihood estimation of a linear model
      under the assumption of additive Gaussian noise.</p>
              </list-item>
            </list>
          </sec>
          <sec id="linear-regression-as-a-neural-network-nb-2">
            <title>Linear Regression as a Neural Network</title>
            <list list-type="bullet">
              <list-item>
                <p>linear models not sufficiently rich to express the many
      complicated networks that we will introduce in this book</p>
              </list-item>
              <list-item>
                <p>(artificial) neural networks are rich enough to subsume linear
      models as networks in which every feature is represented by an
      input neuron, all of which are connected directly to the
      output.</p>
              </list-item>
              <list-item>
                <p>:numref:<monospace>fig_single_neuron</monospace> depicts linear
      regression as a neural network. The diagram highlights the
      connectivity pattern, such as how each input is connected to the
      output, but not the specific values taken by the weights or
      biases.</p>
              </list-item>
            </list>
            <p><inline-graphic mimetype="image" mime-subtype="svg+xml" xlink:href="../img/singleneuron.svg"><alt-text>Linear regression is a single-layer neural
    network.</alt-text></inline-graphic> :label:<monospace>fig_single_neuron</monospace></p>
            <list list-type="bullet">
              <list-item>
                <p>The inputs are <inline-formula><alternatives><tex-math><![CDATA[x_1, \ldots, x_d]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>‚Ä¶</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.
      We refer to <inline-formula><alternatives><tex-math><![CDATA[d]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>d</mml:mi></mml:math></alternatives></inline-formula>
      as the <italic>number of inputs</italic> or the <italic>feature
      dimensionality</italic> in the input layer. The output of the
      network is <inline-formula><alternatives><tex-math><![CDATA[o_1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>o</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>.
      Because we are just trying to predict a single numerical value, we
      have only one output neuron.</p>
              </list-item>
              <list-item>
                <p>Note that the input values are all <italic>given</italic>.
      There is just a single <italic>computed</italic> neuron. In
      summary, we can think of linear regression as a single-layer fully
      connected neural network.</p>
              </list-item>
              <list-item>
                <p>We will encounter networks with far more layers in later
      chapters.</p>
              </list-item>
            </list>
          </sec>
          <sec id="summary-nb-2">
            <title>Summary</title>
            <list list-type="bullet">
              <list-item>
                <p>introduced traditional linear regression, where the parameters
      of a linear function are chosen to minimize squared loss on the
      training set.</p>
              </list-item>
              <list-item>
                <p>We also motivated this choice of objective both via some
      practical considerations and through an interpretation of linear
      regression as maximimum likelihood estimation under an assumption
      of linearity and Gaussian noise.</p>
              </list-item>
              <list-item>
                <p>After discussing both computational considerations and
      connections to statistics, we showed how such linear models could
      be expressed as simple neural networks where the inputs are
      directly wired to the output(s).</p>
              </list-item>
              <list-item>
                <p>While we will soon move past linear models altogether, they are
      sufficient to introduce most of the components that all of our
      models require:</p>
              </list-item>
              <list-item>
                <p>parametric forms, differentiable objectives, optimization via
      minibatch stochastic gradient descent, and ultimately, evaluation
      on previously unseen data.</p>
              </list-item>
            </list>
          </sec>
          <sec id="exercises-nb-2">
            <title>Exercises</title>
            <list list-type="order">
              <list-item>
                <p>Assume that we have some data <inline-formula><alternatives><tex-math><![CDATA[x_1, \ldots, x_n \in \mathbb{R}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>‚Ä¶</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>‚àà</mml:mo><mml:mi>‚Ñù</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
      Our goal is to find a constant <inline-formula><alternatives><tex-math><![CDATA[b]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>
      such that <inline-formula><alternatives><tex-math><![CDATA[\sum_i (x_i - b)^2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mo>‚àë</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>‚àí</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      is minimized.</p>
                <list list-type="order">
                  <list-item>
                    <p>Find an analytic solution for the optimal value of
          <inline-formula><alternatives><tex-math><![CDATA[b]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>.</p>
                  </list-item>
                  <list-item>
                    <p>How does this problem and its solution relate to the normal
          distribution?</p>
                  </list-item>
                  <list-item>
                    <p>What if we change the loss from
          <inline-formula><alternatives><tex-math><![CDATA[\sum_i (x_i - b)^2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mo>‚àë</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>‚àí</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
          to <inline-formula><alternatives><tex-math><![CDATA[\sum_i |x_i-b|]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mo>‚àë</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>‚àí</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>?
          Can you find the optimal solution for
          <inline-formula><alternatives><tex-math><![CDATA[b]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>?</p>
                  </list-item>
                </list>
              </list-item>
              <list-item>
                <p>Prove that the affine functions that can be expressed by
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}^\top \mathbf{w} + b]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùê±</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùê∞</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      are equivalent to linear functions on
      <inline-formula><alternatives><tex-math><![CDATA[(\mathbf{x}, 1)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùê±</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
              </list-item>
              <list-item>
                <p>Assume that you want to find quadratic functions of
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>,
      i.e., <inline-formula><alternatives><tex-math><![CDATA[f(\mathbf{x}) = b + \sum_i w_i x_i + \sum_{j \leq i} w_{ij} x_{i} x_{j}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùê±</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mo>‚àë</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>‚â§</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.
      How would you formulate this in a deep network?</p>
              </list-item>
              <list-item>
                <p>Recall that one of the conditions for the linear regression
      problem to be solvable was that the design matrix
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}^\top \mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùêó</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùêó</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      has full rank.</p>
                <list list-type="order">
                  <list-item>
                    <p>What happens if this is not the case?</p>
                  </list-item>
                  <list-item>
                    <p>How could you fix it? What happens if you add a small
          amount of coordinate-wise independent Gaussian noise to all
          entries of <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>?</p>
                  </list-item>
                  <list-item>
                    <p>What is the expected value of the design matrix
          <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}^\top \mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùêó</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùêó</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
          in this case?</p>
                  </list-item>
                  <list-item>
                    <p>What happens with stochastic gradient descent when
          <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}^\top \mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùêó</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùêó</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
          does not have full rank?</p>
                  </list-item>
                </list>
              </list-item>
              <list-item>
                <p>Assume that the noise model governing the additive noise
      <inline-formula><alternatives><tex-math><![CDATA[\epsilon]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>œµ</mml:mi></mml:math></alternatives></inline-formula>
      is the exponential distribution. That is,
      <inline-formula><alternatives><tex-math><![CDATA[p(\epsilon) = \frac{1}{2} \exp(-|\epsilon|)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>œµ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>exp</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>‚àí</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>œµ</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
                <list list-type="order">
                  <list-item>
                    <p>Write out the negative log-likelihood of the data under the
          model <inline-formula><alternatives><tex-math><![CDATA[-\log P(\mathbf y \mid \mathbf X)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>‚àí</mml:mi><mml:mo>log</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùê≤</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>ùêó</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
                  </list-item>
                  <list-item>
                    <p>Can you find a closed form solution?</p>
                  </list-item>
                  <list-item>
                    <p>Suggest a minibatch stochastic gradient descent algorithm
          to solve this problem. What could possibly go wrong (hint:
          what happens near the stationary point as we keep on updating
          the parameters)? Can you fix this?</p>
                  </list-item>
                </list>
              </list-item>
              <list-item>
                <p>Assume that we want to design a neural network with two layers
      by composing two linear layers. That is, the output of the first
      layer becomes the input of the second layer. Why would such a
      naive composition not work?</p>
              </list-item>
              <list-item>
                <p>What happens if you want to use regression for realistic price
      estimation of houses or stock prices?</p>
                <list list-type="order">
                  <list-item>
                    <p>Show that the additive Gaussian noise assumption is not
          appropriate. Hint: can we have negative prices? What about
          fluctuations?</p>
                  </list-item>
                  <list-item>
                    <p>Why would regression to the logarithm of the price be much
          better, i.e., <inline-formula><alternatives><tex-math><![CDATA[y = \log \textrm{price}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mo>log</mml:mo><mml:mtext mathvariant="normal">price</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula>?</p>
                  </list-item>
                  <list-item>
                    <p>What do you need to worry about when dealing with
          pennystock, i.e., stock with very low prices? Hint: can you
          trade at all possible prices? Why is this a bigger problem for
          cheap stock? For more information review the celebrated
          Black‚ÄìScholes model for option pricing
          :cite:<monospace>Black.Scholes.1973</monospace>.</p>
                  </list-item>
                </list>
              </list-item>
              <list-item>
                <p>Suppose we want to use regression to estimate the
      <italic>number</italic> of apples sold in a grocery store.</p>
                <list list-type="order">
                  <list-item>
                    <p>What are the problems with a Gaussian additive noise model?
          Hint: you are selling apples, not oil.</p>
                  </list-item>
                  <list-item>
                    <p>The
          <ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson
          distribution</ext-link> captures distributions over counts. It
          is given by <inline-formula><alternatives><tex-math><![CDATA[p(k \mid \lambda) = \lambda^k e^{-\lambda}/k!]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>k</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>Œª</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>Œª</mml:mi><mml:mi>k</mml:mi></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>‚àí</mml:mi><mml:mi>Œª</mml:mi></mml:mrow></mml:msup><mml:mi>/</mml:mi><mml:mi>k</mml:mi><mml:mi>!</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
          Here <inline-formula><alternatives><tex-math><![CDATA[\lambda]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>Œª</mml:mi></mml:math></alternatives></inline-formula>
          is the rate function and <inline-formula><alternatives><tex-math><![CDATA[k]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>
          is the number of events you see. Prove that
          <inline-formula><alternatives><tex-math><![CDATA[\lambda]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>Œª</mml:mi></mml:math></alternatives></inline-formula>
          is the expected value of counts <inline-formula><alternatives><tex-math><![CDATA[k]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>.</p>
                  </list-item>
                  <list-item>
                    <p>Design a loss function associated with the Poisson
          distribution.</p>
                  </list-item>
                  <list-item>
                    <p>Design a loss function for estimating
          <inline-formula><alternatives><tex-math><![CDATA[\log \lambda]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo>log</mml:mo><mml:mi>Œª</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
          instead.</p>
                  </list-item>
                </list>
              </list-item>
            </list>
          </sec>
          <sec id="goal-of-optimization-nb-2">
            <title>Goal of Optimization</title>
            <list list-type="bullet">
              <list-item>
                <p>optimization provides a way to minimize the loss function for
      deep learning, in essence, the goals of optimization and deep
      learning are fundamentally different.</p>
              </list-item>
              <list-item>
                <p>optimization: concerned with minimizing an objective</p>
              </list-item>
              <list-item>
                <p>deep learning: concerned with finding a suitable model, given a
      finite amount of data.</p>
              </list-item>
            </list>
          </sec>
          <sec id="cell-34-nb-2" specific-use="notebook-content">
            <code language="python">%matplotlib inline
from d2l import torch as d2l
import numpy as np
from mpl_toolkits import mplot3d
import torch</code>
          </sec>
          <sec id="cell-35-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>To illustrate the aforementioned different goals, let‚Äôs
      consider the empirical risk and the risk.</p>
              </list-item>
              <list-item>
                <p>As described in
      :numref:<monospace>subsec_empirical-risk-and-risk</monospace>, the
      empirical risk is an average loss on the training dataset while
      the risk is the expected loss on the entire population of
      data.</p>
              </list-item>
              <list-item>
                <p>Below we define two functions: the risk function
      <monospace>f</monospace> and the empirical risk function
      <monospace>g</monospace>. Suppose that we have only a finite
      amount of training data. As a result, here
      <monospace>g</monospace> is less smooth than
      <monospace>f</monospace>.</p>
              </list-item>
            </list>
          </sec>
          <sec id="cell-36-nb-2" specific-use="notebook-content">
            <code language="python">def f(x):
    return x * torch.cos(np.pi * x)

def g(x):
    return f(x) + 0.2 * torch.cos(5 * np.pi * x)</code>
          </sec>
          <sec id="cell-38-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>graph below illustrates that the minimum of the empirical risk
      on a training dataset may be at a different location from the
      minimum of the risk (generalization error).</p>
              </list-item>
            </list>
          </sec>
          <sec id="cell-39-nb-2" specific-use="notebook-content">
            <code language="python">def annotate(text, xy, xytext):  #@save
    d2l.plt.gca().annotate(text, xy=xy, xytext=xytext,
                           arrowprops=dict(arrowstyle='-&gt;'))

x = d2l.arange(0.5, 1.5, 0.01)
d2l.set_figsize((4.5, 2.5))
d2l.plot(x, [f(x), g(x)], 'x', 'risk')
annotate('min of\nempirical risk', (1.0, -1.2), (0.5, -1.1))
annotate('min of risk', (1.1, -1.05), (0.95, -0.5))</code>
          </sec>
          <sec id="cell-40-nb-2" specific-use="notebook-content">
</sec>
          <sec id="optimization-challenges-in-deep-learning-nb-2">
            <title>Optimization Challenges in Deep Learning</title>
            <list list-type="bullet">
              <list-item>
                <p>focus specifically on the performance of optimization
      algorithms in minimizing the objective function, rather than a
      model‚Äôs generalization error.</p>
              </list-item>
              <list-item>
                <p>In :numref:<monospace>sec_linear_regression</monospace> we
      distinguished between analytical solutions and numerical solutions
      in optimization problems.</p>
              </list-item>
              <list-item>
                <p>In deep learning, most objective functions are complicated and
      do not have analytical solutions. Instead, we must use numerical
      optimization algorithms.</p>
              </list-item>
              <list-item>
                <p>The optimization algorithms in this chapter all fall into this
      category.</p>
              </list-item>
              <list-item>
                <p>There are many challenges in deep learning optimization. Some
      of the most vexing ones are local minima, saddle points, and
      vanishing gradients. Let‚Äôs have a look at them.</p>
              </list-item>
            </list>
          </sec>
          <sec id="local-minima-nb-2">
            <title>Local Minima</title>
            <list list-type="bullet">
              <list-item>
                <p>For any objective function <inline-formula><alternatives><tex-math><![CDATA[f(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
      if the value of <inline-formula><alternatives><tex-math><![CDATA[f(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      at <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      is smaller than the values of <inline-formula><alternatives><tex-math><![CDATA[f(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      at any other points in the vicinity of
      <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>,
      then <inline-formula><alternatives><tex-math><![CDATA[f(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      could be a local minimum.</p>
              </list-item>
              <list-item>
                <p>If the value of <inline-formula><alternatives><tex-math><![CDATA[f(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      at <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      is the minimum of the objective function over the entire domain,
      then <inline-formula><alternatives><tex-math><![CDATA[f(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      is the global minimum.</p>
              </list-item>
              <list-item>
                <p>For example, given the function</p>
              </list-item>
            </list>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[f(x) = x \cdot \textrm{cos}(\pi x) \textrm{ for } -1.0 \leq x \leq 2.0,]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mi>f</mml:mi>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>x</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>=</mml:mo>
                      <mml:mi>x</mml:mi>
                      <mml:mo>‚ãÖ</mml:mo>
                      <mml:mtext mathvariant="normal">cos</mml:mtext>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>œÄ</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mspace width="0.333em"/>
                        <mml:mtext mathvariant="normal"> for </mml:mtext>
                        <mml:mspace width="0.333em"/>
                      </mml:mrow>
                      <mml:mo>‚àí</mml:mo>
                      <mml:mn>1.0</mml:mn>
                      <mml:mo>‚â§</mml:mo>
                      <mml:mi>x</mml:mi>
                      <mml:mo>‚â§</mml:mo>
                      <mml:mn>2.0</mml:mn>
                      <mml:mo>,</mml:mo>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
          </sec>
          <sec id="cell-41-nb-2" specific-use="notebook-content">
            <p>we can approximate the local minimum and global minimum of this
  function.</p>
          </sec>
          <sec id="cell-42-nb-2" specific-use="notebook-content">
            <code language="python">x = torch.arange(-1.0, 2.0, 0.01)
d2l.plot(x, [f(x), ], 'x', 'f(x)')
annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))
annotate('global minimum', (1.1, -0.95), (0.6, 0.8))</code>
          </sec>
          <sec id="cell-43-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>objective function of deep learning models usually has many
      local optima.</p>
              </list-item>
              <list-item>
                <p>When the numerical solution of an optimization problem is near
      the local optimum, the numerical solution obtained by the final
      iteration may only minimize the objective function
      <italic>locally</italic>, rather than <italic>globally</italic>,
      as the gradient of the objective function‚Äôs solutions approaches
      or becomes zero.</p>
              </list-item>
              <list-item>
                <p>Only some degree of noise might knock the parameter out of the
      local minimum. In fact, this is one of the beneficial properties
      of minibatch stochastic gradient descent where the natural
      variation of gradients over minibatches is able to dislodge the
      parameters from local minima.</p>
              </list-item>
            </list>
          </sec>
          <sec id="saddle-points-nb-2">
            <title>Saddle Points</title>
            <list list-type="bullet">
              <list-item>
                <p>Besides local minima, saddle points are another reason for
      gradients to vanish. A <italic>saddle point</italic> is any
      location where all gradients of a function vanish but which is
      neither a global nor a local minimum.</p>
              </list-item>
              <list-item>
                <p>Consider the function <inline-formula><alternatives><tex-math><![CDATA[f(x) = x^3]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.
      Its first and second derivative vanish for
      <inline-formula><alternatives><tex-math><![CDATA[x=0]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
      Optimization might stall at this point, even though it is not a
      minimum.</p>
              </list-item>
            </list>
          </sec>
          <sec id="cell-44-nb-2" specific-use="notebook-content">
            <code language="python">x = d2l.arange(-2.0, 2.0, 0.01)
d2l.plot(x, [x**3], 'x', 'f(x)')
annotate('saddle point', (0, -0.2), (-0.52, -5.0))</code>
          </sec>
          <sec id="cell-46-nb-2" specific-use="notebook-content">
            <list list-type="bullet">
              <list-item>
                <p>Saddle points in higher dimensions are even more insidious, as
      the example below shows. Consider the function
      <inline-formula><alternatives><tex-math><![CDATA[f(x, y) = x^2 - y^2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>‚àí</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.
      It has its saddle point at <inline-formula><alternatives><tex-math><![CDATA[(0, 0)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.
      This is a maximum with respect to <inline-formula><alternatives><tex-math><![CDATA[y]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>
      and a minimum with respect to <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>.
      Moreover, it <italic>looks</italic> like a saddle, which is where
      this mathematical property got its name.</p>
              </list-item>
            </list>
          </sec>
          <sec id="cell-47-nb-2" specific-use="notebook-content">
            <code language="python">x, y = d2l.meshgrid(
    d2l.linspace(-1.0, 1.0, 101), d2l.linspace(-1.0, 1.0, 101))
z = x**2 - y**2

ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})
ax.plot([0], [0], [0], 'rx')
ticks = [-1, 0, 1]
d2l.plt.xticks(ticks)
d2l.plt.yticks(ticks)
ax.set_zticks(ticks)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y');</code>
          </sec>
          <list list-type="bullet">
            <list-item>
              <p>We assume that the input of a function is a
      <inline-formula><alternatives><tex-math><![CDATA[k]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>-dimensional
      vector and its output is a scalar, so its Hessian matrix will have
      <inline-formula><alternatives><tex-math><![CDATA[k]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>
      eigenvalues.</p>
            </list-item>
            <list-item>
              <p>The solution of the function could be a local minimum, a local
      maximum, or a saddle point at a position where the function
      gradient is zero:</p>
            </list-item>
            <list-item>
              <p>When the eigenvalues of the function‚Äôs Hessian matrix at the
      zero-gradient position are all positive, we have a local minimum
      for the function.</p>
            </list-item>
            <list-item>
              <p>When the eigenvalues of the function‚Äôs Hessian matrix at the
      zero-gradient position are all negative, we have a local maximum
      for the function.</p>
            </list-item>
            <list-item>
              <p>When the eigenvalues of the function‚Äôs Hessian matrix at the
      zero-gradient position are negative and positive, we have a saddle
      point for the function.</p>
            </list-item>
          </list>
          <sec id="cell-49-nb-2" specific-use="notebook-content">
            <p>For high-dimensional problems the likelihood that at least
  <italic>some</italic> of the eigenvalues are negative is quite high.
  This makes saddle points more likely than local minima. We will
  discuss some exceptions to this situation in the next section when
  introducing convexity. In short, convex functions are those where the
  eigenvalues of the Hessian are never negative. Sadly, though, most
  deep learning problems do not fall into this category. Nonetheless it
  is a great tool to study optimization algorithms.</p>
          </sec>
          <sec id="vanishing-gradients-nb-2">
            <title>Vanishing Gradients</title>
            <p>Probably the most insidious problem to encounter is the vanishing
  gradient. Recall our commonly-used activation functions and their
  derivatives in
  :numref:<monospace>subsec_activation-functions</monospace>. For
  instance, assume that we want to minimize the function
  <inline-formula><alternatives><tex-math><![CDATA[f(x) = \tanh(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>tanh</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  and we happen to get started at <inline-formula><alternatives><tex-math><![CDATA[x = 4]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
  As we can see, the gradient of <inline-formula><alternatives><tex-math><![CDATA[f]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  is close to nil.</p>
          </sec>
          <sec id="cell-50-nb-2" specific-use="notebook-content">
            <p>More specifically, <inline-formula><alternatives><tex-math><![CDATA[f'(x) = 1 - \tanh^2(x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mi>‚Ä≤</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>‚àí</mml:mo><mml:msup><mml:mo>tanh</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  and thus <inline-formula><alternatives><tex-math><![CDATA[f'(4) = 0.0013]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mi>‚Ä≤</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.0013</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
  Consequently, optimization will get stuck for a long time before we
  make progress. This turns out to be one of the reasons that training
  deep learning models was quite tricky prior to the introduction of the
  ReLU activation function.</p>
          </sec>
          <sec id="cell-51-nb-2" specific-use="notebook-content">
            <code language="python">#@tab all
x = d2l.arange(-2.0, 5.0, 0.01)
d2l.plot(x, [d2l.tanh(x)], 'x', 'f(x)')
annotate('vanishing gradient', (4, 1), (2, 0.0))</code>
          </sec>
          <sec id="cell-53-nb-2" specific-use="notebook-content">
            <p>As we saw, optimization for deep learning is full of challenges.
  Fortunately there exists a robust range of algorithms that perform
  well and that are easy to use even for beginners. Furthermore, it is
  not really necessary to find <italic>the</italic> best solution. Local
  optima or even approximate solutions thereof are still very
  useful.</p>
          </sec>
          <sec id="summary-1-nb-2">
            <title>Summary</title>
            <list list-type="bullet">
              <list-item>
                <p>Minimizing the training error does <italic>not</italic>
      guarantee that we find the best set of parameters to minimize the
      generalization error.</p>
              </list-item>
              <list-item>
                <p>The optimization problems may have many local minima.</p>
              </list-item>
              <list-item>
                <p>The problem may have even more saddle points, as generally the
      problems are not convex.</p>
              </list-item>
              <list-item>
                <p>Vanishing gradients can cause optimization to stall. Often a
      reparametrization of the problem helps. Good initialization of the
      parameters can be beneficial, too.</p>
              </list-item>
            </list>
          </sec>
          <sec id="exercises-1-nb-2">
            <title>Exercises</title>
            <list list-type="order">
              <list-item>
                <p>Consider a simple MLP with a single hidden layer of, say,
      <inline-formula><alternatives><tex-math><![CDATA[d]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>d</mml:mi></mml:math></alternatives></inline-formula>
      dimensions in the hidden layer and a single output. Show that for
      any local minimum there are at least
      <inline-formula><alternatives><tex-math><![CDATA[d!]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>d</mml:mi><mml:mi>!</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      equivalent solutions that behave identically.</p>
              </list-item>
              <list-item>
                <p>Assume that we have a symmetric random matrix
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{M}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêå</mml:mi></mml:math></alternatives></inline-formula>
      where the entries <inline-formula><alternatives><tex-math><![CDATA[M_{ij} = M_{ji}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>
      are each drawn from some probability distribution
      <inline-formula><alternatives><tex-math><![CDATA[p_{ij}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>.
      Furthermore assume that <inline-formula><alternatives><tex-math><![CDATA[p_{ij}(x) = p_{ij}(-x)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>‚àí</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
      i.e., that the distribution is symmetric (see e.g.,
      :citet:<monospace>Wigner.1958</monospace> for details).</p>
                <list list-type="order">
                  <list-item>
                    <p>Prove that the distribution over eigenvalues is also
          symmetric. That is, for any eigenvector
          <inline-formula><alternatives><tex-math><![CDATA[\mathbf{v}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêØ</mml:mi></mml:math></alternatives></inline-formula>
          the probability that the associated eigenvalue
          <inline-formula><alternatives><tex-math><![CDATA[\lambda]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>Œª</mml:mi></mml:math></alternatives></inline-formula>
          satisfies <inline-formula><alternatives><tex-math><![CDATA[P(\lambda > 0) = P(\lambda < 0)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œª</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œª</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
                  </list-item>
                  <list-item>
                    <p>Why does the above <italic>not</italic> imply
          <inline-formula><alternatives><tex-math><![CDATA[P(\lambda > 0) = 0.5]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œª</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>?</p>
                  </list-item>
                </list>
              </list-item>
              <list-item>
                <p>What other challenges involved in deep learning optimization
      can you think of?</p>
              </list-item>
              <list-item>
                <p>Assume that you want to balance a (real) ball on a (real)
      saddle.</p>
                <list list-type="order">
                  <list-item>
                    <p>Why is this hard?</p>
                  </list-item>
                  <list-item>
                    <p>Can you exploit this effect also for optimization
          algorithms?</p>
                  </list-item>
                </list>
              </list-item>
            </list>
          </sec>
        </sec>
      </sec>
    </body>
    <back>
</back>
  </sub-article>
  <sub-article article-type="notebook" id="nb-6-nb-3">
    <front-stub>
      <title-group>
        <article-title>01.2 Linear Algebra</article-title>
      </title-group>
    </front-stub>
    <body>
      <sec id="a622c08e-nb-3" specific-use="notebook-content">
        <p>By now, we can load datasets into tensors and manipulate these
tensors with basic mathematical operations. To start building
sophisticated models, we will also need a few tools from linear algebra.
This section offers a gentle introduction to the most essential
concepts, starting from scalar arithmetic and ramping up to matrix
multiplication.</p>
      </sec>
      <sec id="dc36b473-nb-3" specific-use="notebook-content">
        <code language="python">import torch</code>
      </sec>
      <sec id="e4a38674-nb-3" specific-use="notebook-content">
        <sec id="scalars-nb-3">
          <title>Scalars</title>
          <p>Most everyday mathematics consists of manipulating numbers one at a
  time. Formally, we call these values <italic>scalars</italic>. For
  example, the temperature in Palo Alto is a balmy
  <inline-formula><alternatives><tex-math><![CDATA[72]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>72</mml:mn></mml:math></alternatives></inline-formula>
  degrees Fahrenheit. If you wanted to convert the temperature to
  Celsius you would evaluate the expression
  <inline-formula><alternatives><tex-math><![CDATA[c = \frac{5}{9}(f - 32)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>5</mml:mn><mml:mn>9</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>f</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>32</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  setting <inline-formula><alternatives><tex-math><![CDATA[f]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  to <inline-formula><alternatives><tex-math><![CDATA[72]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>72</mml:mn></mml:math></alternatives></inline-formula>.
  In this equation, the values <inline-formula><alternatives><tex-math><![CDATA[5]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>5</mml:mn></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives><tex-math><![CDATA[9]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>9</mml:mn></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives><tex-math><![CDATA[32]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>32</mml:mn></mml:math></alternatives></inline-formula>
  are constant scalars. The variables <inline-formula><alternatives><tex-math><![CDATA[c]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>c</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives><tex-math><![CDATA[f]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  in general represent unknown scalars.</p>
          <p>We denote scalars by ordinary lower-cased letters (e.g.,
  <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives><tex-math><![CDATA[y]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives><tex-math><![CDATA[z]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>z</mml:mi></mml:math></alternatives></inline-formula>)
  and the space of all (continuous) <italic>real-valued</italic> scalars
  by <inline-formula><alternatives><tex-math><![CDATA[\mathbb{R}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚Ñù</mml:mi></mml:math></alternatives></inline-formula>.
  For expedience, we will skip past rigorous definitions of
  <italic>spaces</italic>: just remember that the expression
  <inline-formula><alternatives><tex-math><![CDATA[x \in \mathbb{R}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>x</mml:mi><mml:mo>‚àà</mml:mo><mml:mi>‚Ñù</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  is a formal way to say that <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
  is a real-valued scalar. The symbol <inline-formula><alternatives><tex-math><![CDATA[\in]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mo>‚àà</mml:mo></mml:math></alternatives></inline-formula>
  (pronounced ‚Äúin‚Äù) denotes membership in a set. For example,
  <inline-formula><alternatives><tex-math><![CDATA[x, y \in \{0, 1\}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>‚àà</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  indicates that <inline-formula><alternatives><tex-math><![CDATA[x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives><tex-math><![CDATA[y]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>
  are variables that can only take values <inline-formula><alternatives><tex-math><![CDATA[0]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>
  or <inline-formula><alternatives><tex-math><![CDATA[1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula>.</p>
          <p>(<bold>Scalars are implemented as tensors that contain only one
  element.</bold>) Below, we assign two scalars and perform the familiar
  addition, multiplication, division, and exponentiation operations.</p>
        </sec>
        <sec id="cell-4fc9ba1d-nb-3" specific-use="notebook-content">
          <code language="python">x = torch.tensor(3.0)
y = torch.tensor(2.0)

x + y, x * y, x / y, x**y</code>
          <boxed-text>
            <preformat>(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-0b20517b-nb-3" specific-use="notebook-content">
</sec>
        <sec id="vectors-nb-3">
          <title>Vectors</title>
          <p>For current purposes, [<bold>you can think of a vector as a
  fixed-length array of scalars.</bold>] As with their code
  counterparts, we call these scalars the <italic>elements</italic> of
  the vector (synonyms include <italic>entries</italic> and
  <italic>components</italic>). When vectors represent examples from
  real-world datasets, their values hold some real-world significance.
  For example, if we were training a model to predict the risk of a loan
  defaulting, we might associate each applicant with a vector whose
  components correspond to quantities like their income, length of
  employment, or number of previous defaults. If we were studying the
  risk of heart attack, each vector might represent a patient and its
  components might correspond to their most recent vital signs,
  cholesterol levels, minutes of exercise per day, etc. We denote
  vectors by bold lowercase letters, (e.g.,
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê≤</mml:mi></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives><tex-math><![CDATA[\mathbf{z}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê≥</mml:mi></mml:math></alternatives></inline-formula>).</p>
          <p>Vectors are implemented as <inline-formula><alternatives><tex-math><![CDATA[1^{\textrm{st}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mn>1</mml:mn><mml:mtext mathvariant="normal">st</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>-order
  tensors. In general, such tensors can have arbitrary lengths, subject
  to memory limitations. Caution: in Python, as in most programming
  languages, vector indices start at <inline-formula><alternatives><tex-math><![CDATA[0]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>,
  also known as <italic>zero-based indexing</italic>, whereas in linear
  algebra subscripts begin at <inline-formula><alternatives><tex-math><![CDATA[1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula>
  (one-based indexing).</p>
        </sec>
        <sec id="cell-91cd966f-nb-3" specific-use="notebook-content">
          <code language="python">x = torch.arange(3)
x</code>
          <boxed-text>
            <preformat>tensor([0, 1, 2])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-7a603956-nb-3" specific-use="notebook-content">
          <p>We can refer to an element of a vector by using a subscript. For
  example, <inline-formula><alternatives><tex-math><![CDATA[x_2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  denotes the second element of <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>.
  Since <inline-formula><alternatives><tex-math><![CDATA[x_2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  is a scalar, we do not bold it. By default, we visualize vectors by
  stacking their elements vertically.</p>
          <p><disp-formula><alternatives><tex-math><![CDATA[\mathbf{x} =\begin{bmatrix}x_{1}  \\ \vdots  \\x_{n}\end{bmatrix},]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mi>ùê±</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center" style="text-align: center"><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center" style="text-align: center"><mml:mi>‚ãÆ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center" style="text-align: center"><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
  :eqlabel:<monospace>eq_vec_def</monospace></p>
          <p>Here <inline-formula><alternatives><tex-math><![CDATA[x_1, \ldots, x_n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>‚Ä¶</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>
  are elements of the vector. Later on, we will distinguish between such
  <italic>column vectors</italic> and <italic>row vectors</italic> whose
  elements are stacked horizontally. Recall that [<bold>we access a
  tensor‚Äôs elements via indexing.</bold>]</p>
        </sec>
        <sec id="ba15a197-nb-3" specific-use="notebook-content">
          <code language="python">x[2]</code>
          <boxed-text>
            <preformat>tensor(2)</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-02a04823-nb-3" specific-use="notebook-content">
          <p>To indicate that a vector contains <inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
  elements, we write <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x} \in \mathbb{R}^n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùê±</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.
  Formally, we call <inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
  the <italic>dimensionality</italic> of the vector. [<bold>In code,
  this corresponds to the tensor‚Äôs length</bold>], accessible via
  Python‚Äôs built-in <monospace>len</monospace> function.</p>
        </sec>
        <sec id="cell-871cd7e6-nb-3" specific-use="notebook-content">
          <code language="python">len(x)</code>
          <boxed-text>
            <preformat>3</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-4e190d69-nb-3" specific-use="notebook-content">
          <p>We can also access the length via the <monospace>shape</monospace>
  attribute. The shape is a tuple that indicates a tensor‚Äôs length along
  each axis. (<bold>Tensors with just one axis have shapes with just one
  element.</bold>)</p>
        </sec>
        <sec id="cell-34ea04c3-nb-3" specific-use="notebook-content">
          <code language="python">x.shape</code>
          <boxed-text>
            <preformat>torch.Size([3])</preformat>
          </boxed-text>
        </sec>
        <sec id="e416733f-nb-3" specific-use="notebook-content">
          <p>Oftentimes, the word ‚Äúdimension‚Äù gets overloaded to mean both the
  number of axes and the length along a particular axis. To avoid this
  confusion, we use <italic>order</italic> to refer to the number of
  axes and <italic>dimensionality</italic> exclusively to refer to the
  number of components.</p>
        </sec>
        <sec id="matrices-nb-3">
          <title>Matrices</title>
          <p>Just as scalars are <inline-formula><alternatives><tex-math><![CDATA[0^{\textrm{th}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mn>0</mml:mn><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>-order
  tensors and vectors are <inline-formula><alternatives><tex-math><![CDATA[1^{\textrm{st}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mn>1</mml:mn><mml:mtext mathvariant="normal">st</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>-order
  tensors, matrices are <inline-formula><alternatives><tex-math><![CDATA[2^{\textrm{nd}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mn>2</mml:mn><mml:mtext mathvariant="normal">nd</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>-order
  tensors. We denote matrices by bold capital letters (e.g.,
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{Y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêò</mml:mi></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives><tex-math><![CDATA[\mathbf{Z}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêô</mml:mi></mml:math></alternatives></inline-formula>),
  and represent them in code by tensors with two axes. The expression
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A} \in \mathbb{R}^{m \times n}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  indicates that a matrix <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÄ</mml:mi></mml:math></alternatives></inline-formula>
  contains <inline-formula><alternatives><tex-math><![CDATA[m \times n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  real-valued scalars, arranged as <inline-formula><alternatives><tex-math><![CDATA[m]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>m</mml:mi></mml:math></alternatives></inline-formula>
  rows and <inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
  columns. When <inline-formula><alternatives><tex-math><![CDATA[m = n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
  we say that a matrix is <italic>square</italic>. Visually, we can
  illustrate any matrix as a table. To refer to an individual element,
  we subscript both the row and column indices, e.g.,
  <inline-formula><alternatives><tex-math><![CDATA[a_{ij}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
  is the value that belongs to <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÄ</mml:mi></mml:math></alternatives></inline-formula>‚Äôs
  <inline-formula><alternatives><tex-math><![CDATA[i^{\textrm{th}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>i</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
  row and <inline-formula><alternatives><tex-math><![CDATA[j^{\textrm{th}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>j</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
  column:</p>
          <p><disp-formula><alternatives><tex-math><![CDATA[\mathbf{A}=\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \\ \end{bmatrix}.]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center" style="text-align: center"><mml:msub><mml:mi>a</mml:mi><mml:mn>11</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:msub><mml:mi>a</mml:mi><mml:mn>12</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:mi>‚ãØ</mml:mi></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center" style="text-align: center"><mml:msub><mml:mi>a</mml:mi><mml:mn>21</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:msub><mml:mi>a</mml:mi><mml:mn>22</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:mi>‚ãØ</mml:mi></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center" style="text-align: center"><mml:mi>‚ãÆ</mml:mi></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:mi>‚ãÆ</mml:mi></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:mo>‚ã±</mml:mo></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:mi>‚ãÆ</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center" style="text-align: center"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:mi>‚ãØ</mml:mi></mml:mtd><mml:mtd columnalign="center" style="text-align: center"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center" style="text-align: center"/></mml:mtr></mml:mtable><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
  :eqlabel:<monospace>eq_matrix_def</monospace></p>
          <p>In code, we represent a matrix <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A} \in \mathbb{R}^{m \times n}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  by a <inline-formula><alternatives><tex-math><![CDATA[2^{\textrm{nd}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mn>2</mml:mn><mml:mtext mathvariant="normal">nd</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>-order
  tensor with shape (<inline-formula><alternatives><tex-math><![CDATA[m]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>m</mml:mi></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>).
  [<bold>We can convert any appropriately sized
  <inline-formula><alternatives><tex-math><![CDATA[m \times n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  tensor into an <inline-formula><alternatives><tex-math><![CDATA[m \times n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  matrix</bold>] by passing the desired shape to
  <monospace>reshape</monospace>:</p>
        </sec>
        <sec id="cell-80c49751-nb-3" specific-use="notebook-content">
          <code language="python">A = torch.arange(6).reshape(3, 2)
A</code>
          <boxed-text>
            <preformat>tensor([[0, 1],
        [2, 3],
        [4, 5]])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-0d90da51-nb-3" specific-use="notebook-content">
          <p>Sometimes we want to flip the axes. When we exchange a matrix‚Äôs
  rows and columns, the result is called its <italic>transpose</italic>.
  Formally, we signify a matrix <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÄ</mml:mi></mml:math></alternatives></inline-formula>‚Äôs
  transpose by <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}^\top]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>ùêÄ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup></mml:math></alternatives></inline-formula>
  and if <inline-formula><alternatives><tex-math><![CDATA[\mathbf{B} = \mathbf{A}^\top]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÅ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>ùêÄ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
  then <inline-formula><alternatives><tex-math><![CDATA[b_{ij} = a_{ji}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>
  for all <inline-formula><alternatives><tex-math><![CDATA[i]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives><tex-math><![CDATA[j]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>j</mml:mi></mml:math></alternatives></inline-formula>.
  Thus, the transpose of an <inline-formula><alternatives><tex-math><![CDATA[m \times n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  matrix is an <inline-formula><alternatives><tex-math><![CDATA[n \times m]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>n</mml:mi><mml:mo>√ó</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  matrix:</p>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
  \mathbf{A}^\top =
  \begin{bmatrix}
      a_{11} & a_{21} & \dots  & a_{m1} \\
      a_{12} & a_{22} & \dots  & a_{m2} \\
      \vdots & \vdots & \ddots  & \vdots \\
      a_{1n} & a_{2n} & \dots  & a_{mn}
  \end{bmatrix}.
  ]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:msup>
                      <mml:mi>ùêÄ</mml:mi>
                      <mml:mi>‚ä§</mml:mi>
                    </mml:msup>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>11</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>21</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚Ä¶</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mi>m</mml:mi>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>12</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>22</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚Ä¶</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mi>m</mml:mi>
                                <mml:mn>2</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mo>‚ã±</mml:mo>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mn>1</mml:mn>
                                <mml:mi>n</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mn>2</mml:mn>
                                <mml:mi>n</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚Ä¶</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mi>m</mml:mi>
                                <mml:mi>n</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mi>.</mml:mi>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <p>In code, we can access any (<bold>matrix‚Äôs transpose</bold>) as
  follows:</p>
        </sec>
        <sec id="cell-7ef1e23b-nb-3" specific-use="notebook-content">
          <code language="python">A.T</code>
          <boxed-text>
            <preformat>tensor([[0, 2, 4],
        [1, 3, 5]])</preformat>
          </boxed-text>
        </sec>
        <sec id="ce337753-nb-3" specific-use="notebook-content">
          <p>[<bold>Symmetric matrices are the subset of square matrices that
  are equal to their own transposes: <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A} = \mathbf{A}^\top]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>ùêÄ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</bold>]
  The following matrix is symmetric:</p>
        </sec>
        <sec id="cell-028e06ed-nb-3" specific-use="notebook-content">
          <code language="python">A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
A == A.T</code>
          <boxed-text>
            <preformat>tensor([[True, True, True],
        [True, True, True],
        [True, True, True]])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-2f945d82-nb-3" specific-use="notebook-content">
          <p>Matrices are useful for representing datasets. Typically, rows
  correspond to individual records and columns correspond to distinct
  attributes.</p>
        </sec>
        <sec id="tensors-nb-3">
          <title>Tensors</title>
          <p>While you can go far in your machine learning journey with only
  scalars, vectors, and matrices, eventually you may need to work with
  higher-order [<bold>tensors</bold>]. Tensors (<bold>give us a generic
  way of describing extensions to <inline-formula><alternatives><tex-math><![CDATA[n^{\textrm{th}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>n</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>-order
  arrays.</bold>) We call software objects of the <italic>tensor
  class</italic> ‚Äútensors‚Äù precisely because they too can have arbitrary
  numbers of axes. While it may be confusing to use the word
  <italic>tensor</italic> for both the mathematical object and its
  realization in code, our meaning should usually be clear from context.
  We denote general tensors by capital letters with a special font face
  (e.g., <inline-formula><alternatives><tex-math><![CDATA[\mathsf{X}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùñ∑</mml:mi></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives><tex-math><![CDATA[\mathsf{Y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùñ∏</mml:mi></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives><tex-math><![CDATA[\mathsf{Z}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùñπ</mml:mi></mml:math></alternatives></inline-formula>)
  and their indexing mechanism (e.g., <inline-formula><alternatives><tex-math><![CDATA[x_{ijk}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives><tex-math><![CDATA[[\mathsf{X}]_{1, 2i-1, 3}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>ùñ∑</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>)
  follows naturally from that of matrices.</p>
          <p>Tensors will become more important when we start working with
  images. Each image arrives as a <inline-formula><alternatives><tex-math><![CDATA[3^{\textrm{rd}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mn>3</mml:mn><mml:mtext mathvariant="normal">rd</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>-order
  tensor with axes corresponding to the height, width, and
  <italic>channel</italic>. At each spatial location, the intensities of
  each color (red, green, and blue) are stacked along the channel.
  Furthermore, a collection of images is represented in code by a
  <inline-formula><alternatives><tex-math><![CDATA[4^{\textrm{th}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mn>4</mml:mn><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>-order
  tensor, where distinct images are indexed along the first axis.
  Higher-order tensors are constructed, as were vectors and matrices, by
  growing the number of shape components.</p>
        </sec>
        <sec id="cell-306d610e-nb-3" specific-use="notebook-content">
          <code language="python">torch.arange(24).reshape(2, 3, 4)</code>
          <boxed-text>
            <preformat>tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-3113bae4-nb-3" specific-use="notebook-content">
</sec>
        <sec id="basic-properties-of-tensor-arithmetic-nb-3">
          <title>Basic Properties of Tensor Arithmetic</title>
          <p>Scalars, vectors, matrices, and higher-order tensors all have some
  handy properties. For example, elementwise operations produce outputs
  that have the same shape as their operands.</p>
        </sec>
        <sec id="cell-53a34bc0-nb-3" specific-use="notebook-content">
          <code language="python">A = torch.arange(6, dtype=torch.float32).reshape(2, 3)
B = A.clone()  # Assign a copy of A to B by allocating new memory
A, A + B</code>
          <boxed-text>
            <preformat>(tensor([[0., 1., 2.],
         [3., 4., 5.]]),
 tensor([[ 0.,  2.,  4.],
         [ 6.,  8., 10.]]))</preformat>
          </boxed-text>
        </sec>
        <sec id="eb7d4d8e-nb-3" specific-use="notebook-content">
          <p>The [<bold>elementwise product of two matrices is called their
  <italic>Hadamard product</italic></bold>] (denoted
  <inline-formula><alternatives><tex-math><![CDATA[\odot]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>‚äô</mml:mi></mml:math></alternatives></inline-formula>).
  We can spell out the entries of the Hadamard product of two matrices
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mo>,</mml:mo><mml:mi>ùêÅ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>:</p>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
  \mathbf{A} \odot \mathbf{B} =
  \begin{bmatrix}
      a_{11}  b_{11} & a_{12}  b_{12} & \dots  & a_{1n}  b_{1n} \\
      a_{21}  b_{21} & a_{22}  b_{22} & \dots  & a_{2n}  b_{2n} \\
      \vdots & \vdots & \ddots & \vdots \\
      a_{m1}  b_{m1} & a_{m2}  b_{m2} & \dots  & a_{mn}  b_{mn}
  \end{bmatrix}.
  ]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>ùêÄ</mml:mi>
                    <mml:mo>‚äô</mml:mo>
                    <mml:mi>ùêÅ</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>11</mml:mn>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mn>11</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>12</mml:mn>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mn>12</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚Ä¶</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mn>1</mml:mn>
                                <mml:mi>n</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mrow>
                                <mml:mn>1</mml:mn>
                                <mml:mi>n</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>21</mml:mn>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mn>21</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>22</mml:mn>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mn>22</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚Ä¶</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mn>2</mml:mn>
                                <mml:mi>n</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mrow>
                                <mml:mn>2</mml:mn>
                                <mml:mi>n</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mo>‚ã±</mml:mo>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mi>m</mml:mi>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mrow>
                                <mml:mi>m</mml:mi>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mi>m</mml:mi>
                                <mml:mn>2</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mrow>
                                <mml:mi>m</mml:mi>
                                <mml:mn>2</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚Ä¶</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mi>m</mml:mi>
                                <mml:mi>n</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mrow>
                                <mml:mi>m</mml:mi>
                                <mml:mi>n</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mi>.</mml:mi>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
        </sec>
        <sec id="cell-1e2c0645-nb-3" specific-use="notebook-content">
          <code language="python">A * B</code>
          <boxed-text>
            <preformat>tensor([[ 0.,  1.,  4.],
        [ 9., 16., 25.]])</preformat>
          </boxed-text>
        </sec>
        <sec id="e782293c-nb-3" specific-use="notebook-content">
          <p>[<bold>Adding or multiplying a scalar and a tensor</bold>] produces
  a result with the same shape as the original tensor. Here, each
  element of the tensor is added to (or multiplied by) the scalar.</p>
        </sec>
        <sec id="c5c86fb1-nb-3" specific-use="notebook-content">
          <code language="python">a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape</code>
          <boxed-text>
            <preformat>(tensor([[[ 2,  3,  4,  5],
          [ 6,  7,  8,  9],
          [10, 11, 12, 13]],
 
         [[14, 15, 16, 17],
          [18, 19, 20, 21],
          [22, 23, 24, 25]]]),
 torch.Size([2, 3, 4]))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-03bc0c1b-nb-3" specific-use="notebook-content">
</sec>
        <sec id="reduction-nb-3">
          <title>Reduction</title>
          <p>:label:<monospace>subsec_lin-alg-reduction</monospace></p>
          <p>Often, we wish to calculate [<bold>the sum of a tensor‚Äôs
  elements.</bold>] To express the sum of the elements in a vector
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>
  of length <inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>,
  we write <inline-formula><alternatives><tex-math><![CDATA[\sum_{i=1}^n x_i]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msubsup><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.
  There is a simple function for it:</p>
        </sec>
        <sec id="cell-5a3c4cbd-nb-3" specific-use="notebook-content">
          <code language="python">x = torch.arange(3, dtype=torch.float32)
x, x.sum()</code>
          <boxed-text>
            <preformat>(tensor([0., 1., 2.]), tensor(3.))</preformat>
          </boxed-text>
        </sec>
        <sec id="c4e031e9-nb-3" specific-use="notebook-content">
          <p>To express [<bold>sums over the elements of tensors of arbitrary
  shape</bold>], we simply sum over all its axes. For example, the sum
  of the elements of an <inline-formula><alternatives><tex-math><![CDATA[m \times n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  matrix <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÄ</mml:mi></mml:math></alternatives></inline-formula>
  could be written <inline-formula><alternatives><tex-math><![CDATA[\sum_{i=1}^{m} \sum_{j=1}^{n} a_{ij}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msubsup><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:msubsup><mml:msubsup><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.</p>
        </sec>
        <sec id="cell-7594e574-nb-3" specific-use="notebook-content">
          <code language="python">A.shape, A.sum()</code>
          <boxed-text>
            <preformat>(torch.Size([2, 3]), tensor(15.))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-2d9a0995-nb-3" specific-use="notebook-content">
          <p>By default, invoking the sum function <italic>reduces</italic> a
  tensor along all of its axes, eventually producing a scalar. Our
  libraries also allow us to [<bold>specify the axes along which the
  tensor should be reduced.</bold>] To sum over all elements along the
  rows (axis 0), we specify <monospace>axis=0</monospace> in
  <monospace>sum</monospace>. Since the input matrix reduces along axis
  0 to generate the output vector, this axis is missing from the shape
  of the output.</p>
        </sec>
        <sec id="cell-14d191be-nb-3" specific-use="notebook-content">
          <code language="python">A.shape, A.sum(axis=0).shape</code>
          <boxed-text>
            <preformat>(torch.Size([2, 3]), torch.Size([3]))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-22f9f461-nb-3" specific-use="notebook-content">
          <p>Specifying <monospace>axis=1</monospace> will reduce the column
  dimension (axis 1) by summing up elements of all the columns.</p>
        </sec>
        <sec id="ee3c0559-nb-3" specific-use="notebook-content">
          <code language="python">A.shape, A.sum(axis=1).shape</code>
          <boxed-text>
            <preformat>(torch.Size([2, 3]), torch.Size([2]))</preformat>
          </boxed-text>
        </sec>
        <sec id="a40d7419-nb-3" specific-use="notebook-content">
          <p>Reducing a matrix along both rows and columns via summation is
  equivalent to summing up all the elements of the matrix.</p>
        </sec>
        <sec id="cell-25b99ea4-nb-3" specific-use="notebook-content">
          <code language="python">A.sum(axis=[0, 1]) == A.sum()  # Same as A.sum()</code>
          <boxed-text>
            <preformat>tensor(True)</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-46224eef-nb-3" specific-use="notebook-content">
          <p>[<bold>A related quantity is the <italic>mean</italic>, also called
  the <italic>average</italic>.</bold>] We calculate the mean by
  dividing the sum by the total number of elements. Because computing
  the mean is so common, it gets a dedicated library function that works
  analogously to <monospace>sum</monospace>.</p>
        </sec>
        <sec id="cell-9f41e037-nb-3" specific-use="notebook-content">
          <code language="python">A.mean(), A.sum() / A.numel()</code>
          <boxed-text>
            <preformat>(tensor(2.5000), tensor(2.5000))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-1c73c2c9-nb-3" specific-use="notebook-content">
          <p>Likewise, the function for calculating the mean can also reduce a
  tensor along specific axes.</p>
        </sec>
        <sec id="f268d8be-nb-3" specific-use="notebook-content">
          <code language="python">A.mean(axis=0), A.sum(axis=0) / A.shape[0]</code>
          <boxed-text>
            <preformat>(tensor([1.5000, 2.5000, 3.5000]), tensor([1.5000, 2.5000, 3.5000]))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-272a70c1-nb-3" specific-use="notebook-content">
</sec>
        <sec id="non-reduction-sum-nb-3">
          <title>Non-Reduction Sum</title>
          <p>:label:<monospace>subsec_lin-alg-non-reduction</monospace></p>
          <p>Sometimes it can be useful to [<bold>keep the number of axes
  unchanged</bold>] when invoking the function for calculating the sum
  or mean. This matters when we want to use the broadcast mechanism.</p>
        </sec>
        <sec id="cell-863c5aca-nb-3" specific-use="notebook-content">
          <code language="python">sum_A = A.sum(axis=1, keepdims=True)
sum_A, sum_A.shape</code>
          <boxed-text>
            <preformat>(tensor([[ 3.],
         [12.]]),
 torch.Size([2, 1]))</preformat>
          </boxed-text>
        </sec>
        <sec id="db7e1c95-nb-3" specific-use="notebook-content">
          <p>For instance, since <monospace>sum_A</monospace> keeps its two axes
  after summing each row, we can (<bold>divide <monospace>A</monospace>
  by <monospace>sum_A</monospace> with broadcasting</bold>) to create a
  matrix where each row sums up to <inline-formula><alternatives><tex-math><![CDATA[1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula>.</p>
        </sec>
        <sec id="cell-93d20f26-nb-3" specific-use="notebook-content">
          <code language="python">A / sum_A</code>
          <boxed-text>
            <preformat>tensor([[0.0000, 0.3333, 0.6667],
        [0.2500, 0.3333, 0.4167]])</preformat>
          </boxed-text>
        </sec>
        <sec id="d208f582-nb-3" specific-use="notebook-content">
          <p>If we want to calculate [<bold>the cumulative sum of elements of
  <monospace>A</monospace> along some axis</bold>], say
  <monospace>axis=0</monospace> (row by row), we can call the
  <monospace>cumsum</monospace> function. By design, this function does
  not reduce the input tensor along any axis.</p>
        </sec>
        <sec id="e2de0ae7-nb-3" specific-use="notebook-content">
          <code language="python">A.cumsum(axis=0)</code>
          <boxed-text>
            <preformat>tensor([[0., 1., 2.],
        [3., 5., 7.]])</preformat>
          </boxed-text>
        </sec>
        <sec id="a3f6a397-nb-3" specific-use="notebook-content">
</sec>
        <sec id="dot-products-nb-3">
          <title>Dot Products</title>
          <p>So far, we have only performed elementwise operations, sums, and
  averages. And if this was all we could do, linear algebra would not
  deserve its own section. Fortunately, this is where things get more
  interesting. One of the most fundamental operations is the dot
  product. Given two vectors <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}, \mathbf{y} \in \mathbb{R}^d]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùê±</mml:mi><mml:mo>,</mml:mo><mml:mi>ùê≤</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
  their <italic>dot product</italic> <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}^\top \mathbf{y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùê±</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùê≤</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  (also known as <italic>inner product</italic>,
  <inline-formula><alternatives><tex-math><![CDATA[\langle \mathbf{x}, \mathbf{y}  \rangle]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="false" form="prefix">‚ü®</mml:mo><mml:mi>ùê±</mml:mi><mml:mo>,</mml:mo><mml:mi>ùê≤</mml:mi><mml:mo stretchy="false" form="postfix">‚ü©</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>)
  is a sum over the products of the elements at the same position:
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}^\top \mathbf{y} = \sum_{i=1}^{d} x_i y_i]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùê±</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùê≤</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>d</mml:mi></mml:msubsup><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.</p>
          <p>[<strike>The <italic>dot product</italic> of two vectors is a sum
  over the products of the elements at the same position</strike>]</p>
        </sec>
        <sec id="cell-5575e11a-nb-3" specific-use="notebook-content">
          <code language="python">y = torch.ones(3, dtype = torch.float32)
x, y, torch.dot(x, y)</code>
          <boxed-text>
            <preformat>(tensor([0., 1., 2.]), tensor([1., 1., 1.]), tensor(3.))</preformat>
          </boxed-text>
        </sec>
        <sec id="ba9329e0-nb-3" specific-use="notebook-content">
          <p>Equivalently, (<bold>we can calculate the dot product of two
  vectors by performing an elementwise multiplication followed by a
  sum:</bold>)</p>
        </sec>
        <sec id="b5186254-nb-3" specific-use="notebook-content">
          <code language="python">torch.sum(x * y)</code>
          <boxed-text>
            <preformat>tensor(3.)</preformat>
          </boxed-text>
        </sec>
        <sec id="ebe8aff3-nb-3" specific-use="notebook-content">
          <p>Dot products are useful in a wide range of contexts. For example,
  given some set of values, denoted by a vector
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}  \in \mathbb{R}^n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùê±</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
  and a set of weights, denoted by <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w} \in \mathbb{R}^n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùê∞</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
  the weighted sum of the values in <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>
  according to the weights <inline-formula><alternatives><tex-math><![CDATA[\mathbf{w}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê∞</mml:mi></mml:math></alternatives></inline-formula>
  could be expressed as the dot product <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}^\top \mathbf{w}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùê±</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùê∞</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
  When the weights are nonnegative and sum to
  <inline-formula><alternatives><tex-math><![CDATA[1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula>,
  i.e., <inline-formula><alternatives><tex-math><![CDATA[\left(\sum_{i=1}^{n} {w_i} = 1\right)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msubsup><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>,
  the dot product expresses a <italic>weighted average</italic>. After
  normalizing two vectors to have unit length, the dot products express
  the cosine of the angle between them. Later in this section, we will
  formally introduce this notion of <italic>length</italic>.</p>
        </sec>
        <sec id="matrixvector-products-nb-3">
          <title>Matrix‚ÄìVector Products</title>
          <p>Now that we know how to calculate dot products, we can begin to
  understand the <italic>product</italic> between an
  <inline-formula><alternatives><tex-math><![CDATA[m \times n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  matrix <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÄ</mml:mi></mml:math></alternatives></inline-formula>
  and an <inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>-dimensional
  vector <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>.
  To start off, we visualize our matrix in terms of its row vectors</p>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[\mathbf{A}=
  \begin{bmatrix}
  \mathbf{a}^\top_{1} \\
  \mathbf{a}^\top_{2} \\
  \vdots \\
  \mathbf{a}^\top_m \\
  \end{bmatrix},]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>ùêÄ</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>1</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>2</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mi>m</mml:mi>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center"/>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mo>,</mml:mo>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <p>where each <inline-formula><alternatives><tex-math><![CDATA[\mathbf{a}^\top_{i} \in \mathbb{R}^n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msubsup><mml:mi>ùêö</mml:mi><mml:mi>i</mml:mi><mml:mi>‚ä§</mml:mi></mml:msubsup><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  is a row vector representing the <inline-formula><alternatives><tex-math><![CDATA[i^\textrm{th}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>i</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
  row of the matrix <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÄ</mml:mi></mml:math></alternatives></inline-formula>.</p>
          <p>[<bold>The matrix‚Äìvector product <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mi>ùê±</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  is simply a column vector of length <inline-formula><alternatives><tex-math><![CDATA[m]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>m</mml:mi></mml:math></alternatives></inline-formula>,
  whose <inline-formula><alternatives><tex-math><![CDATA[i^\textrm{th}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>i</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
  element is the dot product <inline-formula><alternatives><tex-math><![CDATA[\mathbf{a}^\top_i \mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msubsup><mml:mi>ùêö</mml:mi><mml:mi>i</mml:mi><mml:mi>‚ä§</mml:mi></mml:msubsup><mml:mi>ùê±</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>:</bold>]</p>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
  \mathbf{A}\mathbf{x}
  = \begin{bmatrix}
  \mathbf{a}^\top_{1} \\
  \mathbf{a}^\top_{2} \\
  \vdots \\
  \mathbf{a}^\top_m \\
  \end{bmatrix}\mathbf{x}
  = \begin{bmatrix}
   \mathbf{a}^\top_{1} \mathbf{x}  \\
   \mathbf{a}^\top_{2} \mathbf{x} \\
  \vdots\\
   \mathbf{a}^\top_{m} \mathbf{x}\\
  \end{bmatrix}.
  ]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>ùêÄ</mml:mi>
                    <mml:mi>ùê±</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>1</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>2</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mi>m</mml:mi>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center"/>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mi>ùê±</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>1</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:mi>ùê±</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>2</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:mi>ùê±</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mi>m</mml:mi>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:mi>ùê±</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center"/>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mi>.</mml:mi>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <p>We can think of multiplication with a matrix
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}\in \mathbb{R}^{m \times n}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  as a transformation that projects vectors from
  <inline-formula><alternatives><tex-math><![CDATA[\mathbb{R}^{n}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:math></alternatives></inline-formula>
  to <inline-formula><alternatives><tex-math><![CDATA[\mathbb{R}^{m}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:math></alternatives></inline-formula>.
  These transformations are remarkably useful. For example, we can
  represent rotations as multiplications by certain square matrices.
  Matrix‚Äìvector products also describe the key calculation involved in
  computing the outputs of each layer in a neural network given the
  outputs from the previous layer.</p>
        </sec>
        <sec id="b3769117-nb-3" specific-use="notebook-content">
          <p>To express a matrix‚Äìvector product in code, we use the
  <monospace>mv</monospace> function. Note that the column dimension of
  <monospace>A</monospace> (its length along axis 1) must be the same as
  the dimension of <monospace>x</monospace> (its length). Python has a
  convenience operator <monospace>@</monospace> that can execute both
  matrix‚Äìvector and matrix‚Äìmatrix products (depending on its arguments).
  Thus we can write <monospace>A@x</monospace>.</p>
        </sec>
        <sec id="cell-1a99c29a-nb-3" specific-use="notebook-content">
          <code language="python">A.shape, x.shape, torch.mv(A, x), A@x</code>
          <boxed-text>
            <preformat>(torch.Size([2, 3]), torch.Size([3]), tensor([ 5., 14.]), tensor([ 5., 14.]))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-00e00436-nb-3" specific-use="notebook-content">
</sec>
        <sec id="matrixmatrix-multiplication-nb-3">
          <title>Matrix‚ÄìMatrix Multiplication</title>
          <p>Once you have gotten the hang of dot products and matrix‚Äìvector
  products, then <italic>matrix‚Äìmatrix multiplication</italic> should be
  straightforward.</p>
          <p>Say that we have two matrices <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A} \in \mathbb{R}^{n \times k}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>√ó</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives><tex-math><![CDATA[\mathbf{B} \in \mathbb{R}^{k \times m}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÅ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>√ó</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>:</p>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[\mathbf{A}=\begin{bmatrix}
   a_{11} & a_{12} & \cdots & a_{1k} \\
   a_{21} & a_{22} & \cdots & a_{2k} \\
  \vdots & \vdots & \ddots & \vdots \\
   a_{n1} & a_{n2} & \cdots & a_{nk} \\
  \end{bmatrix},\quad
  \mathbf{B}=\begin{bmatrix}
   b_{11} & b_{12} & \cdots & b_{1m} \\
   b_{21} & b_{22} & \cdots & b_{2m} \\
  \vdots & \vdots & \ddots & \vdots \\
   b_{k1} & b_{k2} & \cdots & b_{km} \\
  \end{bmatrix}.]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>ùêÄ</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>11</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>12</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãØ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mn>1</mml:mn>
                                <mml:mi>k</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>21</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mn>22</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãØ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mn>2</mml:mn>
                                <mml:mi>k</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mo>‚ã±</mml:mo>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mi>n</mml:mi>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mi>n</mml:mi>
                                <mml:mn>2</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãØ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>a</mml:mi>
                              <mml:mrow>
                                <mml:mi>n</mml:mi>
                                <mml:mi>k</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center"/>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mo>,</mml:mo>
                    <mml:mspace width="1.0em"/>
                    <mml:mi>ùêÅ</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mn>11</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mn>12</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãØ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mrow>
                                <mml:mn>1</mml:mn>
                                <mml:mi>m</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mn>21</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mn>22</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãØ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mrow>
                                <mml:mn>2</mml:mn>
                                <mml:mi>m</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mo>‚ã±</mml:mo>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mrow>
                                <mml:mi>k</mml:mi>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mrow>
                                <mml:mi>k</mml:mi>
                                <mml:mn>2</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãØ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>b</mml:mi>
                              <mml:mrow>
                                <mml:mi>k</mml:mi>
                                <mml:mi>m</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center"/>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mi>.</mml:mi>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <p>Let <inline-formula><alternatives><tex-math><![CDATA[\mathbf{a}^\top_{i} \in \mathbb{R}^k]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msubsup><mml:mi>ùêö</mml:mi><mml:mi>i</mml:mi><mml:mi>‚ä§</mml:mi></mml:msubsup><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  denote the row vector representing the <inline-formula><alternatives><tex-math><![CDATA[i^\textrm{th}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>i</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
  row of the matrix <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÄ</mml:mi></mml:math></alternatives></inline-formula>
  and let <inline-formula><alternatives><tex-math><![CDATA[\mathbf{b}_{j} \in \mathbb{R}^k]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>ùêõ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  denote the column vector from the <inline-formula><alternatives><tex-math><![CDATA[j^\textrm{th}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>j</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
  column of the matrix <inline-formula><alternatives><tex-math><![CDATA[\mathbf{B}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÅ</mml:mi></mml:math></alternatives></inline-formula>:</p>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[\mathbf{A}=
  \begin{bmatrix}
  \mathbf{a}^\top_{1} \\
  \mathbf{a}^\top_{2} \\
  \vdots \\
  \mathbf{a}^\top_n \\
  \end{bmatrix},
  \quad \mathbf{B}=\begin{bmatrix}
   \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
  \end{bmatrix}.
  ]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>ùêÄ</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>1</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>2</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mi>n</mml:mi>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center"/>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mo>,</mml:mo>
                    <mml:mspace width="1.0em"/>
                    <mml:mi>ùêÅ</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mn>1</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mn>2</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãØ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mi>m</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center"/>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mi>.</mml:mi>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <p>To form the matrix product <inline-formula><alternatives><tex-math><![CDATA[\mathbf{C} \in \mathbb{R}^{n \times m}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÇ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>√ó</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
  we simply compute each element <inline-formula><alternatives><tex-math><![CDATA[c_{ij}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
  as the dot product between the <inline-formula><alternatives><tex-math><![CDATA[i^{\textrm{th}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>i</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
  row of <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÄ</mml:mi></mml:math></alternatives></inline-formula>
  and the <inline-formula><alternatives><tex-math><![CDATA[j^{\textrm{th}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>j</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
  column of <inline-formula><alternatives><tex-math><![CDATA[\mathbf{B}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÅ</mml:mi></mml:math></alternatives></inline-formula>,
  i.e., <inline-formula><alternatives><tex-math><![CDATA[\mathbf{a}^\top_i \mathbf{b}_j]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msubsup><mml:mi>ùêö</mml:mi><mml:mi>i</mml:mi><mml:mi>‚ä§</mml:mi></mml:msubsup><mml:msub><mml:mi>ùêõ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>:</p>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[\mathbf{C} = \mathbf{AB} = \begin{bmatrix}
  \mathbf{a}^\top_{1} \\
  \mathbf{a}^\top_{2} \\
  \vdots \\
  \mathbf{a}^\top_n \\
  \end{bmatrix}
  \begin{bmatrix}
   \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m} \\
  \end{bmatrix}
  = \begin{bmatrix}
  \mathbf{a}^\top_{1} \mathbf{b}_1 & \mathbf{a}^\top_{1}\mathbf{b}_2& \cdots & \mathbf{a}^\top_{1} \mathbf{b}_m \\
   \mathbf{a}^\top_{2}\mathbf{b}_1 & \mathbf{a}^\top_{2} \mathbf{b}_2 & \cdots & \mathbf{a}^\top_{2} \mathbf{b}_m \\
   \vdots & \vdots & \ddots &\vdots\\
  \mathbf{a}^\top_{n} \mathbf{b}_1 & \mathbf{a}^\top_{n}\mathbf{b}_2& \cdots& \mathbf{a}^\top_{n} \mathbf{b}_m
  \end{bmatrix}.
  ]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mi>ùêÇ</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mi>ùêÄ</mml:mi>
                      <mml:mi>ùêÅ</mml:mi>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>1</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>2</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mi>n</mml:mi>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center"/>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mn>1</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mn>2</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãØ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mi>m</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center"/>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">[</mml:mo>
                      <mml:mtable>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>1</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mn>1</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>1</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mn>2</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãØ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>1</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mi>m</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>2</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mn>1</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>2</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mn>2</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãØ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mn>2</mml:mn>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mi>m</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mo>‚ã±</mml:mo>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãÆ</mml:mi>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mi>n</mml:mi>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mn>1</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mi>n</mml:mi>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mn>2</mml:mn>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:mi>‚ãØ</mml:mi>
                          </mml:mtd>
                          <mml:mtd columnalign="center" style="text-align: center">
                            <mml:msubsup>
                              <mml:mi>ùêö</mml:mi>
                              <mml:mi>n</mml:mi>
                              <mml:mi>‚ä§</mml:mi>
                            </mml:msubsup>
                            <mml:msub>
                              <mml:mi>ùêõ</mml:mi>
                              <mml:mi>m</mml:mi>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                      </mml:mtable>
                      <mml:mo stretchy="true" form="postfix">]</mml:mo>
                    </mml:mrow>
                    <mml:mi>.</mml:mi>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <p>[<bold>We can think of the matrix‚Äìmatrix multiplication
  <inline-formula><alternatives><tex-math><![CDATA[\mathbf{AB}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mi>ùêÅ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  as performing <inline-formula><alternatives><tex-math><![CDATA[m]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>m</mml:mi></mml:math></alternatives></inline-formula>
  matrix‚Äìvector products or <inline-formula><alternatives><tex-math><![CDATA[m \times n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  dot products and stitching the results together to form an
  <inline-formula><alternatives><tex-math><![CDATA[n \times m]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>n</mml:mi><mml:mo>√ó</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  matrix.</bold>] In the following snippet, we perform matrix
  multiplication on <monospace>A</monospace> and
  <monospace>B</monospace>. Here,¬†<monospace>A</monospace> is a matrix
  with two rows and three columns, and <monospace>B</monospace> is a
  matrix with three rows and four columns. After multiplication, we
  obtain a matrix with two rows and four columns.</p>
        </sec>
        <sec id="cell-99535047-nb-3" specific-use="notebook-content">
          <code language="python">B = torch.ones(3, 4)
torch.mm(A, B), A@B</code>
          <boxed-text>
            <preformat>(tensor([[ 3.,  3.,  3.,  3.],
         [12., 12., 12., 12.]]),
 tensor([[ 3.,  3.,  3.,  3.],
         [12., 12., 12., 12.]]))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-28a86db3-nb-3" specific-use="notebook-content">
          <p>The term <italic>matrix‚Äìmatrix multiplication</italic> is often
  simplified to <italic>matrix multiplication</italic>, and should not
  be confused with the Hadamard product.</p>
        </sec>
        <sec id="norms-nb-3">
          <title>Norms</title>
          <p>Some of the most useful operators in linear algebra are
  <italic>norms</italic>. Informally, the norm of a vector tells us how
  <italic>big</italic> it is. For instance, the
  <inline-formula><alternatives><tex-math><![CDATA[\ell_2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  norm measures the (Euclidean) length of a vector. Here, we are
  employing a notion of <italic>size</italic> that concerns the
  magnitude of a vector‚Äôs components (not its dimensionality).</p>
          <p>A norm is a function <inline-formula><alternatives><tex-math><![CDATA[\| \cdot \|]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>‚ãÖ</mml:mi><mml:mo stretchy="false" form="postfix">‚à•</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  that maps a vector to a scalar and satisfies the following three
  properties:</p>
          <list list-type="order">
            <list-item>
              <p>Given any vector <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>,
      if we scale (all elements of) the vector by a scalar
      <inline-formula><alternatives><tex-math><![CDATA[\alpha \in \mathbb{R}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>Œ±</mml:mi><mml:mo>‚àà</mml:mo><mml:mi>‚Ñù</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
      its norm scales accordingly: <disp-formula><alternatives><tex-math><![CDATA[\|\alpha \mathbf{x}\| = |\alpha| \|\mathbf{x}\|.]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>Œ±</mml:mi><mml:mi>ùê±</mml:mi><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>Œ±</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>ùê±</mml:mi><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
            </list-item>
            <list-item>
              <p>For any vectors <inline-formula><alternatives><tex-math><![CDATA[\mathbf{x}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives><tex-math><![CDATA[\mathbf{y}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùê≤</mml:mi></mml:math></alternatives></inline-formula>:
      norms satisfy the triangle inequality:
      <disp-formula><alternatives><tex-math><![CDATA[\|\mathbf{x} + \mathbf{y}\| \leq \|\mathbf{x}\| + \|\mathbf{y}\|.]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>ùê±</mml:mi><mml:mo>+</mml:mo><mml:mi>ùê≤</mml:mi><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mo>‚â§</mml:mo><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>ùê±</mml:mi><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>+</mml:mi><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>ùê≤</mml:mi><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
            </list-item>
            <list-item>
              <p>The norm of a vector is nonnegative and it only vanishes if the
      vector is zero: <disp-formula><alternatives><tex-math><![CDATA[\|\mathbf{x}\| > 0 \textrm{ for all } \mathbf{x} \neq 0.]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>ùê±</mml:mi><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mspace width="0.333em"/><mml:mtext mathvariant="normal"> for all </mml:mtext><mml:mspace width="0.333em"/></mml:mrow><mml:mi>ùê±</mml:mi><mml:mo>‚â†</mml:mo><mml:mn>0</mml:mn><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
            </list-item>
          </list>
          <p>Many functions are valid norms and different norms encode different
  notions of size. The Euclidean norm that we all learned in elementary
  school geometry when calculating the hypotenuse of a right triangle is
  the square root of the sum of squares of a vector‚Äôs elements.
  Formally, this is called [<bold>the <inline-formula><alternatives><tex-math><![CDATA[\ell_2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  <italic>norm</italic></bold>] and expressed as</p>
          <p>(<bold><disp-formula><alternatives><tex-math><![CDATA[\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}.]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>ùê±</mml:mi><mml:msub><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></bold>)</p>
          <p>The method <monospace>norm</monospace> calculates the
  <inline-formula><alternatives><tex-math><![CDATA[\ell_2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  norm.</p>
        </sec>
        <sec id="e215c544-nb-3" specific-use="notebook-content">
          <code language="python">u = torch.tensor([3.0, -4.0])
torch.norm(u)</code>
          <boxed-text>
            <preformat>tensor(5.)</preformat>
          </boxed-text>
        </sec>
        <sec id="d80835a0-nb-3" specific-use="notebook-content">
          <p>[<bold>The <inline-formula><alternatives><tex-math><![CDATA[\ell_1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  norm</bold>] is also common and the associated measure is called the
  Manhattan distance. By definition, the <inline-formula><alternatives><tex-math><![CDATA[\ell_1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  norm sums the absolute values of a vector‚Äôs elements:</p>
          <p>(<bold><disp-formula><alternatives><tex-math><![CDATA[\|\mathbf{x}\|_1 = \sum_{i=1}^n \left|x_i \right|.]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>ùê±</mml:mi><mml:msub><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></bold>)</p>
          <p>Compared to the <inline-formula><alternatives><tex-math><![CDATA[\ell_2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  norm, it is less sensitive to outliers. To compute the
  <inline-formula><alternatives><tex-math><![CDATA[\ell_1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  norm, we compose the absolute value with the sum operation.</p>
        </sec>
        <sec id="cell-8a3e0562-nb-3" specific-use="notebook-content">
          <code language="python">torch.abs(u).sum()</code>
          <boxed-text>
            <preformat>tensor(7.)</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-727b4843-nb-3" specific-use="notebook-content">
          <p>Both the <inline-formula><alternatives><tex-math><![CDATA[\ell_2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives><tex-math><![CDATA[\ell_1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  norms are special cases of the more general
  <inline-formula><alternatives><tex-math><![CDATA[\ell_p]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mi>p</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  <italic>norms</italic>:</p>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[\|\mathbf{x}\|_p = \left(\sum_{i=1}^n \left|x_i \right|^p \right)^{1/p}.]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mo stretchy="false" form="postfix">‚à•</mml:mo>
                    <mml:mi>ùê±</mml:mi>
                    <mml:msub>
                      <mml:mo stretchy="false" form="postfix">‚à•</mml:mo>
                      <mml:mi>p</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:munderover>
                          <mml:mo>‚àë</mml:mo>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                            <mml:mo>=</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                          <mml:mi>n</mml:mi>
                        </mml:munderover>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">|</mml:mo>
                            <mml:msub>
                              <mml:mi>x</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mo stretchy="true" form="postfix">|</mml:mo>
                          </mml:mrow>
                          <mml:mi>p</mml:mi>
                        </mml:msup>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mn>1</mml:mn>
                        <mml:mi>/</mml:mi>
                        <mml:mi>p</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mi>.</mml:mi>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <p>In the case of matrices, matters are more complicated. After all,
  matrices can be viewed both as collections of individual entries
  <italic>and</italic> as objects that operate on vectors and transform
  them into other vectors. For instance, we can ask by how much longer
  the matrix‚Äìvector product <inline-formula><alternatives><tex-math><![CDATA[\mathbf{X} \mathbf{v}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêó</mml:mi><mml:mi>ùêØ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  could be relative to <inline-formula><alternatives><tex-math><![CDATA[\mathbf{v}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêØ</mml:mi></mml:math></alternatives></inline-formula>.
  This line of thought leads to what is called the
  <italic>spectral</italic> norm. For now, we introduce [<bold>the
  <italic>Frobenius norm</italic>, which is much easier to
  compute</bold>] and defined as the square root of the sum of the
  squares of a matrix‚Äôs elements:</p>
          <p>[<bold><disp-formula><alternatives><tex-math><![CDATA[\|\mathbf{X}\|_\textrm{F} = \sqrt{\sum_{i=1}^m \sum_{j=1}^n x_{ij}^2}.]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block"><mml:mrow><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mi>ùêó</mml:mi><mml:msub><mml:mo stretchy="false" form="postfix">‚à•</mml:mo><mml:mtext mathvariant="normal">F</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></bold>]</p>
          <p>The Frobenius norm behaves as if it were an
  <inline-formula><alternatives><tex-math><![CDATA[\ell_2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  norm of a matrix-shaped vector. Invoking the following function will
  calculate the Frobenius norm of a matrix.</p>
        </sec>
        <sec id="cell-3e00a124-nb-3" specific-use="notebook-content">
          <code language="python">torch.norm(torch.ones((4, 9)))</code>
          <boxed-text>
            <preformat>tensor(6.)</preformat>
          </boxed-text>
        </sec>
        <sec id="b8478a55-nb-3" specific-use="notebook-content">
          <p>While we do not want to get too far ahead of ourselves, we already
  can plant some intuition about why these concepts are useful. In deep
  learning, we are often trying to solve optimization problems:
  <italic>maximize</italic> the probability assigned to observed data;
  <italic>maximize</italic> the revenue associated with a recommender
  model; <italic>minimize</italic> the distance between predictions and
  the ground truth observations; <italic>minimize</italic> the distance
  between representations of photos of the same person while
  <italic>maximizing</italic> the distance between representations of
  photos of different people. These distances, which constitute the
  objectives of deep learning algorithms, are often expressed as
  norms.</p>
        </sec>
        <sec id="discussion-nb-3">
          <title>Discussion</title>
          <p>In this section, we have reviewed all the linear algebra that you
  will need to understand a significant chunk of modern deep learning.
  There is a lot more to linear algebra, though, and much of it is
  useful for machine learning. For example, matrices can be decomposed
  into factors, and these decompositions can reveal low-dimensional
  structure in real-world datasets. There are entire subfields of
  machine learning that focus on using matrix decompositions and their
  generalizations to high-order tensors to discover structure in
  datasets and solve prediction problems. But this book focuses on deep
  learning. And we believe you will be more inclined to learn more
  mathematics once you have gotten your hands dirty applying machine
  learning to real datasets. So while we reserve the right to introduce
  more mathematics later on, we wrap up this section here.</p>
          <p>If you are eager to learn more linear algebra, there are many
  excellent books and online resources. For a more advanced crash
  course, consider checking out
  :citet:<monospace>Strang.1993</monospace>,
  :citet:<monospace>Kolter.2008</monospace>, and
  :citet:<monospace>Petersen.Pedersen.ea.2008</monospace>.</p>
          <p>To recap:</p>
          <list list-type="bullet">
            <list-item>
              <p>Scalars, vectors, matrices, and tensors are the basic
      mathematical objects used in linear algebra and have zero, one,
      two, and an arbitrary number of axes, respectively.</p>
            </list-item>
            <list-item>
              <p>Tensors can be sliced or reduced along specified axes via
      indexing, or operations such as <monospace>sum</monospace> and
      <monospace>mean</monospace>, respectively.</p>
            </list-item>
            <list-item>
              <p>Elementwise products are called Hadamard products. By contrast,
      dot products, matrix‚Äìvector products, and matrix‚Äìmatrix products
      are not elementwise operations and in general return objects
      having shapes that are different from the the operands.</p>
            </list-item>
            <list-item>
              <p>Compared to Hadamard products, matrix‚Äìmatrix products take
      considerably longer to compute (cubic rather than quadratic
      time).</p>
            </list-item>
            <list-item>
              <p>Norms capture various notions of the magnitude of a vector (or
      matrix), and are commonly applied to the difference of two vectors
      to measure their distance apart.</p>
            </list-item>
            <list-item>
              <p>Common vector norms include the <inline-formula><alternatives><tex-math><![CDATA[\ell_1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives><tex-math><![CDATA[\ell_2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
      norms, and common matrix norms include the
      <italic>spectral</italic> and <italic>Frobenius</italic>
      norms.</p>
            </list-item>
          </list>
        </sec>
        <sec id="exercises-nb-3">
          <title>Exercises</title>
          <list list-type="order">
            <list-item>
              <p>Prove that the transpose of the transpose of a matrix is the
      matrix itself: <inline-formula><alternatives><tex-math><![CDATA[(\mathbf{A}^\top)^\top = \mathbf{A}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>ùêÄ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>‚ä§</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>ùêÄ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</p>
            </list-item>
            <list-item>
              <p>Given two matrices <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÄ</mml:mi></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives><tex-math><![CDATA[\mathbf{B}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÅ</mml:mi></mml:math></alternatives></inline-formula>,
      show that sum and transposition commute:
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}^\top + \mathbf{B}^\top = (\mathbf{A} + \mathbf{B})^\top]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msup><mml:mi>ùêÄ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>ùêÅ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùêÄ</mml:mi><mml:mo>+</mml:mo><mml:mi>ùêÅ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>‚ä§</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
            </list-item>
            <list-item>
              <p>Given any square matrix <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÄ</mml:mi></mml:math></alternatives></inline-formula>,
      is <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A} + \mathbf{A}^\top]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>ùêÄ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      always symmetric? Can you prove the result by using only the
      results of the previous two exercises?</p>
            </list-item>
            <list-item>
              <p>We defined the tensor <monospace>X</monospace> of shape (2, 3,
      4) in this section. What is the output of
      <monospace>len(X)</monospace>? Write your answer without
      implementing any code, then check your answer using code.</p>
            </list-item>
            <list-item>
              <p>For a tensor <monospace>X</monospace> of arbitrary shape, does
      <monospace>len(X)</monospace> always correspond to the length of a
      certain axis of <monospace>X</monospace>? What is that axis?</p>
            </list-item>
            <list-item>
              <p>Run <monospace>A / A.sum(axis=1)</monospace> and see what
      happens. Can you analyze the results?</p>
            </list-item>
            <list-item>
              <p>When traveling between two points in downtown Manhattan, what
      is the distance that you need to cover in terms of the
      coordinates, i.e., in terms of avenues and streets? Can you travel
      diagonally?</p>
            </list-item>
            <list-item>
              <p>Consider a tensor of shape (2, 3, 4). What are the shapes of
      the summation outputs along axes 0, 1, and 2?</p>
            </list-item>
            <list-item>
              <p>Feed a tensor with three or more axes to the
      <monospace>linalg.norm</monospace> function and observe its
      output. What does this function compute for tensors of arbitrary
      shape?</p>
            </list-item>
            <list-item>
              <p>Consider three large matrices, say
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A} \in \mathbb{R}^{2^{10} \times 2^{16}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mn>10</mml:mn></mml:msup><mml:mo>√ó</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mn>16</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{B} \in \mathbb{R}^{2^{16} \times 2^{5}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÅ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mn>16</mml:mn></mml:msup><mml:mo>√ó</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives><tex-math><![CDATA[\mathbf{C} \in \mathbb{R}^{2^{5} \times 2^{14}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÇ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mn>5</mml:mn></mml:msup><mml:mo>√ó</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mn>14</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
      initialized with Gaussian random variables. You want to compute
      the product <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A} \mathbf{B} \mathbf{C}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mi>ùêÅ</mml:mi><mml:mi>ùêÇ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
      Is there any difference in memory footprint and speed, depending
      on whether you compute <inline-formula><alternatives><tex-math><![CDATA[(\mathbf{A} \mathbf{B}) \mathbf{C}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùêÄ</mml:mi><mml:mi>ùêÅ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>ùêÇ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      or <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A} (\mathbf{B} \mathbf{C})]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùêÅ</mml:mi><mml:mi>ùêÇ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
      Why?</p>
            </list-item>
            <list-item>
              <p>Consider three large matrices, say
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A} \in \mathbb{R}^{2^{10} \times 2^{16}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mn>10</mml:mn></mml:msup><mml:mo>√ó</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mn>16</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{B} \in \mathbb{R}^{2^{16} \times 2^{5}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÅ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mn>16</mml:mn></mml:msup><mml:mo>√ó</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives><tex-math><![CDATA[\mathbf{C} \in \mathbb{R}^{2^{5} \times 2^{16}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÇ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mn>5</mml:mn></mml:msup><mml:mo>√ó</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mn>16</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.
      Is there any difference in speed depending on whether you compute
      <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A} \mathbf{B}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mi>ùêÅ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      or <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A} \mathbf{C}^\top]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:msup><mml:mi>ùêÇ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>?
      Why? What changes if you initialize <inline-formula><alternatives><tex-math><![CDATA[\mathbf{C} = \mathbf{B}^\top]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÇ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>ùêÅ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      without cloning memory? Why?</p>
            </list-item>
            <list-item>
              <p>Consider three matrices, say <inline-formula><alternatives><tex-math><![CDATA[\mathbf{A}, \mathbf{B}, \mathbf{C} \in \mathbb{R}^{100 \times 200}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêÄ</mml:mi><mml:mo>,</mml:mo><mml:mi>ùêÅ</mml:mi><mml:mo>,</mml:mo><mml:mi>ùêÇ</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mn>100</mml:mn><mml:mo>√ó</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.
      Construct a tensor with three axes by stacking
      <inline-formula><alternatives><tex-math><![CDATA[[\mathbf{A}, \mathbf{B}, \mathbf{C}]]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>ùêÄ</mml:mi><mml:mo>,</mml:mo><mml:mi>ùêÅ</mml:mi><mml:mo>,</mml:mo><mml:mi>ùêÇ</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.
      What is the dimensionality? Slice out the second coordinate of the
      third axis to recover <inline-formula><alternatives><tex-math><![CDATA[\mathbf{B}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÅ</mml:mi></mml:math></alternatives></inline-formula>.
      Check that your answer is correct.</p>
            </list-item>
          </list>
        </sec>
      </sec>
    </body>
    <back>
</back>
  </sub-article>
  <sub-article article-type="notebook" id="nb-8-nb-4">
    <front-stub>
      <title-group>
        <article-title>01.1 Data Manipulation</article-title>
      </title-group>
    </front-stub>
    <body>
      <sec id="cell-0de623d7-nb-4" specific-use="notebook-content">
        <p>In order to get anything done, we need some way to store and
manipulate data. Generally, there are two important things we need to do
with data: (i) acquire them; and (ii) process them once they are inside
the computer. There is no point in acquiring data without some way to
store it, so to start, let‚Äôs get our hands dirty with
<inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>-dimensional
arrays, which we also call <italic>tensors</italic>. If you already know
the NumPy scientific computing package, this will be a breeze. For all
modern deep learning frameworks, the <italic>tensor class</italic>
(<monospace>ndarray</monospace> in MXNet, <monospace>Tensor</monospace>
in PyTorch and TensorFlow) resembles NumPy‚Äôs
<monospace>ndarray</monospace>, with a few killer features added. First,
the tensor class supports automatic differentiation. Second, it
leverages GPUs to accelerate numerical computation, whereas NumPy only
runs on CPUs. These properties make neural networks both easy to code
and fast to run.</p>
        <sec id="getting-started-nb-4">
          <title>Getting Started</title>
        </sec>
        <sec id="cell-084dc517-nb-4" specific-use="notebook-content">
          <p>(<bold>To start, we import the PyTorch library. Note that the
  package name is <monospace>torch</monospace>.</bold>)</p>
        </sec>
        <sec id="cell-01fa8e58-nb-4" specific-use="notebook-content">
          <code language="python">import torch</code>
        </sec>
        <sec id="cell-8d828de8-nb-4" specific-use="notebook-content">
          <p>[<bold>A tensor represents a (possibly multidimensional) array of
  numerical values.</bold>] In the one-dimensional case, i.e., when only
  one axis is needed for the data, a tensor is called a
  <italic>vector</italic>. With two axes, a tensor is called a
  <italic>matrix</italic>. With <inline-formula><alternatives><tex-math><![CDATA[k > 2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>k</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  axes, we drop the specialized names and just refer to the object as a
  <inline-formula><alternatives><tex-math><![CDATA[k^\textrm{th}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>k</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>-<italic>order
  tensor</italic>.</p>
        </sec>
        <sec id="cell-1a471639-nb-4" specific-use="notebook-content">
          <p>PyTorch provides a variety of functions for creating new tensors
  prepopulated with values. For example, by invoking
  <monospace>arange(n)</monospace>, we can create a vector of evenly
  spaced values, starting at 0 (included) and ending at
  <monospace>n</monospace> (not included). By default, the interval size
  is <inline-formula><alternatives><tex-math><![CDATA[1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula>.
  Unless otherwise specified, new tensors are stored in main memory and
  designated for CPU-based computation.</p>
        </sec>
        <sec id="b6aa30a9-nb-4" specific-use="notebook-content">
          <code language="python">x = torch.arange(12, dtype=torch.float32)
x</code>
          <boxed-text>
            <preformat>tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-1a12b5d8-nb-4" specific-use="notebook-content">
          <p>Each of these values is called an <italic>element</italic> of the
  tensor. The tensor <monospace>x</monospace> contains 12 elements. We
  can inspect the total number of elements in a tensor via its
  <monospace>numel</monospace> method.</p>
        </sec>
        <sec id="cell-640cadaf-nb-4" specific-use="notebook-content">
          <code language="python">x.numel()</code>
          <boxed-text>
            <preformat>12</preformat>
          </boxed-text>
        </sec>
        <sec id="d50c7483-nb-4" specific-use="notebook-content">
          <p>(<bold>We can access a tensor‚Äôs <italic>shape</italic></bold>) (the
  length along each axis) by inspecting its <monospace>shape</monospace>
  attribute. Because we are dealing with a vector here, the
  <monospace>shape</monospace> contains just a single element and is
  identical to the size.</p>
        </sec>
        <sec id="cell-6e0a9616-nb-4" specific-use="notebook-content">
          <code language="python">x.shape</code>
          <boxed-text>
            <preformat>torch.Size([12])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-5c60413a-nb-4" specific-use="notebook-content">
          <p>We can [<bold>change the shape of a tensor without altering its
  size or values</bold>], by invoking <monospace>reshape</monospace>.
  For example, we can transform our vector <monospace>x</monospace>
  whose shape is (12,) to a matrix <monospace>X</monospace> with shape
  (3, 4). This new tensor retains all elements but reconfigures them
  into a matrix. Notice that the elements of our vector are laid out one
  row at a time and thus <monospace>x[3] == X[0, 3]</monospace>.</p>
        </sec>
        <sec id="cell-6092207c-nb-4" specific-use="notebook-content">
          <code language="python">X = x.reshape(3, 4)
X</code>
          <boxed-text>
            <preformat>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.]])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-2d2e1706-nb-4" specific-use="notebook-content">
          <p>Note that specifying every shape component to
  <monospace>reshape</monospace> is redundant. Because we already know
  our tensor‚Äôs size, we can work out one component of the shape given
  the rest. For example, given a tensor of size
  <inline-formula><alternatives><tex-math><![CDATA[n]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
  and target shape (<inline-formula><alternatives><tex-math><![CDATA[h]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>h</mml:mi></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives><tex-math><![CDATA[w]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>w</mml:mi></mml:math></alternatives></inline-formula>),
  we know that <inline-formula><alternatives><tex-math><![CDATA[w = n/h]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi><mml:mi>/</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
  To automatically infer one component of the shape, we can place a
  <monospace>-1</monospace> for the shape component that should be
  inferred automatically. In our case, instead of calling
  <monospace>x.reshape(3, 4)</monospace>, we could have equivalently
  called <monospace>x.reshape(-1, 4)</monospace> or
  <monospace>x.reshape(3, -1)</monospace>.</p>
          <p>Practitioners often need to work with tensors initialized to
  contain all 0s or 1s. [<bold>We can construct a tensor with all
  elements set to 0</bold>] (<strike>or one</strike>) and a shape of (2,
  3, 4) via the <monospace>zeros</monospace> function.</p>
        </sec>
        <sec id="cell-383cafca-nb-4" specific-use="notebook-content">
          <code language="python">torch.zeros((2, 3, 4))</code>
          <boxed-text>
            <preformat>tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],

        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-1e967d02-nb-4" specific-use="notebook-content">
          <p>Similarly, we can create a tensor with all 1s by invoking
  <monospace>ones</monospace>.</p>
        </sec>
        <sec id="cell-0ea249d4-nb-4" specific-use="notebook-content">
          <code language="python">torch.ones((2, 3, 4))</code>
          <boxed-text>
            <preformat>tensor([[[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]],

        [[1., 1., 1., 1.],
         [1., 1., 1., 1.],
         [1., 1., 1., 1.]]])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-0615f2d6-nb-4" specific-use="notebook-content">
          <p>We often wish to [<bold>sample each element randomly (and
  independently)</bold>] from a given probability distribution. For
  example, the parameters of neural networks are often initialized
  randomly. The following snippet creates a tensor with elements drawn
  from a standard Gaussian (normal) distribution with mean 0 and
  standard deviation 1.</p>
        </sec>
        <sec id="cell-2254595d-nb-4" specific-use="notebook-content">
          <code language="python">torch.randn(3, 4)</code>
          <boxed-text>
            <preformat>tensor([[ 0.1351, -0.9099, -0.2028,  2.1937],
        [-0.3200, -0.7545,  0.8086, -1.8730],
        [ 0.3929,  0.4931,  0.9114, -0.7072]])</preformat>
          </boxed-text>
        </sec>
        <sec id="d35eda39-nb-4" specific-use="notebook-content">
          <p>Finally, we can construct tensors by [<bold>supplying the exact
  values for each element</bold>] by supplying (possibly nested) Python
  list(s) containing numerical literals. Here, we construct a matrix
  with a list of lists, where the outermost list corresponds to axis 0,
  and the inner list corresponds to axis 1.</p>
        </sec>
        <sec id="b26863d8-nb-4" specific-use="notebook-content">
          <code language="python">torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])</code>
          <boxed-text>
            <preformat>tensor([[2, 1, 4, 3],
        [1, 2, 3, 4],
        [4, 3, 2, 1]])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-5b589cdb-nb-4" specific-use="notebook-content">
</sec>
        <sec id="indexing-and-slicing-nb-4">
          <title>Indexing and Slicing</title>
          <p>As with Python lists, we can access tensor elements by indexing
  (starting with 0). To access an element based on its position relative
  to the end of the list, we can use negative indexing. Finally, we can
  access whole ranges of indices via slicing (e.g.,
  <monospace>X[start:stop]</monospace>), where the returned value
  includes the first index (<monospace>start</monospace>) <italic>but
  not the last</italic> (<monospace>stop</monospace>). Finally, when
  only one index (or slice) is specified for a
  <inline-formula><alternatives><tex-math><![CDATA[k^\textrm{th}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>k</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>-order
  tensor, it is applied along axis 0. Thus, in the following code,
  [<bold><monospace>[-1]</monospace> selects the last row and
  <monospace>[1:3]</monospace> selects the second and third
  rows</bold>].</p>
        </sec>
        <sec id="d9049a53-nb-4" specific-use="notebook-content">
          <code language="python">X[-1], X[1:3]</code>
          <boxed-text>
            <preformat>(tensor([ 8.,  9., 10., 11.]),
 tensor([[ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.]]))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-5450673b-nb-4" specific-use="notebook-content">
          <p>Beyond reading them, (<bold>we can also <italic>write</italic>
  elements of a matrix by specifying indices.</bold>)</p>
        </sec>
        <sec id="cell-9246619c-nb-4" specific-use="notebook-content">
          <code language="python">X[1, 2] = 17
X</code>
          <boxed-text>
            <preformat>tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5., 17.,  7.],
        [ 8.,  9., 10., 11.]])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-31f06903-nb-4" specific-use="notebook-content">
          <p>If we want [<bold>to assign multiple elements the same value, we
  apply the indexing on the left-hand side of the assignment
  operation.</bold>] For instance, <monospace>[:2, :]</monospace>
  accesses the first and second rows, where <monospace>:</monospace>
  takes all the elements along axis 1 (column). While we discussed
  indexing for matrices, this also works for vectors and for tensors of
  more than two dimensions.</p>
        </sec>
        <sec id="cell-0532f024-nb-4" specific-use="notebook-content">
          <code language="python">X[:2, :] = 12
X</code>
          <boxed-text>
            <preformat>tensor([[12., 12., 12., 12.],
        [12., 12., 12., 12.],
        [ 8.,  9., 10., 11.]])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-02cdce97-nb-4" specific-use="notebook-content">
</sec>
        <sec id="operations-nb-4">
          <title>Operations</title>
          <p>Now that we know how to construct tensors and how to read from and
  write to their elements, we can begin to manipulate them with various
  mathematical operations. Among the most useful of these are the
  <italic>elementwise</italic> operations. These apply a standard scalar
  operation to each element of a tensor. For functions that take two
  tensors as inputs, elementwise operations apply some standard binary
  operator on each pair of corresponding elements. We can create an
  elementwise function from any function that maps from a scalar to a
  scalar.</p>
          <p>In mathematical notation, we denote such <italic>unary</italic>
  scalar operators (taking one input) by the signature
  <inline-formula><alternatives><tex-math><![CDATA[f: \mathbb{R} \rightarrow \mathbb{R}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi>‚Ñù</mml:mi><mml:mo>‚Üí</mml:mo><mml:mi>‚Ñù</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
  This just means that the function maps from any real number onto some
  other real number. Most standard operators, including unary ones like
  <inline-formula><alternatives><tex-math><![CDATA[e^x]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:msup></mml:math></alternatives></inline-formula>,
  can be applied elementwise.</p>
        </sec>
        <sec id="cell-6dd6724c-nb-4" specific-use="notebook-content">
          <code language="python">torch.exp(x)</code>
          <boxed-text>
            <preformat>tensor([162754.7969, 162754.7969, 162754.7969, 162754.7969, 162754.7969,
        162754.7969, 162754.7969, 162754.7969,   2980.9580,   8103.0840,
         22026.4648,  59874.1406])</preformat>
          </boxed-text>
        </sec>
        <sec id="b70f353f-nb-4" specific-use="notebook-content">
          <p>Likewise, we denote <italic>binary</italic> scalar operators, which
  map pairs of real numbers to a (single) real number via the signature
  <inline-formula><alternatives><tex-math><![CDATA[f: \mathbb{R}, \mathbb{R} \rightarrow \mathbb{R}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:mi>‚Ñù</mml:mi><mml:mo>,</mml:mo><mml:mi>‚Ñù</mml:mi><mml:mo>‚Üí</mml:mo><mml:mi>‚Ñù</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
  Given any two vectors <inline-formula><alternatives><tex-math><![CDATA[\mathbf{u}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêÆ</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives><tex-math><![CDATA[\mathbf{v}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêØ</mml:mi></mml:math></alternatives></inline-formula>
  <italic>of the same shape</italic>, and a binary operator
  <inline-formula><alternatives><tex-math><![CDATA[f]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>,
  we can produce a vector <inline-formula><alternatives><tex-math><![CDATA[\mathbf{c} = F(\mathbf{u},\mathbf{v})]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêú</mml:mi><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùêÆ</mml:mi><mml:mo>,</mml:mo><mml:mi>ùêØ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  by setting <inline-formula><alternatives><tex-math><![CDATA[c_i \gets f(u_i, v_i)]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>‚Üê</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  for all <inline-formula><alternatives><tex-math><![CDATA[i]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>,
  where <inline-formula><alternatives><tex-math><![CDATA[c_i, u_i]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives><tex-math><![CDATA[v_i]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  are the <inline-formula><alternatives><tex-math><![CDATA[i^\textrm{th}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msup><mml:mi>i</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
  elements of vectors <inline-formula><alternatives><tex-math><![CDATA[\mathbf{c}, \mathbf{u}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>ùêú</mml:mi><mml:mo>,</mml:mo><mml:mi>ùêÆ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives><tex-math><![CDATA[\mathbf{v}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>ùêØ</mml:mi></mml:math></alternatives></inline-formula>.
  Here, we produced the vector-valued <inline-formula><alternatives><tex-math><![CDATA[F: \mathbb{R}^d, \mathbb{R}^d \rightarrow \mathbb{R}^d]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>F</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>‚Üí</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  by <italic>lifting</italic> the scalar function to an elementwise
  vector operation. The common standard arithmetic operators for
  addition (<monospace>+</monospace>), subtraction
  (<monospace>-</monospace>), multiplication (<monospace>*</monospace>),
  division (<monospace>/</monospace>), and exponentiation
  (<monospace>**</monospace>) have all been <italic>lifted</italic> to
  elementwise operations for identically-shaped tensors of arbitrary
  shape.</p>
        </sec>
        <sec id="cell-89bc996d-nb-4" specific-use="notebook-content">
          <code language="python">x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
x + y, x - y, x * y, x / y, x ** y</code>
          <boxed-text>
            <preformat>(tensor([ 3.,  4.,  6., 10.]),
 tensor([-1.,  0.,  2.,  6.]),
 tensor([ 2.,  4.,  8., 16.]),
 tensor([0.5000, 1.0000, 2.0000, 4.0000]),
 tensor([ 1.,  4., 16., 64.]))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-04ae1d38-nb-4" specific-use="notebook-content">
          <p>In addition to elementwise computations, we can also perform linear
  algebraic operations, such as dot products and matrix multiplications.
  We will elaborate on these in the Linear Algebra Section.</p>
          <p>We can also [<bold><italic>concatenate</italic> multiple
  tensors,</bold>] stacking them end-to-end to form a larger one. We
  just need to provide a list of tensors and tell the system along which
  axis to concatenate. The example below shows what happens when we
  concatenate two matrices along rows (axis 0) instead of columns (axis
  1). We can see that the first output‚Äôs axis-0 length
  (<inline-formula><alternatives><tex-math><![CDATA[6]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>6</mml:mn></mml:math></alternatives></inline-formula>)
  is the sum of the two input tensors‚Äô axis-0 lengths
  (<inline-formula><alternatives><tex-math><![CDATA[3 + 3]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>3</mml:mn><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>);
  while the second output‚Äôs axis-1 length
  (<inline-formula><alternatives><tex-math><![CDATA[8]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mn>8</mml:mn></mml:math></alternatives></inline-formula>)
  is the sum of the two input tensors‚Äô axis-1 lengths
  (<inline-formula><alternatives><tex-math><![CDATA[4 + 4]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>4</mml:mn><mml:mo>+</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>).</p>
        </sec>
        <sec id="cell-43aa9012-nb-4" specific-use="notebook-content">
          <code language="python">X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)</code>
          <boxed-text>
            <preformat>(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [ 2.,  1.,  4.,  3.],
         [ 1.,  2.,  3.,  4.],
         [ 4.,  3.,  2.,  1.]]),
 tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-346adeed-nb-4" specific-use="notebook-content">
          <p>Sometimes, we want to [<bold>construct a binary tensor via
  <italic>logical statements</italic>.</bold>] Take
  <monospace>X == Y</monospace> as an example. For each position
  <monospace>i, j</monospace>, if <monospace>X[i, j]</monospace> and
  <monospace>Y[i, j]</monospace> are equal, then the corresponding entry
  in the result takes value <monospace>1</monospace>, otherwise it takes
  value <monospace>0</monospace>.</p>
        </sec>
        <sec id="cell-91d39e58-nb-4" specific-use="notebook-content">
          <code language="python">X == Y</code>
          <boxed-text>
            <preformat>tensor([[False,  True, False,  True],
        [False, False, False, False],
        [False, False, False, False]])</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-00448db5-nb-4" specific-use="notebook-content">
          <p>[<bold>Summing all the elements in the tensor</bold>] yields a
  tensor with only one element.</p>
        </sec>
        <sec id="cell-080b0125-nb-4" specific-use="notebook-content">
          <code language="python">X.sum()</code>
          <boxed-text>
            <preformat>tensor(66.)</preformat>
          </boxed-text>
        </sec>
        <sec id="e6a78360-nb-4" specific-use="notebook-content">
</sec>
        <sec id="broadcasting-nb-4">
          <title>Broadcasting</title>
          <p>By now, you know how to perform elementwise binary operations on
  two tensors of the same shape. Under certain conditions, even when
  shapes differ, we can still [<bold>perform elementwise binary
  operations by invoking the <italic>broadcasting
  mechanism</italic>.</bold>] Broadcasting works according to the
  following two-step procedure: (i) expand one or both arrays by copying
  elements along axes with length 1 so that after this transformation,
  the two tensors have the same shape; (ii) perform an elementwise
  operation on the resulting arrays.</p>
        </sec>
        <sec id="be37d2de-nb-4" specific-use="notebook-content">
          <code language="python">a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b</code>
          <boxed-text>
            <preformat>(tensor([[0],
         [1],
         [2]]),
 tensor([[0, 1]]))</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-6c7e8410-nb-4" specific-use="notebook-content">
          <p>Since <monospace>a</monospace> and <monospace>b</monospace> are
  <inline-formula><alternatives><tex-math><![CDATA[3\times1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>3</mml:mn><mml:mo>√ó</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives><tex-math><![CDATA[1\times2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mo>√ó</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  matrices, respectively, their shapes do not match up. Broadcasting
  produces a larger <inline-formula><alternatives><tex-math><![CDATA[3\times2]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>3</mml:mn><mml:mo>√ó</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  matrix by replicating matrix <monospace>a</monospace> along the
  columns and matrix <monospace>b</monospace> along the rows before
  adding them elementwise.</p>
        </sec>
        <sec id="cell-9f62e827-nb-4" specific-use="notebook-content">
          <code language="python">a + b</code>
          <boxed-text>
            <preformat>tensor([[0, 1],
        [1, 2],
        [2, 3]])</preformat>
          </boxed-text>
        </sec>
        <sec id="c5d68609-nb-4" specific-use="notebook-content">
</sec>
        <sec id="saving-memory-nb-4">
          <title>Saving Memory</title>
          <p>[<bold>Running operations can cause new memory to be allocated to
  host results.</bold>] For example, if we write
  <monospace>Y = X + Y</monospace>, we dereference the tensor that
  <monospace>Y</monospace> used to point to and instead point
  <monospace>Y</monospace> at the newly allocated memory. We can
  demonstrate this issue with Python‚Äôs <monospace>id()</monospace>
  function, which gives us the exact address of the referenced object in
  memory. Note that after we run <monospace>Y = Y + X</monospace>,
  <monospace>id(Y)</monospace> points to a different location. That is
  because Python first evaluates <monospace>Y + X</monospace>,
  allocating new memory for the result and then points
  <monospace>Y</monospace> to this new location in memory.</p>
        </sec>
        <sec id="cell-754a7433-nb-4" specific-use="notebook-content">
          <code language="python">before = id(Y)
Y = Y + X
id(Y) == before</code>
          <boxed-text>
            <preformat>False</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-322d26f5-nb-4" specific-use="notebook-content">
          <p>This might be undesirable for two reasons. First, we do not want to
  run around allocating memory unnecessarily all the time. In machine
  learning, we often have hundreds of megabytes of parameters and update
  all of them multiple times per second. Whenever possible, we want to
  perform these updates <italic>in place</italic>. Second, we might
  point at the same parameters from multiple variables. If we do not
  update in place, we must be careful to update all of these references,
  lest we spring a memory leak or inadvertently refer to stale
  parameters.</p>
        </sec>
        <sec id="cell-82880947-nb-4" specific-use="notebook-content">
          <p>Fortunately, (<bold>performing in-place operations</bold>) is easy.
  We can assign the result of an operation to a previously allocated
  array <monospace>Y</monospace> by using slice notation:
  <monospace>Y[:] = &lt;expression&gt;</monospace>. To illustrate this
  concept, we overwrite the values of tensor <monospace>Z</monospace>,
  after initializing it, using <monospace>zeros_like</monospace>, to
  have the same shape as <monospace>Y</monospace>.</p>
        </sec>
        <sec id="c4d62609-nb-4" specific-use="notebook-content">
          <code language="python">Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))</code>
          <boxed-text>
            <preformat>id(Z): 140381179266448
id(Z): 140381179266448</preformat>
          </boxed-text>
        </sec>
        <sec id="d745b125-nb-4" specific-use="notebook-content">
          <p>[<bold>If the value of <monospace>X</monospace> is not reused in
  subsequent computations, we can also use
  <monospace>X[:] = X + Y</monospace> or <monospace>X += Y</monospace>
  to reduce the memory overhead of the operation.</bold>]</p>
        </sec>
        <sec id="b8c13447-nb-4" specific-use="notebook-content">
          <code language="python">before = id(X)
X += Y
id(X) == before</code>
          <boxed-text>
            <preformat>True</preformat>
          </boxed-text>
        </sec>
        <sec id="b5f887dd-nb-4" specific-use="notebook-content">
</sec>
        <sec id="conversion-to-other-python-objects-nb-4">
          <title>Conversion to Other Python Objects</title>
        </sec>
        <sec id="cd057d04-nb-4" specific-use="notebook-content">
          <p>[<bold>Converting to a NumPy tensor
  (<monospace>ndarray</monospace>)</bold>], or vice versa, is easy. The
  torch tensor and NumPy array will share their underlying memory, and
  changing one through an in-place operation will also change the
  other.</p>
        </sec>
        <sec id="cell-576963aa-nb-4" specific-use="notebook-content">
          <code language="python">A = X.numpy()
B = torch.from_numpy(A)
type(A), type(B)</code>
          <boxed-text>
            <preformat>(numpy.ndarray, torch.Tensor)</preformat>
          </boxed-text>
        </sec>
        <sec id="b2def017-nb-4" specific-use="notebook-content">
          <p>To (<bold>convert a size-1 tensor to a Python scalar</bold>), we
  can invoke the <monospace>item</monospace> function or Python‚Äôs
  built-in functions.</p>
        </sec>
        <sec id="cell-388c5252-nb-4" specific-use="notebook-content">
          <code language="python">a = torch.tensor([3.5])
a, a.item(), float(a), int(a)</code>
          <boxed-text>
            <preformat>(tensor([3.5000]), 3.5, 3.5, 3)</preformat>
          </boxed-text>
        </sec>
        <sec id="cell-9373077d-nb-4" specific-use="notebook-content">
</sec>
        <sec id="summary-nb-4">
          <title>Summary</title>
          <p>The tensor class is the main interface for storing and manipulating
  data in deep learning libraries. Tensors provide a variety of
  functionalities including construction routines; indexing and slicing;
  basic mathematics operations; broadcasting; memory-efficient
  assignment; and conversion to and from other Python objects.</p>
        </sec>
        <sec id="exercises-nb-4">
          <title>Exercises</title>
          <list list-type="order">
            <list-item>
              <p>Run the code in this section. Change the conditional statement
      <monospace>X == Y</monospace> to <monospace>X &lt; Y</monospace>
      or <monospace>X &gt; Y</monospace>, and then see what kind of
      tensor you can get.</p>
            </list-item>
            <list-item>
              <p>Replace the two tensors that operate by element in the
      broadcasting mechanism with other shapes, e.g., 3-dimensional
      tensors. Is the result the same as expected?</p>
            </list-item>
          </list>
        </sec>
      </sec>
    </body>
    <back>
</back>
  </sub-article>
  <!-- (F2ED4C6E)[nb-5]:/home/philipp/projects/DataScienceForElectronMicroscopy/notebooks/02_data_fusion.ipynb -->
</article>
