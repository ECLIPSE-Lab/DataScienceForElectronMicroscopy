<sub-article article-type="notebook" id="nb-2">
<front-stub>
<title-group>
<article-title>01.3 Automatic Differentiation</article-title>
</title-group>
</front-stub>

<body>
<sec id="cell-4144c129" specific-use="notebook-content">
<p>Recall from Section Calculus that calculating derivatives is the
crucial step in all the optimization algorithms that we will use to
train deep networks. While the calculations are straightforward, working
them out by hand can be tedious and error-prone, and these issues only
grow as our models become more complex.</p>
<p>Fortunately all modern deep learning frameworks take this work off
our plates by offering <italic>automatic differentiation</italic> (often
shortened to <italic>autograd</italic>). As we pass data through each
successive function, the framework builds a <italic>computational
graph</italic> that tracks how each value depends on others. To
calculate derivatives, automatic differentiation works backwards through
this graph applying the chain rule. The computational algorithm for
applying the chain rule in this fashion is called
<italic>backpropagation</italic>.</p>
<p>While autograd libraries have become a hot concern over the past
decade, they have a long history. In fact the earliest references to
autograd date back over half of a century
:cite:<monospace>Wengert.1964</monospace>. The core ideas behind modern
backpropagation date to a PhD thesis from 1980
:cite:<monospace>Speelpenning.1980</monospace> and were further
developed in the late 1980s :cite:<monospace>Griewank.1989</monospace>.
While backpropagation has become the default method for computing
gradients, it is not the only option. For instance, the Julia
programming language employs forward propagation
:cite:<monospace>Revels.Lubin.Papamarkou.2016</monospace>. Before
exploring methods, let‚Äôs first master the autograd package.</p>
</sec>
<sec id="cell-130439cd" specific-use="notebook-content">
<code language="python">import torch</code>
</sec>
<sec id="e2ab3850" specific-use="notebook-content">
<sec id="a-simple-function">
  <title>A Simple Function</title>
  <p>Let‚Äôs assume that we are interested in (<bold>differentiating the
  function <inline-formula><alternatives>
  <tex-math><![CDATA[y = 2\mathbf{x}^{\top}\mathbf{x}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>ùê±</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùê±</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  with respect to the column vector <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{x}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>.</bold>)
  To start, we assign <monospace>x</monospace> an initial value.</p>
  </sec>
  <sec id="cell-4253cfab" specific-use="notebook-content">
  <code language="python">x = torch.arange(4.0)
x</code>
  <boxed-text>
    <preformat>tensor([0., 1., 2., 3.])</preformat>
  </boxed-text>
  </sec>
  <sec id="e75614b0" specific-use="notebook-content">
  <p>[<bold>Before we calculate the gradient of
  <inline-formula><alternatives>
  <tex-math><![CDATA[y]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>
  with respect to <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{x}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>,
  we need a place to store it.</bold>] In general, we avoid allocating
  new memory every time we take a derivative because deep learning
  requires successively computing derivatives with respect to the same
  parameters a great many times, and we might risk running out of
  memory. Note that the gradient of a scalar-valued function with
  respect to a vector <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{x}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>
  is vector-valued with the same shape as <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{x}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>.</p>
  </sec>
  <sec id="cell-2a001d1e" specific-use="notebook-content">
  <code language="python"># Can also create x = torch.arange(4.0, requires_grad=True)
x.requires_grad_(True)
x.grad  # The gradient is None by default</code>
  </sec>
  <sec id="cell-2e74bc02" specific-use="notebook-content">
  <p>(<bold>We now calculate our function of <monospace>x</monospace>
  and assign the result to <monospace>y</monospace>.</bold>)</p>
  </sec>
  <sec id="cell-6e3bd777" specific-use="notebook-content">
  <code language="python">y = 2 * torch.dot(x, x)
y</code>
  <boxed-text>
    <preformat>tensor(28., grad_fn=&lt;MulBackward0&gt;)</preformat>
  </boxed-text>
  </sec>
  <sec id="c3067490" specific-use="notebook-content">
  <p>[<bold>We can now take the gradient of <monospace>y</monospace>
  with respect to <monospace>x</monospace></bold>] by calling its
  <monospace>backward</monospace> method. Next, we can access the
  gradient via <monospace>x</monospace>‚Äôs <monospace>grad</monospace>
  attribute.</p>
  </sec>
  <sec id="cell-21b134ae" specific-use="notebook-content">
  <code language="python">y.backward()
x.grad</code>
  <boxed-text>
    <preformat>tensor([ 0.,  4.,  8., 12.])</preformat>
  </boxed-text>
  </sec>
  <sec id="cell-17d1390b" specific-use="notebook-content">
  <p>(<bold>We already know that the gradient of the function
  <inline-formula><alternatives>
  <tex-math><![CDATA[y = 2\mathbf{x}^{\top}\mathbf{x}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>ùê±</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:mi>ùê±</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  with respect to <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{x}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùê±</mml:mi></mml:math></alternatives></inline-formula>
  should be <inline-formula><alternatives>
  <tex-math><![CDATA[4\mathbf{x}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>4</mml:mn><mml:mi>ùê±</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</bold>)
  We can now verify that the automatic gradient computation and the
  expected result are identical.</p>
  </sec>
  <sec id="cell-5030e37d" specific-use="notebook-content">
  <code language="python">x.grad == 4 * x</code>
  <boxed-text>
    <preformat>tensor([True, True, True, True])</preformat>
  </boxed-text>
  </sec>
  <sec id="da440e48" specific-use="notebook-content">
  <p>[<bold>Now let‚Äôs calculate another function of
  <monospace>x</monospace> and take its gradient.</bold>] Note that
  PyTorch does not automatically reset the gradient buffer when we
  record a new gradient. Instead, the new gradient is added to the
  already-stored gradient. This behavior comes in handy when we want to
  optimize the sum of multiple objective functions. To reset the
  gradient buffer, we can call <monospace>x.grad.zero_()</monospace> as
  follows:</p>
  </sec>
  <sec id="add5cf4b" specific-use="notebook-content">
  <code language="python">x.grad.zero_()  # Reset the gradient
y = x.sum()
y.backward()
x.grad</code>
  <boxed-text>
    <preformat>tensor([1., 1., 1., 1.])</preformat>
  </boxed-text>
  </sec>
  <sec id="cell-8bdd4c0c" specific-use="notebook-content">
</sec>
<sec id="backward-for-non-scalar-variables">
  <title>Backward for Non-Scalar Variables</title>
  <p>When <monospace>y</monospace> is a vector, the most natural
  representation of the derivative of <monospace>y</monospace> with
  respect to a vector <monospace>x</monospace> is a matrix called the
  <italic>Jacobian</italic> that contains the partial derivatives of
  each component of <monospace>y</monospace> with respect to each
  component of <monospace>x</monospace>. Likewise, for higher-order
  <monospace>y</monospace> and <monospace>x</monospace>, the result of
  differentiation could be an even higher-order tensor.</p>
  <p>While Jacobians do show up in some advanced machine learning
  techniques, more commonly we want to sum up the gradients of each
  component of <monospace>y</monospace> with respect to the full vector
  <monospace>x</monospace>, yielding a vector of the same shape as
  <monospace>x</monospace>. For example, we often have a vector
  representing the value of our loss function calculated separately for
  each example among a <italic>batch</italic> of training examples.
  Here, we just want to (<bold>sum up the gradients computed
  individually for each example</bold>).</p>
  </sec>
  <sec id="cell-9dda7124" specific-use="notebook-content">
  <p>Because deep learning frameworks vary in how they interpret
  gradients of non-scalar tensors, PyTorch takes some steps to avoid
  confusion. Invoking <monospace>backward</monospace> on a non-scalar
  elicits an error unless we tell PyTorch how to reduce the object to a
  scalar. More formally, we need to provide some vector
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{v}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêØ</mml:mi></mml:math></alternatives></inline-formula>
  such that <monospace>backward</monospace> will compute
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{v}^\top \partial_{\mathbf{x}} \mathbf{y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>ùêØ</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:msub><mml:mi>‚àÇ</mml:mi><mml:mi>ùê±</mml:mi></mml:msub><mml:mi>ùê≤</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  rather than <inline-formula><alternatives>
  <tex-math><![CDATA[\partial_{\mathbf{x}} \mathbf{y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>‚àÇ</mml:mi><mml:mi>ùê±</mml:mi></mml:msub><mml:mi>ùê≤</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
  This next part may be confusing, but for reasons that will become
  clear later, this argument (representing
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{v}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêØ</mml:mi></mml:math></alternatives></inline-formula>)
  is named <monospace>gradient</monospace>. For a more detailed
  description, see Yang Zhang‚Äôs
  <ext-link ext-link-type="uri" xlink:href="https://zhang-yang.medium.com/the-gradient-argument-in-pytorchs-backward-function-explained-by-examples-68f266950c29">Medium
  post</ext-link>.</p>
  </sec>
  <sec id="cell-1baa40bd" specific-use="notebook-content">
  <code language="python">x.grad.zero_()
y = x * x
y.backward(gradient=torch.ones(len(y)))  # Faster: y.sum().backward()
x.grad</code>
  <boxed-text>
    <preformat>tensor([0., 2., 4., 6.])</preformat>
  </boxed-text>
  </sec>
  <sec id="ffbd2c9d" specific-use="notebook-content">
</sec>
<sec id="detaching-computation">
  <title>Detaching Computation</title>
  <p>Sometimes, we wish to [<bold>move some calculations outside of the
  recorded computational graph.</bold>] For example, say that we use the
  input to create some auxiliary intermediate terms for which we do not
  want to compute a gradient. In this case, we need to
  <italic>detach</italic> the respective computational graph from the
  final result. The following toy example makes this clearer: suppose we
  have <monospace>z = x * y</monospace> and
  <monospace>y = x * x</monospace> but we want to focus on the
  <italic>direct</italic> influence of <monospace>x</monospace> on
  <monospace>z</monospace> rather than the influence conveyed via
  <monospace>y</monospace>. In this case, we can create a new variable
  <monospace>u</monospace> that takes the same value as
  <monospace>y</monospace> but whose <italic>provenance</italic> (how it
  was created) has been wiped out. Thus <monospace>u</monospace> has no
  ancestors in the graph and gradients do not flow through
  <monospace>u</monospace> to <monospace>x</monospace>. For example,
  taking the gradient of <monospace>z = x * u</monospace> will yield the
  result <monospace>u</monospace>, (not <monospace>3 * x * x</monospace>
  as you might have expected since
  <monospace>z = x * x * x</monospace>).</p>
  </sec>
  <sec id="cell-107ac041" specific-use="notebook-content">
  <code language="python">x.grad.zero_()
y = x * x
u = y.detach()
z = u * x

z.sum().backward()
x.grad == u</code>
  <boxed-text>
    <preformat>tensor([True, True, True, True])</preformat>
  </boxed-text>
  </sec>
  <sec id="e0378e1f" specific-use="notebook-content">
  <p>Note that while this procedure detaches <monospace>y</monospace>‚Äôs
  ancestors from the graph leading to <monospace>z</monospace>, the
  computational graph leading to <monospace>y</monospace> persists and
  thus we can calculate the gradient of <monospace>y</monospace> with
  respect to <monospace>x</monospace>.</p>
  </sec>
  <sec id="cb8c674b" specific-use="notebook-content">
  <code language="python">x.grad.zero_()
y.sum().backward()
x.grad == 2 * x</code>
  <boxed-text>
    <preformat>tensor([True, True, True, True])</preformat>
  </boxed-text>
  </sec>
  <sec id="cell-76f056ce" specific-use="notebook-content">
</sec>
<sec id="gradients-and-python-control-flow">
  <title>Gradients and Python Control Flow</title>
  <p>So far we reviewed cases where the path from input to output was
  well defined via a function such as
  <monospace>z = x * x * x</monospace>. Programming offers us a lot more
  freedom in how we compute results. For instance, we can make them
  depend on auxiliary variables or condition choices on intermediate
  results. One benefit of using automatic differentiation is that
  [<bold>even if</bold>] building the computational graph of (<bold>a
  function required passing through a maze of Python control
  flow</bold>) (e.g., conditionals, loops, and arbitrary function
  calls), (<bold>we can still calculate the gradient of the resulting
  variable.</bold>) To illustrate this, consider the following code
  snippet where the number of iterations of the
  <monospace>while</monospace> loop and the evaluation of the
  <monospace>if</monospace> statement both depend on the value of the
  input <monospace>a</monospace>.</p>
  </sec>
  <sec id="a83327c2" specific-use="notebook-content">
  <code language="python">def f(a):
    b = a * 2
    while b.norm() &lt; 1000:
        b = b * 2
    if b.sum() &gt; 0:
        c = b
    else:
        c = 100 * b
    return c</code>
  </sec>
  <sec id="cell-189f6785" specific-use="notebook-content">
  <p>Below, we call this function, passing in a random value, as input.
  Since the input is a random variable, we do not know what form the
  computational graph will take. However, whenever we execute
  <monospace>f(a)</monospace> on a specific input, we realize a specific
  computational graph and can subsequently run
  <monospace>backward</monospace>.</p>
  </sec>
  <sec id="c5ef0264" specific-use="notebook-content">
  <code language="python">a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()</code>
  </sec>
  <sec id="cell-51065133" specific-use="notebook-content">
  <p>Even though our function <monospace>f</monospace> is, for
  demonstration purposes, a bit contrived, its dependence on the input
  is quite simple: it is a <italic>linear</italic> function of
  <monospace>a</monospace> with piecewise defined scale. As such,
  <monospace>f(a) / a</monospace> is a vector of constant entries and,
  moreover, <monospace>f(a) / a</monospace> needs to match the gradient
  of <monospace>f(a)</monospace> with respect to
  <monospace>a</monospace>.</p>
  </sec>
  <sec id="ab14ef91" specific-use="notebook-content">
  <code language="python">a.grad == d / a</code>
  <boxed-text>
    <preformat>tensor(True)</preformat>
  </boxed-text>
  </sec>
  <sec id="a992f28c" specific-use="notebook-content">
  <p>Dynamic control flow is very common in deep learning. For instance,
  when processing text, the computational graph depends on the length of
  the input. In these cases, automatic differentiation becomes vital for
  statistical modeling since it is impossible to compute the gradient
  <italic>a priori</italic>.</p>
</sec>
<sec id="discussion">
  <title>Discussion</title>
  <p>You have now gotten a taste of the power of automatic
  differentiation. The development of libraries for calculating
  derivatives both automatically and efficiently has been a massive
  productivity booster for deep learning practitioners, liberating them
  so they can focus on less menial. Moreover, autograd lets us design
  massive models for which pen and paper gradient computations would be
  prohibitively time consuming. Interestingly, while we use autograd to
  <italic>optimize</italic> models (in a statistical sense) the
  <italic>optimization</italic> of autograd libraries themselves (in a
  computational sense) is a rich subject of vital interest to framework
  designers. Here, tools from compilers and graph manipulation are
  leveraged to compute results in the most expedient and
  memory-efficient manner.</p>
  <p>For now, try to remember these basics: (i) attach gradients to
  those variables with respect to which we desire derivatives; (ii)
  record the computation of the target value; (iii) execute the
  backpropagation function; and (iv) access the resulting gradient.</p>
</sec>
<sec id="exercises">
  <title>Exercises</title>
  <list list-type="order">
    <list-item>
      <p>Why is the second derivative much more expensive to compute
      than the first derivative?</p>
    </list-item>
    <list-item>
      <p>After running the function for backpropagation, immediately run
      it again and see what happens. Investigate.</p>
    </list-item>
    <list-item>
      <p>In the control flow example where we calculate the derivative
      of <monospace>d</monospace> with respect to
      <monospace>a</monospace>, what would happen if we changed the
      variable <monospace>a</monospace> to a random vector or a matrix?
      At this point, the result of the calculation
      <monospace>f(a)</monospace> is no longer a scalar. What happens to
      the result? How do we analyze this?</p>
    </list-item>
    <list-item>
      <p>Let <inline-formula><alternatives>
      <tex-math><![CDATA[f(x) = \sin(x)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>sin</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
      Plot the graph of <inline-formula><alternatives>
      <tex-math><![CDATA[f]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
      and of its derivative <inline-formula><alternatives>
      <tex-math><![CDATA[f']]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mi>‚Ä≤</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
      Do not exploit the fact that <inline-formula><alternatives>
      <tex-math><![CDATA[f'(x) = \cos(x)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mi>‚Ä≤</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>cos</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      but rather use automatic differentiation to get the result.</p>
    </list-item>
    <list-item>
      <p>Let <inline-formula><alternatives>
      <tex-math><![CDATA[f(x) = ((\log x^2) \cdot \sin x) + x^{-1}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mo>log</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>‚ãÖ</mml:mo><mml:mo>sin</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.
      Write out a dependency graph tracing results from
      <inline-formula><alternatives>
      <tex-math><![CDATA[x]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      to <inline-formula><alternatives>
      <tex-math><![CDATA[f(x)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
    </list-item>
    <list-item>
      <p>Use the chain rule to compute the derivative
      <inline-formula><alternatives>
      <tex-math><![CDATA[\frac{df}{dx}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>
      of the aforementioned function, placing each term on the
      dependency graph that you constructed previously.</p>
    </list-item>
    <list-item>
      <p>Given the graph and the intermediate derivative results, you
      have a number of options when computing the gradient. Evaluate the
      result once starting from <inline-formula><alternatives>
      <tex-math><![CDATA[x]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      to <inline-formula><alternatives>
      <tex-math><![CDATA[f]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
      and once from <inline-formula><alternatives>
      <tex-math><![CDATA[f]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
      tracing back to <inline-formula><alternatives>
      <tex-math><![CDATA[x]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>.
      The path from <inline-formula><alternatives>
      <tex-math><![CDATA[x]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      to <inline-formula><alternatives>
      <tex-math><![CDATA[f]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
      is commonly known as <italic>forward differentiation</italic>,
      whereas the path from <inline-formula><alternatives>
      <tex-math><![CDATA[f]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
      to <inline-formula><alternatives>
      <tex-math><![CDATA[x]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      is known as backward differentiation.</p>
    </list-item>
    <list-item>
      <p>When might you want to use forward, and when backward,
      differentiation? Hint: consider the amount of intermediate data
      needed, the ability to parallelize steps, and the size of matrices
      and vectors involved.</p>
    </list-item>
  </list>
  </sec>
</sec>
</body>



<back>
</back>


</sub-article>