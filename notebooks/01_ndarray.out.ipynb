{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01.1 Data Manipulation\n",
        "\n",
        "In order to get anything done, we need some way to store and manipulate\n",
        "data. Generally, there are two important things we need to do with data:\n",
        "(i) acquire them; and (ii) process them once they are inside the\n",
        "computer. There is no point in acquiring data without some way to store\n",
        "it, so to start, let’s get our hands dirty with $n$-dimensional arrays,\n",
        "which we also call *tensors*. If you already know the NumPy scientific\n",
        "computing package, this will be a breeze. For all modern deep learning\n",
        "frameworks, the *tensor class* (`ndarray` in MXNet, `Tensor` in PyTorch\n",
        "and TensorFlow) resembles NumPy’s `ndarray`, with a few killer features\n",
        "added. First, the tensor class supports automatic differentiation.\n",
        "Second, it leverages GPUs to accelerate numerical computation, whereas\n",
        "NumPy only runs on CPUs. These properties make neural networks both easy\n",
        "to code and fast to run.\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "(**To start, we import the PyTorch library. Note that the package name\n",
        "is `torch`.**)"
      ],
      "id": "c0995405-e6b1-4855-b571-b9fdef711cc9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:55.152236Z",
          "iopub.status.busy": "2023-08-18T19:32:55.151500Z",
          "iopub.status.idle": "2023-08-18T19:32:57.051589Z",
          "shell.execute_reply": "2023-08-18T19:32:57.050409Z"
        },
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [],
      "source": [
        "import torch"
      ],
      "id": "01fa8e58"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\[**A tensor represents a (possibly multidimensional) array of numerical\n",
        "values.**\\] In the one-dimensional case, i.e., when only one axis is\n",
        "needed for the data, a tensor is called a *vector*. With two axes, a\n",
        "tensor is called a *matrix*. With $k > 2$ axes, we drop the specialized\n",
        "names and just refer to the object as a $k^\\textrm{th}$-*order tensor*.\n",
        "\n",
        "PyTorch provides a variety of functions for creating new tensors\n",
        "prepopulated with values. For example, by invoking `arange(n)`, we can\n",
        "create a vector of evenly spaced values, starting at 0 (included) and\n",
        "ending at `n` (not included). By default, the interval size is $1$.\n",
        "Unless otherwise specified, new tensors are stored in main memory and\n",
        "designated for CPU-based computation."
      ],
      "id": "9fa0e747-4c9b-4da3-9aa9-e4c35c3566f5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.056039Z",
          "iopub.status.busy": "2023-08-18T19:32:57.055276Z",
          "iopub.status.idle": "2023-08-18T19:32:57.089028Z",
          "shell.execute_reply": "2023-08-18T19:32:57.088195Z"
        },
        "origin_pos": 14,
        "outputId": "1306a523-f421-41ab-a4ae-cecba676f704",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
            ]
          }
        }
      ],
      "source": [
        "x = torch.arange(12, dtype=torch.float32)\n",
        "x"
      ],
      "id": "b6aa30a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each of these values is called an *element* of the tensor. The tensor\n",
        "`x` contains 12 elements. We can inspect the total number of elements in\n",
        "a tensor via its `numel` method."
      ],
      "id": "a52ffb17-e8cb-4934-a3ae-f7e022bd47c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.093138Z",
          "iopub.status.busy": "2023-08-18T19:32:57.092473Z",
          "iopub.status.idle": "2023-08-18T19:32:57.098450Z",
          "shell.execute_reply": "2023-08-18T19:32:57.097452Z"
        },
        "origin_pos": 21,
        "outputId": "17fc0da2-56d1-4bb6-e2e3-96723cd36338",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "12"
            ]
          }
        }
      ],
      "source": [
        "x.numel()"
      ],
      "id": "640cadaf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(**We can access a tensor’s *shape***) (the length along each axis) by\n",
        "inspecting its `shape` attribute. Because we are dealing with a vector\n",
        "here, the `shape` contains just a single element and is identical to the\n",
        "size."
      ],
      "id": "e911ee26-e335-4a02-a812-7e0943382462"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.102194Z",
          "iopub.status.busy": "2023-08-18T19:32:57.101575Z",
          "iopub.status.idle": "2023-08-18T19:32:57.107424Z",
          "shell.execute_reply": "2023-08-18T19:32:57.106501Z"
        },
        "origin_pos": 24,
        "outputId": "6d10f079-7c31-4266-b659-082d25222563",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "torch.Size([12])"
            ]
          }
        }
      ],
      "source": [
        "x.shape"
      ],
      "id": "6e0a9616"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can \\[**change the shape of a tensor without altering its size or\n",
        "values**\\], by invoking `reshape`. For example, we can transform our\n",
        "vector `x` whose shape is (12,) to a matrix `X` with shape (3, 4). This\n",
        "new tensor retains all elements but reconfigures them into a matrix.\n",
        "Notice that the elements of our vector are laid out one row at a time\n",
        "and thus `x[3] == X[0, 3]`."
      ],
      "id": "da92fcaa-cd10-401f-ad78-0b8181990a3a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.111467Z",
          "iopub.status.busy": "2023-08-18T19:32:57.110749Z",
          "iopub.status.idle": "2023-08-18T19:32:57.117759Z",
          "shell.execute_reply": "2023-08-18T19:32:57.116917Z"
        },
        "origin_pos": 26,
        "outputId": "fae038b7-05a7-4669-c470-4398985c419e",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  6.,  7.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          }
        }
      ],
      "source": [
        "X = x.reshape(3, 4)\n",
        "X"
      ],
      "id": "6092207c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that specifying every shape component to `reshape` is redundant.\n",
        "Because we already know our tensor’s size, we can work out one component\n",
        "of the shape given the rest. For example, given a tensor of size $n$ and\n",
        "target shape ($h$, $w$), we know that $w = n/h$. To automatically infer\n",
        "one component of the shape, we can place a `-1` for the shape component\n",
        "that should be inferred automatically. In our case, instead of calling\n",
        "`x.reshape(3, 4)`, we could have equivalently called `x.reshape(-1, 4)`\n",
        "or `x.reshape(3, -1)`.\n",
        "\n",
        "Practitioners often need to work with tensors initialized to contain all\n",
        "0s or 1s. \\[**We can construct a tensor with all elements set to 0**\\]\n",
        "(~~or one~~) and a shape of (2, 3, 4) via the `zeros` function."
      ],
      "id": "f42e7c61-9441-4e53-9e57-8a6e79513d61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.122018Z",
          "iopub.status.busy": "2023-08-18T19:32:57.121194Z",
          "iopub.status.idle": "2023-08-18T19:32:57.128294Z",
          "shell.execute_reply": "2023-08-18T19:32:57.127285Z"
        },
        "origin_pos": 30,
        "outputId": "d1033ebb-c901-4d98-b333-782c43fdfadb",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]]])"
            ]
          }
        }
      ],
      "source": [
        "torch.zeros((2, 3, 4))"
      ],
      "id": "383cafca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, we can create a tensor with all 1s by invoking `ones`."
      ],
      "id": "7ba40c9c-3c5e-4129-8253-06c5992af8c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.132534Z",
          "iopub.status.busy": "2023-08-18T19:32:57.131716Z",
          "iopub.status.idle": "2023-08-18T19:32:57.139029Z",
          "shell.execute_reply": "2023-08-18T19:32:57.138135Z"
        },
        "origin_pos": 35,
        "outputId": "295fb3b2-0c1b-47c9-be8e-b16f27fbf7fc",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]]])"
            ]
          }
        }
      ],
      "source": [
        "torch.ones((2, 3, 4))"
      ],
      "id": "0ea249d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We often wish to \\[**sample each element randomly (and\n",
        "independently)**\\] from a given probability distribution. For example,\n",
        "the parameters of neural networks are often initialized randomly. The\n",
        "following snippet creates a tensor with elements drawn from a standard\n",
        "Gaussian (normal) distribution with mean 0 and standard deviation 1."
      ],
      "id": "2e003c56-ab77-41c5-93d5-99ae2f3b0a1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.143051Z",
          "iopub.status.busy": "2023-08-18T19:32:57.142388Z",
          "iopub.status.idle": "2023-08-18T19:32:57.149695Z",
          "shell.execute_reply": "2023-08-18T19:32:57.148813Z"
        },
        "origin_pos": 40,
        "outputId": "0e069922-7b57-45bb-f8d2-9af959c13b89",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[ 0.1351, -0.9099, -0.2028,  2.1937],\n",
              "        [-0.3200, -0.7545,  0.8086, -1.8730],\n",
              "        [ 0.3929,  0.4931,  0.9114, -0.7072]])"
            ]
          }
        }
      ],
      "source": [
        "torch.randn(3, 4)"
      ],
      "id": "2254595d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can construct tensors by \\[**supplying the exact values for\n",
        "each element**\\] by supplying (possibly nested) Python list(s)\n",
        "containing numerical literals. Here, we construct a matrix with a list\n",
        "of lists, where the outermost list corresponds to axis 0, and the inner\n",
        "list corresponds to axis 1."
      ],
      "id": "284af369-cd6c-46c6-b2e9-f46b73bb3e02"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.153567Z",
          "iopub.status.busy": "2023-08-18T19:32:57.153222Z",
          "iopub.status.idle": "2023-08-18T19:32:57.160436Z",
          "shell.execute_reply": "2023-08-18T19:32:57.159548Z"
        },
        "origin_pos": 45,
        "outputId": "f832b911-6573-4384-d66e-d459ef4e7e2c",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[2, 1, 4, 3],\n",
              "        [1, 2, 3, 4],\n",
              "        [4, 3, 2, 1]])"
            ]
          }
        }
      ],
      "source": [
        "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
      ],
      "id": "b26863d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Indexing and Slicing\n",
        "\n",
        "As with Python lists, we can access tensor elements by indexing\n",
        "(starting with 0). To access an element based on its position relative\n",
        "to the end of the list, we can use negative indexing. Finally, we can\n",
        "access whole ranges of indices via slicing (e.g., `X[start:stop]`),\n",
        "where the returned value includes the first index (`start`) *but not the\n",
        "last* (`stop`). Finally, when only one index (or slice) is specified for\n",
        "a $k^\\textrm{th}$-order tensor, it is applied along axis 0. Thus, in the\n",
        "following code, \\[**`[-1]` selects the last row and `[1:3]` selects the\n",
        "second and third rows**\\]."
      ],
      "id": "4874d0aa-fa2a-4894-bad3-ee7cd0a4eab1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.164537Z",
          "iopub.status.busy": "2023-08-18T19:32:57.163812Z",
          "iopub.status.idle": "2023-08-18T19:32:57.171699Z",
          "shell.execute_reply": "2023-08-18T19:32:57.170451Z"
        },
        "origin_pos": 49,
        "outputId": "7927b279-a900-42d9-9335-b073c4618d8e",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(tensor([ 8.,  9., 10., 11.]),\n",
              " tensor([[ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.]]))"
            ]
          }
        }
      ],
      "source": [
        "X[-1], X[1:3]"
      ],
      "id": "d9049a53"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Beyond reading them, (**we can also *write* elements of a matrix by\n",
        "specifying indices.**)"
      ],
      "id": "fb78fc96-50a4-46f9-8f08-42bdaf431cba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.176047Z",
          "iopub.status.busy": "2023-08-18T19:32:57.175685Z",
          "iopub.status.idle": "2023-08-18T19:32:57.182893Z",
          "shell.execute_reply": "2023-08-18T19:32:57.181890Z"
        },
        "origin_pos": 52,
        "outputId": "5799f12e-3394-48f6-f159-0c5c40fd9d79",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5., 17.,  7.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          }
        }
      ],
      "source": [
        "X[1, 2] = 17\n",
        "X"
      ],
      "id": "9246619c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want \\[**to assign multiple elements the same value, we apply the\n",
        "indexing on the left-hand side of the assignment operation.**\\] For\n",
        "instance, `[:2, :]` accesses the first and second rows, where `:` takes\n",
        "all the elements along axis 1 (column). While we discussed indexing for\n",
        "matrices, this also works for vectors and for tensors of more than two\n",
        "dimensions."
      ],
      "id": "45cd79f6-37c0-4b53-a2e4-05f7a97e7f48"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.186970Z",
          "iopub.status.busy": "2023-08-18T19:32:57.186270Z",
          "iopub.status.idle": "2023-08-18T19:32:57.193303Z",
          "shell.execute_reply": "2023-08-18T19:32:57.192338Z"
        },
        "origin_pos": 56,
        "outputId": "e7a4c050-40e9-4e5c-82dd-3144eea02486",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[12., 12., 12., 12.],\n",
              "        [12., 12., 12., 12.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          }
        }
      ],
      "source": [
        "X[:2, :] = 12\n",
        "X"
      ],
      "id": "0532f024"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Operations\n",
        "\n",
        "Now that we know how to construct tensors and how to read from and write\n",
        "to their elements, we can begin to manipulate them with various\n",
        "mathematical operations. Among the most useful of these are the\n",
        "*elementwise* operations. These apply a standard scalar operation to\n",
        "each element of a tensor. For functions that take two tensors as inputs,\n",
        "elementwise operations apply some standard binary operator on each pair\n",
        "of corresponding elements. We can create an elementwise function from\n",
        "any function that maps from a scalar to a scalar.\n",
        "\n",
        "In mathematical notation, we denote such *unary* scalar operators\n",
        "(taking one input) by the signature\n",
        "$f: \\mathbb{R} \\rightarrow \\mathbb{R}$. This just means that the\n",
        "function maps from any real number onto some other real number. Most\n",
        "standard operators, including unary ones like $e^x$, can be applied\n",
        "elementwise."
      ],
      "id": "ed757401-3f10-444c-8b22-602474fb5f0f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.197301Z",
          "iopub.status.busy": "2023-08-18T19:32:57.196599Z",
          "iopub.status.idle": "2023-08-18T19:32:57.206136Z",
          "shell.execute_reply": "2023-08-18T19:32:57.205188Z"
        },
        "origin_pos": 61,
        "outputId": "8b319940-fecc-46ad-9e0a-ee4918a8e8b7",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([162754.7969, 162754.7969, 162754.7969, 162754.7969, 162754.7969,\n",
              "        162754.7969, 162754.7969, 162754.7969,   2980.9580,   8103.0840,\n",
              "         22026.4648,  59874.1406])"
            ]
          }
        }
      ],
      "source": [
        "torch.exp(x)"
      ],
      "id": "6dd6724c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Likewise, we denote *binary* scalar operators, which map pairs of real\n",
        "numbers to a (single) real number via the signature\n",
        "$f: \\mathbb{R}, \\mathbb{R} \\rightarrow \\mathbb{R}$. Given any two\n",
        "vectors $\\mathbf{u}$ and $\\mathbf{v}$ *of the same shape*, and a binary\n",
        "operator $f$, we can produce a vector\n",
        "$\\mathbf{c} = F(\\mathbf{u},\\mathbf{v})$ by setting\n",
        "$c_i \\gets f(u_i, v_i)$ for all $i$, where $c_i, u_i$, and $v_i$ are the\n",
        "$i^\\textrm{th}$ elements of vectors $\\mathbf{c}, \\mathbf{u}$, and\n",
        "$\\mathbf{v}$. Here, we produced the vector-valued\n",
        "$F: \\mathbb{R}^d, \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$ by *lifting*\n",
        "the scalar function to an elementwise vector operation. The common\n",
        "standard arithmetic operators for addition (`+`), subtraction (`-`),\n",
        "multiplication (`*`), division (`/`), and exponentiation (`**`) have all\n",
        "been *lifted* to elementwise operations for identically-shaped tensors\n",
        "of arbitrary shape."
      ],
      "id": "a422371f-eeb7-494e-9581-597500ded79b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.210417Z",
          "iopub.status.busy": "2023-08-18T19:32:57.209741Z",
          "iopub.status.idle": "2023-08-18T19:32:57.219298Z",
          "shell.execute_reply": "2023-08-18T19:32:57.218318Z"
        },
        "origin_pos": 66,
        "outputId": "5c5cba5a-fbae-4a4f-d326-bfb0c312ace1",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(tensor([ 3.,  4.,  6., 10.]),\n",
              " tensor([-1.,  0.,  2.,  6.]),\n",
              " tensor([ 2.,  4.,  8., 16.]),\n",
              " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
              " tensor([ 1.,  4., 16., 64.]))"
            ]
          }
        }
      ],
      "source": [
        "x = torch.tensor([1.0, 2, 4, 8])\n",
        "y = torch.tensor([2, 2, 2, 2])\n",
        "x + y, x - y, x * y, x / y, x ** y"
      ],
      "id": "89bc996d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition to elementwise computations, we can also perform linear\n",
        "algebraic operations, such as dot products and matrix multiplications.\n",
        "We will elaborate on these in the Linear Algebra Section.\n",
        "\n",
        "We can also \\[***concatenate* multiple tensors,**\\] stacking them\n",
        "end-to-end to form a larger one. We just need to provide a list of\n",
        "tensors and tell the system along which axis to concatenate. The example\n",
        "below shows what happens when we concatenate two matrices along rows\n",
        "(axis 0) instead of columns (axis 1). We can see that the first output’s\n",
        "axis-0 length ($6$) is the sum of the two input tensors’ axis-0 lengths\n",
        "($3 + 3$); while the second output’s axis-1 length ($8$) is the sum of\n",
        "the two input tensors’ axis-1 lengths ($4 + 4$)."
      ],
      "id": "34aea3cc-28e9-4a6c-b373-6a9e47b38549"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.223534Z",
          "iopub.status.busy": "2023-08-18T19:32:57.222711Z",
          "iopub.status.idle": "2023-08-18T19:32:57.233166Z",
          "shell.execute_reply": "2023-08-18T19:32:57.232145Z"
        },
        "origin_pos": 71,
        "outputId": "2fbe190c-0a7b-4f06-d14f-dcc9c33bd650",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [ 2.,  1.,  4.,  3.],\n",
              "         [ 1.,  2.,  3.,  4.],\n",
              "         [ 4.,  3.,  2.,  1.]]),\n",
              " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
              "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
            ]
          }
        }
      ],
      "source": [
        "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
        "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
      ],
      "id": "43aa9012"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sometimes, we want to \\[**construct a binary tensor via *logical\n",
        "statements*.**\\] Take `X == Y` as an example. For each position `i, j`,\n",
        "if `X[i, j]` and `Y[i, j]` are equal, then the corresponding entry in\n",
        "the result takes value `1`, otherwise it takes value `0`."
      ],
      "id": "a7c4cc08-3891-491e-a1d8-45ec1201afca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.237276Z",
          "iopub.status.busy": "2023-08-18T19:32:57.236485Z",
          "iopub.status.idle": "2023-08-18T19:32:57.243133Z",
          "shell.execute_reply": "2023-08-18T19:32:57.242117Z"
        },
        "origin_pos": 75,
        "outputId": "5bbe89b2-44c0-45ec-bf72-b90110a10c24",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[False,  True, False,  True],\n",
              "        [False, False, False, False],\n",
              "        [False, False, False, False]])"
            ]
          }
        }
      ],
      "source": [
        "X == Y"
      ],
      "id": "91d39e58"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\[**Summing all the elements in the tensor**\\] yields a tensor with only\n",
        "one element."
      ],
      "id": "52735284-465a-4b3d-8abd-16ad9d996db2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.247142Z",
          "iopub.status.busy": "2023-08-18T19:32:57.246480Z",
          "iopub.status.idle": "2023-08-18T19:32:57.253117Z",
          "shell.execute_reply": "2023-08-18T19:32:57.252212Z"
        },
        "origin_pos": 77,
        "outputId": "447ffb68-81ac-4f57-d57f-4c098f0f9428",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor(66.)"
            ]
          }
        }
      ],
      "source": [
        "X.sum()"
      ],
      "id": "080b0125"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Broadcasting\n",
        "\n",
        "By now, you know how to perform elementwise binary operations on two\n",
        "tensors of the same shape. Under certain conditions, even when shapes\n",
        "differ, we can still \\[**perform elementwise binary operations by\n",
        "invoking the *broadcasting mechanism*.**\\] Broadcasting works according\n",
        "to the following two-step procedure: (i) expand one or both arrays by\n",
        "copying elements along axes with length 1 so that after this\n",
        "transformation, the two tensors have the same shape; (ii) perform an\n",
        "elementwise operation on the resulting arrays."
      ],
      "id": "3dfe94f2-7afc-499f-a073-aff5acbe613d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.256932Z",
          "iopub.status.busy": "2023-08-18T19:32:57.256264Z",
          "iopub.status.idle": "2023-08-18T19:32:57.263823Z",
          "shell.execute_reply": "2023-08-18T19:32:57.262881Z"
        },
        "origin_pos": 81,
        "outputId": "e65b1e5b-fbea-4cab-eba6-58202b988ada",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(tensor([[0],\n",
              "         [1],\n",
              "         [2]]),\n",
              " tensor([[0, 1]]))"
            ]
          }
        }
      ],
      "source": [
        "a = torch.arange(3).reshape((3, 1))\n",
        "b = torch.arange(2).reshape((1, 2))\n",
        "a, b"
      ],
      "id": "be37d2de"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since `a` and `b` are $3\\times1$ and $1\\times2$ matrices, respectively,\n",
        "their shapes do not match up. Broadcasting produces a larger $3\\times2$\n",
        "matrix by replicating matrix `a` along the columns and matrix `b` along\n",
        "the rows before adding them elementwise."
      ],
      "id": "7de8cd60-cec0-4a8b-9550-48390c59b181"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.267856Z",
          "iopub.status.busy": "2023-08-18T19:32:57.267172Z",
          "iopub.status.idle": "2023-08-18T19:32:57.273497Z",
          "shell.execute_reply": "2023-08-18T19:32:57.272587Z"
        },
        "origin_pos": 85,
        "outputId": "2016b3f0-88ae-4581-fe24-66aa3f53ce35",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [1, 2],\n",
              "        [2, 3]])"
            ]
          }
        }
      ],
      "source": [
        "a + b"
      ],
      "id": "9f62e827"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Memory\n",
        "\n",
        "\\[**Running operations can cause new memory to be allocated to host\n",
        "results.**\\] For example, if we write `Y = X + Y`, we dereference the\n",
        "tensor that `Y` used to point to and instead point `Y` at the newly\n",
        "allocated memory. We can demonstrate this issue with Python’s `id()`\n",
        "function, which gives us the exact address of the referenced object in\n",
        "memory. Note that after we run `Y = Y + X`, `id(Y)` points to a\n",
        "different location. That is because Python first evaluates `Y + X`,\n",
        "allocating new memory for the result and then points `Y` to this new\n",
        "location in memory."
      ],
      "id": "aa8e35fc-7762-4731-aa80-10e234de546d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.277697Z",
          "iopub.status.busy": "2023-08-18T19:32:57.277047Z",
          "iopub.status.idle": "2023-08-18T19:32:57.283549Z",
          "shell.execute_reply": "2023-08-18T19:32:57.282613Z"
        },
        "origin_pos": 87,
        "outputId": "af739076-95a9-4f92-cdcc-a4f7aa70756e",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "False"
            ]
          }
        }
      ],
      "source": [
        "before = id(Y)\n",
        "Y = Y + X\n",
        "id(Y) == before"
      ],
      "id": "754a7433"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This might be undesirable for two reasons. First, we do not want to run\n",
        "around allocating memory unnecessarily all the time. In machine\n",
        "learning, we often have hundreds of megabytes of parameters and update\n",
        "all of them multiple times per second. Whenever possible, we want to\n",
        "perform these updates *in place*. Second, we might point at the same\n",
        "parameters from multiple variables. If we do not update in place, we\n",
        "must be careful to update all of these references, lest we spring a\n",
        "memory leak or inadvertently refer to stale parameters.\n",
        "\n",
        "Fortunately, (**performing in-place operations**) is easy. We can assign\n",
        "the result of an operation to a previously allocated array `Y` by using\n",
        "slice notation: `Y[:] = <expression>`. To illustrate this concept, we\n",
        "overwrite the values of tensor `Z`, after initializing it, using\n",
        "`zeros_like`, to have the same shape as `Y`."
      ],
      "id": "32cebf78-891d-4bf0-bc59-069d0a37bd43"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.287695Z",
          "iopub.status.busy": "2023-08-18T19:32:57.286964Z",
          "iopub.status.idle": "2023-08-18T19:32:57.293078Z",
          "shell.execute_reply": "2023-08-18T19:32:57.292048Z"
        },
        "origin_pos": 92,
        "outputId": "00a541a7-f34a-4c9d-87c6-b2fab8cb935a",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id(Z): 140381179266448\n",
            "id(Z): 140381179266448"
          ]
        }
      ],
      "source": [
        "Z = torch.zeros_like(Y)\n",
        "print('id(Z):', id(Z))\n",
        "Z[:] = X + Y\n",
        "print('id(Z):', id(Z))"
      ],
      "id": "c4d62609"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\[**If the value of `X` is not reused in subsequent computations, we can\n",
        "also use `X[:] = X + Y` or `X += Y` to reduce the memory overhead of the\n",
        "operation.**\\]"
      ],
      "id": "d9119ad2-e816-41f0-92e3-8e30b01de010"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.296911Z",
          "iopub.status.busy": "2023-08-18T19:32:57.296361Z",
          "iopub.status.idle": "2023-08-18T19:32:57.302754Z",
          "shell.execute_reply": "2023-08-18T19:32:57.301805Z"
        },
        "origin_pos": 97,
        "outputId": "601c43ad-08ac-4563-e4f9-9ec0b75a1f7c",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "True"
            ]
          }
        }
      ],
      "source": [
        "before = id(X)\n",
        "X += Y\n",
        "id(X) == before"
      ],
      "id": "b8c13447"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conversion to Other Python Objects\n",
        "\n",
        "\\[**Converting to a NumPy tensor (`ndarray`)**\\], or vice versa, is\n",
        "easy. The torch tensor and NumPy array will share their underlying\n",
        "memory, and changing one through an in-place operation will also change\n",
        "the other."
      ],
      "id": "1972744d-da48-41fc-aec9-cc808b5dd6d8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.306812Z",
          "iopub.status.busy": "2023-08-18T19:32:57.306088Z",
          "iopub.status.idle": "2023-08-18T19:32:57.312356Z",
          "shell.execute_reply": "2023-08-18T19:32:57.311478Z"
        },
        "origin_pos": 103,
        "outputId": "99d0de0a-2c84-4c1c-bd06-7b7a44ea3538",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(numpy.ndarray, torch.Tensor)"
            ]
          }
        }
      ],
      "source": [
        "A = X.numpy()\n",
        "B = torch.from_numpy(A)\n",
        "type(A), type(B)"
      ],
      "id": "576963aa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To (**convert a size-1 tensor to a Python scalar**), we can invoke the\n",
        "`item` function or Python’s built-in functions."
      ],
      "id": "6b7088e1-c49c-432d-af47-43736701b461"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-08-18T19:32:57.316471Z",
          "iopub.status.busy": "2023-08-18T19:32:57.315825Z",
          "iopub.status.idle": "2023-08-18T19:32:57.322867Z",
          "shell.execute_reply": "2023-08-18T19:32:57.322007Z"
        },
        "origin_pos": 108,
        "outputId": "9be47dd0-c5b6-49ff-d933-1bc75fdd2f42",
        "tab": [
          "pytorch"
        ]
      },
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(tensor([3.5000]), 3.5, 3.5, 3)"
            ]
          }
        }
      ],
      "source": [
        "a = torch.tensor([3.5])\n",
        "a, a.item(), float(a), int(a)"
      ],
      "id": "388c5252"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "The tensor class is the main interface for storing and manipulating data\n",
        "in deep learning libraries. Tensors provide a variety of functionalities\n",
        "including construction routines; indexing and slicing; basic mathematics\n",
        "operations; broadcasting; memory-efficient assignment; and conversion to\n",
        "and from other Python objects.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1.  Run the code in this section. Change the conditional statement\n",
        "    `X == Y` to `X < Y` or `X > Y`, and then see what kind of tensor you\n",
        "    can get.\n",
        "2.  Replace the two tensors that operate by element in the broadcasting\n",
        "    mechanism with other shapes, e.g., 3-dimensional tensors. Is the\n",
        "    result the same as expected?"
      ],
      "id": "187c30b6-9e8d-4444-8afa-71aa8f8227f9"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": []
  }
}